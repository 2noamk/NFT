{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "# Calculate percentage MSE reduction for each dataset and horizon\n",
    "def Calculate_percentage_MSE_reduction(data, dft_only=False, non_dft_only=False):\n",
    "    results = {}\n",
    "    non_comperable_models = ['nft']\n",
    "    if dft_only: non_comperable_models = ['nft', 'tcn', 'patchtst', 'dlinear']\n",
    "    if non_dft_only: non_comperable_models = ['nft', 'timesnet', 'nbeats', 'autoformer']\n",
    "    \n",
    "    for dataset, values in data.items():\n",
    "        mse_reductions = {}\n",
    "        for horizon_idx, horizon in enumerate(values['horizons']):\n",
    "            # Extract MSE values for this horizon across all models\n",
    "            mse_values = {model: mse[horizon_idx] for model, mse in values['models'].items()}\n",
    "            # Calculate closest MSE (minimum) excluding NFT\n",
    "            closest_mse = min(mse for model, mse in mse_values.items() if model not in non_comperable_models)\n",
    "            # Calculate MSE reduction for NFT\n",
    "            nft_mse = mse_values['nft']\n",
    "            mse_reduction = (1 - (nft_mse / closest_mse)) * 100\n",
    "            mse_reductions[horizon] = mse_reduction\n",
    "        results[dataset] = mse_reductions\n",
    "\n",
    "    return results\n",
    "\n",
    "# Calculate the average percentage MSE reduction across horizons for each dataset\n",
    "def Calculate_average_percentage_MSE(results, value_type='all'):\n",
    "    average_results = {}\n",
    "    for dataset, values in results.items():\n",
    "        average_reduction = sum(values.values()) / len(values)\n",
    "        average_results[dataset] = average_reduction\n",
    "        \n",
    "# Create DataFrame from the results\n",
    "    new_data = pd.DataFrame({\n",
    "        'dataset': average_results.keys(),\n",
    "        'values': average_results.values(),\n",
    "        'type': value_type\n",
    "    })\n",
    "    \n",
    "    file_path  = '/home/noam.koren/multiTS/NFT/results/precent_results.xlsx'\n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(file_path):\n",
    "        # Read the existing file\n",
    "        existing_data = pd.read_excel(file_path, engine='openpyxl')\n",
    "        # Concatenate the new data with the existing data\n",
    "        combined_data = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "    else:\n",
    "        combined_data = new_data\n",
    "    \n",
    "    # Save the combined data to Excel\n",
    "    combined_data.to_excel(file_path, index=False, engine='openpyxl')\n",
    "    \n",
    "    print(average_results)\n",
    "    return average_results\n",
    "\n",
    "def Calculate_percentage_MSE_reduction_ablation(data_ablation):\n",
    "    results_ablation_lstm = {}\n",
    "    for dataset, values in data_ablation.items():\n",
    "        mse_reductions = {}\n",
    "        for horizon_idx, horizon in enumerate(values['horizons']):\n",
    "            # Extract MSE values for this horizon across all models\n",
    "            mse_values = {model: mse[horizon_idx] for model, mse in values['models'].items()}\n",
    "            # Calculate closest MSE (minimum) excluding NFT\n",
    "            closest_mse = min(mse for model, mse in mse_values.items() if model != 'nft' and model != 'fc nft')\n",
    "            # Calculate MSE reduction for NFT\n",
    "            nft_mse = mse_values['nft']\n",
    "            mse_reduction = (1 - (nft_mse / closest_mse)) * 100\n",
    "            mse_reductions[horizon] = mse_reduction\n",
    "        results_ablation_lstm[dataset] = mse_reductions\n",
    "        \n",
    "    results_ablation_fc = {}\n",
    "    for dataset, values in data_ablation.items():\n",
    "        mse_reductions = {}\n",
    "        for horizon_idx, horizon in enumerate(values['horizons']):\n",
    "            # Extract MSE values for this horizon across all models\n",
    "            mse_values = {model: mse[horizon_idx] for model, mse in values['models'].items()}\n",
    "            # Calculate closest MSE (minimum) excluding NFT\n",
    "            closest_mse = min(mse for model, mse in mse_values.items() if model != 'nft' and model != 'lstm nft')\n",
    "            # Calculate MSE reduction for NFT\n",
    "            nft_mse = mse_values['nft']\n",
    "            mse_reduction = (1 - (nft_mse / closest_mse)) * 100\n",
    "            mse_reductions[horizon] = mse_reduction\n",
    "        results_ablation_fc[dataset] = mse_reductions\n",
    "        \n",
    "    \n",
    "    return results_ablation_lstm, results_ablation_fc\n",
    "\n",
    "\n",
    "def average_dict_values(data):\n",
    "    if not data:\n",
    "        return 0  # Return 0 for an empty dictionary\n",
    "    \n",
    "    total = 0\n",
    "    count = 0\n",
    "    \n",
    "    for value in data.values():\n",
    "        if not math.isnan(value):\n",
    "            total += value\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        return 0  # Avoid division by zero if all values are NaN\n",
    "    return total / count\n",
    "\n",
    "def Calulate_t_test(data):\n",
    "    t_stat_dict = {}\n",
    "    p_value_dict = {}\n",
    "\n",
    "    for dataset, values in data.items():\n",
    "        t_stat_dict[dataset] = {}\n",
    "        p_value_dict[dataset] = {}\n",
    "        nft_vals=values['models']['nft']\n",
    "        for model, vals in values['models'].items():\n",
    "            if model != 'nft':\n",
    "                t_stat, p_value = stats.ttest_rel(nft_vals, vals)\n",
    "                t_stat_dict[dataset][model] = t_stat\n",
    "                p_value_dict[dataset][model] = p_value\n",
    "    return t_stat_dict, p_value_dict\n",
    "\n",
    "def calculate_average_t_p(t_stat_dict, p_value_dict):\n",
    "    avg_t_stat = {}\n",
    "    avg_p_value = {}\n",
    "\n",
    "    # Calculate average for each dataset\n",
    "    for dataset in t_stat_dict:\n",
    "        avg_t_stat[dataset] = sum(t_stat_dict[dataset].values()) / len(t_stat_dict[dataset])\n",
    "        avg_p_value[dataset] = sum(p_value_dict[dataset].values()) / len(p_value_dict[dataset])\n",
    "\n",
    "    # Calculate overall average across all datasets\n",
    "    total_t_stat = sum([sum(vals.values()) for vals in t_stat_dict.values()])\n",
    "    total_t_count = sum([len(vals) for vals in t_stat_dict.values()])\n",
    "    overall_avg_t_stat = average_dict_values(avg_t_stat)#total_t_stat / total_t_count\n",
    "\n",
    "    total_p_value = sum([sum(vals.values()) for vals in p_value_dict.values()])\n",
    "    total_p_count = sum([len(vals) for vals in p_value_dict.values()])\n",
    "    overall_avg_p_value = average_dict_values(avg_p_value)#total_p_value / total_p_count\n",
    "\n",
    "    return avg_t_stat, avg_p_value, overall_avg_t_stat, overall_avg_p_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_data = {\n",
    "'Traffic': {\n",
    "    'horizons': [1, 16, 32, 48],\n",
    "    'models': {\n",
    "        'nft': [0.09, 0.18, 0.26, 0.4],\n",
    "        'timesnet': [0.53, 0.58, 0.62, 0.62],\n",
    "        'patchtst': [0.26, 0.58, 0.63, 0.66],\n",
    "        'dlinear': [0.4, 0.63, 0.67, 0.72],\n",
    "        'Autoformer': [0.5, 0.56, 0.67, 0.65],\n",
    "        'tcn': [0.1, 0.46, 0.49, 0.51],\n",
    "        'nbeats': [0.2, 0.51, 0.59, 0.71]\n",
    "    }\n",
    "},\n",
    "\n",
    "    'Weather': {\n",
    "        'horizons': [1, 7, 15, 30, 60, 90, 120, 180],\n",
    "        'models': {\n",
    "            'nft': [0.19, 0.24, 0.24, 0.25, 0.26, 0.27, 0.28, 0.27],\n",
    "            'timesnet': [0.33, 0.35, 0.44, 0.36, 0.37, 0.37, 0.38, 0.37],\n",
    "            'patchtst': [0.34, 0.36, 0.37, 0.38, 0.40, 0.42, 0.42, 0.43],\n",
    "            'dlinear': [0.29, 0.32, 0.33, 0.35, 0.37, 0.40, 0.41, 0.40],\n",
    "            'Autoformer': [0.37, 0.37, 0.37, 0.38, 0.42, 0.44, 0.46, 0.47],\n",
    "            'tcn': [0.25, 0.29, 0.30, 0.32, 0.36, 0.33, 0.33, 0.35],\n",
    "            'nbeats': [0.31, 0.33, 0.35, 0.38, 0.40, 0.42, 0.43, 0.44]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'Electricity': {\n",
    "        'horizons': [1, 16, 32],\n",
    "        'models': {\n",
    "            'nft': [0.04, 0.08, 0.13],\n",
    "            'timesnet': [0.14, 0.16, 0.17],\n",
    "            'patchtst': [0.09, 0.20, 0.17],\n",
    "            'dlinear': [0.07, 0.16, 0.18],\n",
    "            'Autoformer': [0.20, 0.40, 0.60],\n",
    "            'tcn': [0.05, 0.32, 0.44],\n",
    "            'nbeats': [0.06, 0.13, 0.18]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'Exchange Rate all': {\n",
    "    'horizons': [1, 16, 32, 48, 96, 192, 336, 720],\n",
    "    'models': {\n",
    "        'nft': [0.0001, 0.01, 0.02, 0.02, 0.02, 0.03, 0.04, 0.05],\n",
    "        'timesnet': [0.05, 0.18, 0.15, 0.11, 0.107, 0.226, 0.367, 0.964],\n",
    "        'patchtst': [0.01, 0.02, 0.03, 0.04, 0.09, 0.18, 0.4, 0.94],\n",
    "        'dlinear': [0.01, 0.02, 0.03, 0.05, 0.088, 0.176, 0.313, 0.839],\n",
    "        'Autoformer': [0.04, 0.04, 0.06, 0.09, 0.197, 0.300, 0.509, 1.447],\n",
    "        'tcn': [0.2, 0.11, 0.04, 0.06, 0.09, 0.17, 0.24, 0.41],\n",
    "        'nbeats': [0.01, 0.03, 0.05, 0.12, 0.17, 0.19, 0.21, 0.25]\n",
    "    }\n",
    "    },\n",
    "    \n",
    "    'ILI': {\n",
    "        'horizons': [1, 12, 24, 36, 48, 60],\n",
    "        'models': {\n",
    "            'nft': [0.05, 0.18, 0.25, 0.31, 0.35, 0.42],\n",
    "            'timesnet': [0.64, 1.01, 2.317, 1.972, 2.238, 2.027],\n",
    "            'patchtst': [0.24, 1.25, 1.319, 1.579, 1.553, 1.470],\n",
    "            'dlinear': [2.32, 2.83, 2.215, 1.963, 2.13, 2.368],\n",
    "            'Autoformer': [0.84, 2.47, 3.483, 3.103, 2.669, 2.770],\n",
    "            'tcn': [0.86, 0.98, 0.95, 0.94, 1.01, 1.03],\n",
    "            'nbeats': [0.3, 1.33, 1.19, 1.44, 1.09, 1.48]\n",
    "        }\n",
    "    },\n",
    "    'Air Quality': {\n",
    "        'horizons': [1, 5, 10, 15, 20, 25, 30],\n",
    "        'models': {\n",
    "            'nft': [0.13, 0.21, 0.28, 0.31, 0.35, 0.38, 0.4],\n",
    "            'timesnet': [0.74, 0.81, 0.85, 0.87, 0.85, 0.92, 0.96],\n",
    "            'patchtst': [0.22, 0.58, 0.68, 0.72, 0.75, 0.78, 0.83],\n",
    "            'dlinear': [0.39, 0.69, 0.78, 0.83, 0.87, 0.93, 1.01],\n",
    "            'Autoformer': [0.72, 0.86, 0.97, 1.02, 0.98, 1.03, 0.99],\n",
    "            'tcn': [0.21, 0.42, 0.53, 0.64, 0.69, 0.68, 0.69],\n",
    "            'nbeats': [0.38, 0.64, 0.82, 0.86, 0.93, 1, 1.1]\n",
    "        }\n",
    "    },\n",
    "    'ECG (600)': {\n",
    "        'horizons': [1, 5, 10, 15, 20, 25, 30],\n",
    "        'models': {\n",
    "            'nft': [0.02, 0.08, 0.07, 0.1, 0.15, 0.15, 0.1],\n",
    "            'timesnet': [0.02, 0.08, 0.08, 0.1, 0.15, 0.15, 0.17],\n",
    "            'patchtst': [0.02 ,0.05 ,0.08 ,0.1 ,0.15 ,0.15 ,0.18],\n",
    "            'dlinear': [0.02 ,0.06 ,0.08 ,0.11 ,0.14 ,0.16 ,0.19],\n",
    "            'Autoformer': [0.8,0.49,0.38, 0.41 ,0.46 ,0.44 ,0.45],\n",
    "            'tcn': [ 0.03 ,0.07 ,0.1 ,0.12 ,0.16 ,0.18 ,0.21],\n",
    "            'nbeats': [ 0.07 ,0.07 ,0.11 ,0.15 ,0.18 ,0.22 ,0.28]\n",
    "        }\n",
    "    },\n",
    "    'ECG': {\n",
    "        'horizons': [1, 10, 25, 50, 100],\n",
    "        'models': {\n",
    "            'nft': [0.04, 0.08, 0.12, 0.14, 0.18],\n",
    "            'timesnet': [0.31 ,0.49 ,0.625 ,1.16 ,1.09],\n",
    "            'patchtst': [0.34 ,0.57 ,0.77 ,0.94 ,0.98],\n",
    "            'dlinear': [0.35 ,0.46 ,0.58 ,0.66 ,0.69],\n",
    "            'Autoformer': [1.62 ,1.13 ,1.5 ,1.15 ,1.26],\n",
    "            'tcn': [0.16 ,0.42 ,0.56 ,0.70 ,0.81],\n",
    "            'nbeats': [0.22 ,0.44 ,0.50 ,0.72 ,0.67]\n",
    "        }\n",
    "    },\n",
    "    'EEG': {\n",
    "        'horizons': [1, 10, 25],\n",
    "        'models': {\n",
    "            'nft': [0.03, 0.1, 0.16],\n",
    "            'timesnet': [0.16, 0.35, 0.44],\n",
    "            'patchtst': [0.15, 0.345, 0.42],\n",
    "            'dlinear': [0.135, 0.365, 0.435],\n",
    "            'Autoformer': [0.63, 0.545, 0.62],\n",
    "            'tcn': [0.09, 0.32, 0.39],\n",
    "            'nbeats': [0.37, 0.335, 0.336]\n",
    "        }\n",
    "    },\n",
    "    'Chorales': {\n",
    "        'horizons': [1, 2, 3, 4, 5],\n",
    "        'models': {\n",
    "            'nft': [0.15, 0.18, 0.20, 0.23, 0.22],\n",
    "            'timesnet': [0.29, 0.3, 0.31, 0.31, 0.32],\n",
    "            'patchtst': [0.34, 0.27, 0.29, 0.30, 0.31],\n",
    "            'dlinear': [0.29, 0.29, 0.40, 0.37, 0.36],\n",
    "            'Autoformer': [0.30, 0.25, 0.25, 0.26, 0.24],\n",
    "            'tcn': [0.27, 0.63, 0.62, 0.37, 0.56],\n",
    "            'nbeats': [0.35, 0.27, 0.27, 0.26, 0.29],\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'ETTh1': {\n",
    "        'horizons': [1, 10, 24, 48, 96, 192, 336, 720],\n",
    "        'models': {\n",
    "            'nft': [0.07, 0.14, 0.18, 0.25, 0.39, 0.43, 0.44, 0.46],\n",
    "            'timesnet': [0.15, 0.29, 0.33, 0.37, 0.38, 0.44, 0.49, 0.52],\n",
    "            'patchtst': [0.1, 0.25, 0.29, 0.34, 0.37, 0.41, 0.42, 0.45],\n",
    "            'dlinear': [0.12, 0.28, 0.31, 0.35, 0.37, 0.41, 0.44, 0.47],\n",
    "            'Autoformer': [0.17, 0.4, 0.4, 0.427, 0.44, 0.46, 0.49, 0.52],\n",
    "            'tcn': [0.14, 0.29, 0.43, 0.46, 0.46, 0.54, 0.53, 0.58],\n",
    "            'nbeats': [0.11, 0.29, 0.33, 0.47, 0.71, 0.78, 0.79, 0.84]\n",
    "        }\n",
    "    },\n",
    "    'ETTh1_long': {\n",
    "        'horizons': [96, 192, 336, 720],\n",
    "        'models': {\n",
    "            'nft': [0.39, 0.43, 0.44, 0.46],\n",
    "            'timesnet': [0.4, 0.44, 0.49, 0.52],\n",
    "            'patchtst': [0.37, 0.41, 0.42, 0.45],\n",
    "            'dlinear': [0.37, 0.41, 0.44, 0.47],\n",
    "            'Autoformer': [0.43, 0.46, 0.49, 0.52],\n",
    "            'tcn': [0.46, 0.54, 0.53, 0.58],\n",
    "            'nbeats': [0.71, 0.78, 0.79, 0.84]\n",
    "        }\n",
    "    },\n",
    "    'ETTh2': {\n",
    "        'horizons': [1, 10, 24, 48, 96, 192, 336, 720],\n",
    "        'models': {\n",
    "            'nft': [0.06, 0.1, 0.13, 0.16, 0.19, 0.23, 0.26, 0.34],\n",
    "            'timesnet': [0.1, 0.15, 0.2, 0.25, 0.34, 0.4, 0.45, 0.46],\n",
    "            'patchtst': [0.06, 0.12, 0.17, 0.22, 0.274, 0.34, 0.33, 0.38],\n",
    "            'dlinear': [0.11, 0.13, 0.18, 0.25, 0.289, 0.38, 0.45, 0.61],\n",
    "            'Autoformer': [0.12, 0.21, 0.26, 0.32, 0.332, 0.43, 0.48, 0.45],\n",
    "            'tcn': [0.06, 0.11, 0.15, 0.19, 0.24, 0.48, 0.54, 0.55],\n",
    "            'nbeats': [0.14, 0.18, 0.24, 0.41, 1.65, 1.95, 1.75, 1.73]\n",
    "        }\n",
    "    },\n",
    "    'ETTm1': {\n",
    "        'horizons': [1, 10, 24, 48, 96, 192, 336, 720],\n",
    "        'models': {\n",
    "            'nft': [0.04, 0.07, 0.11, 0.14, 0.16, 0.24, 0.37, 0.43],\n",
    "            'timesnet': [0.05, 0.13, 0.20, 0.29, 0.34, 0.37, 0.41, 0.48],\n",
    "            'patchtst': [0.05, 0.12, 0.21, 0.28, 0.293, 0.33, 0.37, 0.42],\n",
    "            'dlinear': [0.05, 0.14, 0.24, 0.31, 0.299, 0.34, 0.37, 0.43],\n",
    "            'Autoformer': [0.09, 0.24, 0.4, 0.56, 0.51, 0.51, 0.51, 0.53],\n",
    "            'tcn': [0.04, 0.09, 0.14, 0.19, 0.18, 0.47, 0.45, 0.54],\n",
    "            'nbeats': [0.05, 0.13, 0.23, 0.3, 0.35, 0.5, 0.58, 0.79]\n",
    "        }\n",
    "    },\n",
    "    'ETTm2': {\n",
    "        'horizons': [1, 10, 24, 48, 96, 192, 336, 720],\n",
    "        'models': {\n",
    "            'nft': [0.03, 0.05, 0.07, 0.1, 0.13, 0.17, 0.22, 0.26],\n",
    "            'timesnet': [0.04, 0.07, 0.11, 0.15, 0.16, 0.25, 0.32, 0.41],\n",
    "            'patchtst': [0.03, 0.07, 0.1, 0.14, 0.166, 0.22, 0.27, 0.36],\n",
    "            'dlinear': [0.03, 0.07, 0.11, 0.15, 0.167, 0.22, 0.28, 0.40],\n",
    "            'Autoformer': [0.07, 0.1, 0.14, 0.165, 0.205, 0.28, 0.34, 0.41],\n",
    "            'tcn': [0.03, 0.06, 0.09, 0.12, 0.16, 0.3, 0.34, 0.42],\n",
    "            'nbeats': [0.04, 0.07, 0.14, 0.18, 0.26, 0.59, 0.81, 1.00]\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "\n",
    "data_nbeats_mase = {\n",
    "    'Weather': {\n",
    "        'horizons': [1, 7, 15, 30, 60, 90, 120, 180],\n",
    "        'models': {\n",
    "            'nft': [79.68 , 90.39 , 91.27 , 92.2 , 95.39, 95.53 , 96.86 , 97.38],\n",
    "            'nbeats': [92.77 ,96.86 ,99.92 ,97.02 ,99.48 ,99.89 ,100.69 , 102.27]\n",
    "        }\n",
    "    },\n",
    "    'Electricity': {\n",
    "        'horizons': [1, 16, 32],\n",
    "        'models': {\n",
    "            'nft': [34.06 , 52.14 , 63.29],\n",
    "            'nbeats': [35.36 , 48.86 ,57.62]\n",
    "        }\n",
    "    },\n",
    "    'Exchange Rate': {\n",
    "        'horizons': [1, 16, 48],\n",
    "        'models': {\n",
    "            'nft': [7.25, 18.71, 30.5],\n",
    "            'nbeats': [8.2, 17.9, 29.54],\n",
    "        }\n",
    "    },\n",
    "    'ILI': {\n",
    "        'horizons': [1, 12, 60],\n",
    "        'models': {\n",
    "            'nft': [44.86, 85.97, 92.21],\n",
    "            'nbeats': [49.49, 86.61, 111.63],\n",
    "        }\n",
    "    },\n",
    "    'Air Quality': {\n",
    "        'horizons': [1, 5, 10, 15, 20, 25, 30],\n",
    "        'models': {\n",
    "            'nft': [58.38, 79.23 , 87.01 , 93.56 , 95.79 , 99.38 , 102.58],\n",
    "             'nbeats': [75.37 ,83.85 ,100.55 ,102.78 ,103.91 ,109.81 ,111.67],\n",
    "        }\n",
    "    },\n",
    "    'ECG (600)': {\n",
    "        'horizons': [1, 5, 10, 15, 20, 25, 30],\n",
    "        'models': {\n",
    "            'nft': [24.07 ,43.86 ,45.26 ,53.87 ,59.81 ,60.03 , 68.3],\n",
    "            'nbeats': [25.31 ,41.26 ,49.96 ,56.41 ,63.83 ,68.31 , 74.1],\n",
    "        }\n",
    "    },\n",
    "    'ECG': {\n",
    "        'horizons': [1, 10, 25, 50, 100],\n",
    "        'models': {\n",
    "            'nft': [40.47, 64.81, 74.70, 82.94, 88.75],\n",
    "            'nbeats': [73.5,87.02,96.57,108.48,106.95],\n",
    "        }\n",
    "    },\n",
    "    'EEG': {\n",
    "        'horizons': [1, 10, 25],\n",
    "        'models': {\n",
    "            'nft': [45.83 , 62.90 , 72.23],\n",
    "            'nbeats': [67.38,82.965,94],\n",
    "        }\n",
    "    },\n",
    "    'Chorales': {\n",
    "        'horizons': [1, 2, 3, 4, 5],\n",
    "        'models': {\n",
    "            'nft': [69.37 , 71.21 , 73.56 , 73.46 , 75.82],\n",
    "            'nbeats': [70.36 ,71.43 ,71.55 ,72.68 ,76.4],\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "data_ablation_mase = {\n",
    "    'Weather': {\n",
    "        'horizons': [1, 7, 15, 30, 60, 90, 120, 180],\n",
    "        'models': {\n",
    "            'nft': [0.65, 0.75, 0.76, 0.79, 0.81, 0.82, 0.83, 0.85],\n",
    "            'lstm nft': [0.73, 0.83, 0.87, 0.88, 0.90, 0.91, 0.91, 0.92],\n",
    "            'fc nft': [0.86, 0.89, 0.90, 0.90, 0.92, 0.92, 0.91, 0.91]\n",
    "        }\n",
    "    },\n",
    "    'Air Quality': {\n",
    "        'horizons': [1, 5, 10, 15, 20, 25, 30],\n",
    "        'models': {\n",
    "            'nft': [0.15, 0.25, 0.30, 0.34, 0.35, 0.41, 0.39],\n",
    "            'lstm nft': [0.25, 0.50, 0.52, 0.51, 0.61, 0.65, 0.62],\n",
    "            'fc nft': [0.30, 0.45, 0.51, 0.56, 0.59, 0.60, 0.63]\n",
    "        }\n",
    "    },\n",
    "    'ECG (600)': {\n",
    "        'horizons': [1, 5, 10, 15, 20, 25, 30],\n",
    "        'models': {\n",
    "            'nft': [0.11, 0.24, 0.25, 0.32, 0.36, 0.37, 0.44],\n",
    "            'lstm nft': [0.11, 0.20, 0.25, 0.30, 0.35, 0.38, 0.41],\n",
    "            'fc nft': [0.16, 0.23, 0.27, 0.35, 0.36, 0.41, 0.44]\n",
    "        }\n",
    "    },\n",
    "    'ECG': {\n",
    "        'horizons': [1, 10, 25, 50, 100],\n",
    "        'models': {\n",
    "            'nft': [0.18, 0.34, 0.42, 0.49, 0.54],\n",
    "            'lstm nft': [0.27, 0.59, 0.613, 0.6825, 0.715],\n",
    "            'fc nft': [0.47, 0.557, 0.56, 0.6275, 0.6425]\n",
    "        }\n",
    "    },\n",
    "    'EEG': {\n",
    "        'horizons': [1, 10, 25],\n",
    "        'models': {\n",
    "            'nft': [0.14, 0.27, 0.34],\n",
    "            'lstm nft': [0.185, 0.48, 0.63],\n",
    "            'fc nft': [0.245, 0.49, 0.585]\n",
    "        }\n",
    "    },\n",
    "    'Chorales': {\n",
    "        'horizons': [1, 2, 3, 4, 5],\n",
    "        'models': {\n",
    "            'nft': [0.27, 0.24, 0.25, 0.27, 0.28],\n",
    "            'lstm nft': [0.21, 0.22, 0.23, 0.23, 0.24],\n",
    "            'fc nft': [0.32, 0.27, 0.26, 0.29, 0.32]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "data_ablation_smape = {\n",
    "    'Weather': {\n",
    "        'horizons': [1, 7, 15, 30, 60, 90, 120, 180],\n",
    "        'models': {\n",
    "            'nft': [79.68, 90.39, 91.27, 92.27, 95.39, 95.53, 96.86, 97.38],\n",
    "            'lstm nft': [84.94, 92.57, 93.35, 94.36, 95.72, 96.18, 97.90, 98.64],\n",
    "            'fc nft': [91.08, 93.95, 95.89, 94.80, 97.08, 98.38, 98.54, 98.12]\n",
    "        }\n",
    "    },\n",
    "    'Air Quality': {\n",
    "        'horizons': [1, 5, 10, 15, 20, 25, 30],\n",
    "        'models': {\n",
    "            'nft': [58.38, 79.23, 87.01, 93.56, 95.79, 99.38, 102.58],\n",
    "            'lstm nft': [76.03, 113.43, 117.59, 113.79, 126.13, 127.83, 124.97],\n",
    "            'fc nft': [86.05, 104.42, 113.86, 119.42, 124.11, 125.50, 126.63]\n",
    "        }\n",
    "    },\n",
    "    'ECG (600)': {\n",
    "        'horizons': [1, 5, 10, 15, 20, 25, 30],\n",
    "        'models': {\n",
    "            'nft': [24.07, 43.86, 45.26, 53.87, 59.81, 60.03, 68.39],\n",
    "            'lstm nft': [25.86, 39.42, 46.68, 52.26, 59.88, 63.40, 66.84],\n",
    "            'fc nft': [35.07, 43.96, 49.07, 60.21, 59.71, 65.59, 69.99]\n",
    "        }\n",
    "    },\n",
    "    'ECG': {\n",
    "        'horizons': [1, 10, 25, 50, 100],\n",
    "        'models': {\n",
    "            'nft': [40.47, 64.81, 74.70, 82.94, 88.75],\n",
    "            'lstm nft': [52.91, 87.48, 94.17, 98.95, 103.40],\n",
    "            'fc nft': [79.23, 86.99, 90.93, 95.28, 98.03]\n",
    "        }\n",
    "    },\n",
    "    'EEG': {\n",
    "        'horizons': [1, 10, 25],\n",
    "        'models': {\n",
    "            'nft': [45.83, 62.90, 72.23],\n",
    "            'lstm nft': [49.80, 88.16, 104.18],\n",
    "            'fc nft': [59.50, 89.85, 104.16]\n",
    "        }\n",
    "    },\n",
    "    'Chorales': {\n",
    "        'horizons': [1, 2, 3, 4, 5],\n",
    "        'models': {\n",
    "            'nft': [69.37, 71.21, 73.56, 73.46, 75.82],\n",
    "            'lstm nft': [95.14, 90.37, 86.56, 88.99, 91.87],\n",
    "            'fc nft': [86.47, 76.74, 75.65, 79.05, 84.91]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "data_ablation_mse = {\n",
    "    'Weather': {\n",
    "        'horizons': [1, 7, 15, 30, 60, 90, 120, 180],\n",
    "        'models': {\n",
    "            'nft': [0.19, 0.24, 0.24, 0.25, 0.26, 0.27, 0.28, 0.27],\n",
    "            'lstm nft': [0.29, 0.33, 0.36, 0.37, 0.39, 0.41, 0.42, 0.44],\n",
    "            'fc nft': [0.41, 0.39, 0.40, 0.39, 0.40, 0.41, 0.42, 0.41]\n",
    "        }\n",
    "    },\n",
    "    'Air Quality': {\n",
    "        'horizons': [1, 5, 10, 15, 20, 25, 30],\n",
    "        'models': {\n",
    "            'nft': [0.13, 0.21, 0.28, 0.31, 0.35, 0.38, 0.40],\n",
    "            'lstm nft': [0.38, 1.21, 1.29, 1.25, 1.76, 2.02, 1.78],\n",
    "            'fc nft': [0.49, 1.04, 1.27, 1.51, 1.62, 1.72, 1.88]\n",
    "        }\n",
    "    },\n",
    "    'ECG (600)': {\n",
    "        'horizons': [1, 5, 10, 15, 20, 25, 30],\n",
    "        'models': {\n",
    "            'nft': [0.02, 0.08, 0.07, 0.10, 0.15, 0.15, 0.10],\n",
    "            'lstm nft': [0.07, 0.07, 0.10, 0.14, 0.15, 0.18, 0.21],\n",
    "            'fc nft': [0.08, 0.08, 0.12, 0.16, 0.19, 0.22, 0.24]\n",
    "        }\n",
    "    },\n",
    "    'ECG': {\n",
    "        'horizons': [1, 10, 25, 50, 100],\n",
    "        'models': {\n",
    "            'nft': [0.04, 0.08, 0.12, 0.14, 0.18],\n",
    "            'lstm nft': [0.085, 0.3675, 0.4375, 0.5225, 0.5825],\n",
    "            'fc nft': [0.225, 0.3375, 0.4025, 0.475, 0.5175]\n",
    "        }\n",
    "    },\n",
    "    'EEG': {\n",
    "        'horizons': [1, 10, 25],\n",
    "        'models': {\n",
    "            'nft': [0.03, 0.10, 0.16],\n",
    "            'lstm nft': [0.10, 0.445, 0.655],\n",
    "            'fc nft': [0.125, 0.425, 0.545]\n",
    "        }\n",
    "    },\n",
    "    'Chorales': {\n",
    "        'horizons': [1, 2, 3, 4, 5],\n",
    "        'models': {\n",
    "            'nft': [0.15, 0.18, 0.20, 0.23, 0.22],\n",
    "            'lstm nft': [0.36, 0.30, 0.26, 0.245, 0.23],\n",
    "            'fc nft': [0.35, 0.29, 0.28, 0.29, 0.28]\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Calculate_percentage_MSE_reduction(mse_data)\n",
    "Calculate_average_percentage_MSE(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Calculate_percentage_MSE_reduction(mse_data, non_dft_only=True)\n",
    "Calculate_average_percentage_MSE(results, 'non dft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Calculate_percentage_MSE_reduction(mse_data, dft_only=True)\n",
    "Calculate_average_percentage_MSE(results, 'dft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "datasets = []\n",
    "horizons_list = []\n",
    "predictive_ranges = []\n",
    "nft_mses = []\n",
    "closest_baseline_mses = []\n",
    "improvements = []\n",
    "\n",
    "for dataset in mse_data:\n",
    "    horizons = mse_data[dataset]['horizons']\n",
    "    nft_mse = mse_data[dataset]['models']['nft']\n",
    "    \n",
    "    # Get the MSE values of all baseline models excluding NFT\n",
    "    baseline_models = {model: mse for model, mse in mse_data[dataset]['models'].items() if model != 'nft'}\n",
    "    \n",
    "    for i, mse in enumerate(nft_mse):\n",
    "        # Calculate the percentage of the predictive range\n",
    "        predictive_range_percentage = horizons[i] / max(horizons) * 100\n",
    "        \n",
    "        # Find the closest baseline MSE for the current NFT MSE\n",
    "        closest_baseline_mse = min([mse_list[i] for mse_list in baseline_models.values()], key=lambda x: abs(x - mse))\n",
    "        \n",
    "        # Calculate the improvement percentage over the closest baseline MSE\n",
    "        improvement_percentage = (closest_baseline_mse - mse) / closest_baseline_mse * 100\n",
    "        \n",
    "        datasets.append(dataset)\n",
    "        horizons_list.append(horizons[i])\n",
    "        predictive_ranges.append(predictive_range_percentage)\n",
    "        nft_mses.append(mse)\n",
    "        closest_baseline_mses.append(closest_baseline_mse)\n",
    "        improvements.append(improvement_percentage)\n",
    "\n",
    "# Create a dataframe for the results\n",
    "df_results = pd.DataFrame({\n",
    "    'dataset': datasets,\n",
    "    'horizon': horizons_list,\n",
    "    'predictive_range_percentage': predictive_ranges,\n",
    "    'nft_mse': nft_mses,\n",
    "    'closest_baseline_mse': closest_baseline_mses,\n",
    "    'improvement_percentage': improvements\n",
    "})\n",
    "\n",
    "# Calculate the correlation\n",
    "correlation = df_results.groupby('dataset').apply(lambda x: x['predictive_range_percentage'].corr(x['improvement_percentage']))\n",
    "print(\"Correlation between the percentage of improvement and the percentage of the predictive range\")\n",
    "print(correlation)\n",
    "\n",
    "# Save the detailed results to an Excel file\n",
    "df_results.to_excel(\"precentage_results.xlsx\", index=False)\n",
    "print(\"Results saved to results.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETT_mse_data = {\n",
    "    'ETTh1': {\n",
    "        'horizons': [1, 10, 24, 48, 96, 192, 336, 720],\n",
    "        'models': {\n",
    "            'nft': [0.07, 0.14, 0.18, 0.25, 0.39, 0.43, 0.44, 0.46],\n",
    "            'timesnet': [0.15, 0.29, 0.33, 0.37, 0.4, 0.44, 0.49, 0.52],\n",
    "            'patchtst': [0.1, 0.25, 0.29, 0.34, 0.37, 0.41, 0.42, 0.45],\n",
    "            'dlinear': [0.12, 0.28, 0.31, 0.35, 0.37, 0.41, 0.44, 0.47],\n",
    "            'Autoformer': [0.17, 0.4, 0.4, 0.427, 0.43, 0.46, 0.49, 0.52],\n",
    "            'tcn': [0.14, 0.29, 0.43, 0.46, 0.46, 0.54, 0.53, 0.58],\n",
    "            'nbeats': [0.11, 0.29, 0.33, 0.47, 0.71, 0.78, 0.79, 0.84]\n",
    "        }\n",
    "    },\n",
    "    'ETTh2': {\n",
    "        'horizons': [1, 10, 24, 48, 96, 192, 336, 720],\n",
    "        'models': {\n",
    "            'nft': [0.06, 0.1, 0.13, 0.16, 0.19, 0.23, 0.26, 0.34],\n",
    "            'timesnet': [0.1, 0.15, 0.2, 0.25, 0.25, 0.4, 0.45, 0.46],\n",
    "            'patchtst': [0.06, 0.12, 0.17, 0.22, 0.274, 0.34, 0.33, 0.38],\n",
    "            'dlinear': [0.11, 0.13, 0.18, 0.25, 0.289, 0.38, 0.45, 0.61],\n",
    "            'Autoformer': [0.12, 0.21, 0.26, 0.32, 0.332, 0.43, 0.48, 0.45],\n",
    "            'tcn': [0.06, 0.11, 0.15, 0.19, 0.24, 0.48, 0.54, 0.55],\n",
    "            'nbeats': [0.14, 0.18, 0.24, 0.41, 0.5, 1.2, 1.35, 1.73]\n",
    "        }\n",
    "    },\n",
    "    'ETTm1': {\n",
    "        'horizons': [1, 10, 24, 48, 96, 192, 336, 720],\n",
    "        'models': {\n",
    "            'nft': [0.04, 0.07, 0.11, 0.14, 0.16, 0.24, 0.37, 0.43],\n",
    "            'timesnet': [0.05, 0.13, 0.17, 0.24, 0.26, 0.37, 0.41, 0.48],\n",
    "            'patchtst': [0.05, 0.12, 0.21, 0.28, 0.293, 0.33, 0.37, 0.42],\n",
    "            'dlinear': [0.05, 0.14, 0.24, 0.31, 0.299, 0.34, 0.37, 0.43],\n",
    "            'Autoformer': [0.09, 0.24, 0.4, 0.56, 0.51, 0.51, 0.51, 0.53],\n",
    "            'tcn': [0.04, 0.09, 0.14, 0.19, 0.23, 0.47, 0.45, 0.54],\n",
    "            'nbeats': [0.05, 0.13, 0.23, 0.3, 0.35, 0.5, 0.58, 0.79]\n",
    "        }\n",
    "    },\n",
    "    'ETTm2': {\n",
    "        'horizons': [1, 10, 24, 48, 96, 192, 336, 720],\n",
    "        'models': {\n",
    "            'nft': [0.03, 0.05, 0.07, 0.1, 0.13, 0.17, 0.22, 0.26],\n",
    "            'timesnet': [0.04, 0.07, 0.11, 0.14, 0.16, 0.25, 0.32, 0.41],\n",
    "            'patchtst': [0.03, 0.07, 0.1, 0.14, 0.166, 0.22, 0.27, 0.36],\n",
    "            'dlinear': [0.03, 0.07, 0.11, 0.15, 0.167, 0.22, 0.28, 0.40],\n",
    "            'Autoformer': [0.07, 0.1, 0.14, 0.165, 0.205, 0.28, 0.34, 0.41],\n",
    "            'tcn': [0.03, 0.06, 0.09, 0.12, 0.16, 0.3, 0.34, 0.42],\n",
    "            'nbeats': [0.04, 0.07, 0.14, 0.18, 0.26, 0.59, 0.81, 1.00]\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Calculate_percentage_MSE_reduction(ETT_mse_data)\n",
    "Calculate_average_percentage_MSE(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETT_mse_data = {\n",
    "    'ETTh1': {\n",
    "        'horizons': [1, 10, 24, 48, 96, 192, 336, 720],\n",
    "        'models': {\n",
    "            'nft': [0.07, 0.14, 0.18, 0.25, 0.39, 0.43, 0.44, 0.46],\n",
    "            'timesnet': [0.15, 0.29, 0.33, 0.37, 0.38, 0.44, 0.49, 0.52],\n",
    "            'patchtst': [0.1, 0.25, 0.29, 0.34, 0.37, 0.41, 0.42, 0.45],\n",
    "            'dlinear': [0.12, 0.28, 0.31, 0.35, 0.37, 0.41, 0.44, 0.47],\n",
    "            'Autoformer': [0.17, 0.4, 0.4, 0.427, 0.44, 0.46, 0.49, 0.52],\n",
    "            'tcn': [0.14, 0.29, 0.43, 0.46, 0.46, 0.54, 0.53, 0.58],\n",
    "            'nbeats': [0.11, 0.29, 0.33, 0.47, 0.71, 0.78, 0.79, 0.84]\n",
    "        }\n",
    "    },\n",
    "    'ETTh1_long': {\n",
    "        'horizons': [96, 192, 336, 720],\n",
    "        'models': {\n",
    "            'nft': [0.39, 0.43, 0.44, 0.46],\n",
    "            'timesnet': [0.4, 0.44, 0.49, 0.52],\n",
    "            'patchtst': [0.37, 0.41, 0.42, 0.45],\n",
    "            'dlinear': [0.37, 0.41, 0.44, 0.47],\n",
    "            'Autoformer': [0.43, 0.46, 0.49, 0.52],\n",
    "            'tcn': [0.46, 0.54, 0.53, 0.58],\n",
    "            'nbeats': [0.71, 0.78, 0.79, 0.84]\n",
    "        }\n",
    "    },\n",
    "    'ETTh2': {\n",
    "        'horizons': [1, 10, 24, 48, 96, 192, 336, 720],\n",
    "        'models': {\n",
    "            'nft': [0.06, 0.1, 0.13, 0.16, 0.19, 0.23, 0.26, 0.34],\n",
    "            'timesnet': [0.1, 0.15, 0.2, 0.25, 0.34, 0.4, 0.45, 0.46],\n",
    "            'patchtst': [0.06, 0.12, 0.17, 0.22, 0.274, 0.34, 0.33, 0.38],\n",
    "            'dlinear': [0.11, 0.13, 0.18, 0.25, 0.289, 0.38, 0.45, 0.61],\n",
    "            'Autoformer': [0.12, 0.21, 0.26, 0.32, 0.332, 0.43, 0.48, 0.45],\n",
    "            'tcn': [0.06, 0.11, 0.15, 0.19, 0.24, 0.48, 0.54, 0.55],\n",
    "            'nbeats': [0.14, 0.18, 0.24, 0.41, 1.65, 1.95, 1.75, 1.73]\n",
    "        }\n",
    "    },\n",
    "    'ETTm1': {\n",
    "        'horizons': [1, 10, 24, 48, 96, 192, 336, 720],\n",
    "        'models': {\n",
    "            'nft': [0.04, 0.07, 0.11, 0.14, 0.16, 0.24, 0.37, 0.43],\n",
    "            'timesnet': [0.05, 0.13, 0.20, 0.29, 0.34, 0.37, 0.41, 0.48],\n",
    "            'patchtst': [0.05, 0.12, 0.21, 0.28, 0.293, 0.33, 0.37, 0.42],\n",
    "            'dlinear': [0.05, 0.14, 0.24, 0.31, 0.299, 0.34, 0.37, 0.43],\n",
    "            'Autoformer': [0.09, 0.24, 0.4, 0.56, 0.51, 0.51, 0.51, 0.53],\n",
    "            'tcn': [0.04, 0.09, 0.14, 0.19, 0.18, 0.47, 0.45, 0.54],\n",
    "            'nbeats': [0.05, 0.13, 0.23, 0.3, 0.35, 0.5, 0.58, 0.79]\n",
    "        }\n",
    "    },\n",
    "    'ETTm2': {\n",
    "        'horizons': [1, 10, 24, 48, 96, 192, 336, 720],\n",
    "        'models': {\n",
    "            'nft': [0.03, 0.05, 0.07, 0.1, 0.13, 0.17, 0.22, 0.26],\n",
    "            'timesnet': [0.04, 0.07, 0.11, 0.15, 0.16, 0.25, 0.32, 0.41],\n",
    "            'patchtst': [0.03, 0.07, 0.1, 0.14, 0.166, 0.22, 0.27, 0.36],\n",
    "            'dlinear': [0.03, 0.07, 0.11, 0.15, 0.167, 0.22, 0.28, 0.40],\n",
    "            'Autoformer': [0.07, 0.1, 0.14, 0.165, 0.205, 0.28, 0.34, 0.41],\n",
    "            'tcn': [0.03, 0.06, 0.09, 0.12, 0.16, 0.3, 0.34, 0.42],\n",
    "            'nbeats': [0.04, 0.07, 0.14, 0.18, 0.26, 0.59, 0.81, 1.00]\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_data_long = {\n",
    "'Traffic long': {\n",
    "    'horizons': [32, 48],\n",
    "    'models': {\n",
    "        'nft': [0.26, 0.4],\n",
    "        'timesnet': [0.62, 0.62],\n",
    "        'patchtst': [0.63, 0.66],\n",
    "        'dlinear': [0.67, 0.72],\n",
    "        'Autoformer': [0.67, 0.65],\n",
    "        'tcn': [0.49, 0.51]\n",
    "    }\n",
    "},\n",
    "    'Weather long': {\n",
    "        'horizons': [60, 90, 120, 180],\n",
    "        'models': {\n",
    "            'nft': [0.26, 0.27, 0.28, 0.27],\n",
    "            'timesnet': [0.37, 0.37, 0.38, 0.37],\n",
    "            'patchtst': [0.40, 0.42, 0.42, 0.43],\n",
    "            'dlinear': [0.37, 0.40, 0.41, 0.40],\n",
    "            'Autoformer': [0.42, 0.44, 0.46, 0.47],\n",
    "            'tcn': [0.36, 0.33, 0.33, 0.35]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'Electricity long': {\n",
    "        'horizons': [32],\n",
    "        'models': {\n",
    "            'nft': [0.13],\n",
    "            'timesnet': [0.17],\n",
    "            'patchtst': [0.17],\n",
    "            'dlinear': [0.18],\n",
    "            'Autoformer': [0.60],\n",
    "            'tcn': [0.44]\n",
    "        }\n",
    "    },\n",
    "    'Exchange Rate long': {\n",
    "    'horizons': [96, 192, 336, 720],\n",
    "    'models': {\n",
    "        'nft': [0.02, 0.03, 0.04, 0.05],\n",
    "        'timesnet': [0.107, 0.226, 0.367, 0.964],\n",
    "        'patchtst': [0.09, 0.18, 0.4, 0.94],\n",
    "        'dlinear': [0.088, 0.176, 0.313, 0.839],\n",
    "        'Autoformer': [0.197, 0.300, 0.509, 1.447],\n",
    "        'tcn': [0.09, 0.17, 0.24, 0.41]\n",
    "    }\n",
    "    },\n",
    "    'ILI long': {\n",
    "        'horizons': [36, 48, 60],\n",
    "        'models': {\n",
    "            'nft': [0.31, 0.35, 0.42],\n",
    "            'timesnet': [1.972, 2.238, 2.027],\n",
    "            'patchtst': [1.579, 1.553, 1.470],\n",
    "            'dlinear': [1.963, 2.13, 2.368],\n",
    "            'Autoformer': [3.103, 2.669, 2.770],\n",
    "            'tcn': [0.94, 1.01, 1.03]\n",
    "        }\n",
    "    },\n",
    "    'Air Quality long': {\n",
    "        'horizons': [20, 25, 30],\n",
    "        'models': {\n",
    "            'nft': [0.35, 0.38, 0.4],\n",
    "            'timesnet': [0.85, 0.92, 0.96],\n",
    "            'patchtst': [0.75, 0.78, 0.83],\n",
    "            'dlinear': [0.87, 0.93, 1.01],\n",
    "            'Autoformer': [0.98, 1.03, 0.99],\n",
    "            'tcn': [0.69, 0.68, 0.69]\n",
    "        }\n",
    "    },\n",
    "   'ECG (600) long': {\n",
    "        'horizons': [20, 25, 30],\n",
    "        'models': {\n",
    "            'nft': [0.15, 0.15, 0.1],\n",
    "            'timesnet': [0.15,0.15,0.1],\n",
    "            'patchtst': [0.15 ,0.15 ,0.18],\n",
    "            'dlinear': [0.14 ,0.16 ,0.19],\n",
    "            'Autoformer': [0.46 ,0.44 ,0.45],\n",
    "            'tcn': [0.16 ,0.18 ,0.21]\n",
    "        }\n",
    "    },\n",
    "   'ECG long': {\n",
    "        'horizons': [50, 100],\n",
    "        'models': {\n",
    "            'nft': [0.14, 0.18],\n",
    "            'timesnet': [1.16 ,1.09],\n",
    "            'patchtst': [0.94 ,0.98],\n",
    "            'dlinear': [0.66 ,0.69],\n",
    "            'Autoformer': [1.15 ,1.26],\n",
    "            'tcn': [0.70 ,0.81]\n",
    "        }\n",
    "    },\n",
    "    'EEG long': {\n",
    "        'horizons': [25],\n",
    "        'models': {\n",
    "            'nft': [0.16],\n",
    "            'timesnet': [0.44],\n",
    "            'patchtst': [0.42],\n",
    "            'dlinear': [0.435],\n",
    "            'Autoformer': [0.62],\n",
    "            'tcn': [0.39]\n",
    "        }\n",
    "    },\n",
    "    'Chorales long': {\n",
    "        'horizons': [4, 5],\n",
    "        'models': {\n",
    "            'nft': [0.23, 0.22],\n",
    "            'timesnet': [0.31, 0.32],\n",
    "            'patchtst': [0.30, 0.31],\n",
    "            'dlinear': [0.37, 0.36],\n",
    "            'Autoformer': [0.26, 0.24],\n",
    "            'tcn': [0.37, 0.56]\n",
    "        }\n",
    "    },\n",
    "    'ETTh1 long': {\n",
    "        'horizons': [48, 96],\n",
    "        'models': {\n",
    "            'nft': [0.25, 0.39],\n",
    "            'timesnet': [0.37, 0.4],\n",
    "            'patchtst': [0.34, 0.37],\n",
    "            'dlinear': [0.35, 0.37],\n",
    "            'Autoformer': [0.427, 0.43],\n",
    "            'tcn': [0.46, 0.46],\n",
    "            'nbeats': [0.47, 0.71]\n",
    "        }\n",
    "    },\n",
    "    'ETTh2 long': {\n",
    "        'horizons': [48, 96],\n",
    "        'models': {\n",
    "            'nft': [0.16, 0.19],\n",
    "            'timesnet': [0.25, 0.25],\n",
    "            'patchtst': [0.22, 0.274],\n",
    "            'dlinear': [0.25, 0.289],\n",
    "            'Autoformer': [0.32, 0.332],\n",
    "            'tcn': [0.19, 0.24],\n",
    "            'nbeats': [0.41, 0.5]\n",
    "        }\n",
    "    },\n",
    "   'ETTm1 long': {\n",
    "        'horizons': [48, 96],\n",
    "        'models': {\n",
    "            'nft': [0.14, 0.16],\n",
    "            'timesnet': [0.24, 0.26],\n",
    "            'patchtst': [0.28, 0.293],\n",
    "            'dlinear': [0.31, 0.299],\n",
    "            'Autoformer': [0.56, 0.51],\n",
    "            'tcn': [0.19, 0.23],\n",
    "            'nbeats': [0.3, 0.35]\n",
    "        }\n",
    "    },\n",
    "    'ETTm2 long': {\n",
    "        'horizons': [48, 96],\n",
    "        'models': {\n",
    "            'nft': [0.1, 0.13],\n",
    "            'timesnet': [0.14, 0.16],\n",
    "            'patchtst': [0.14, 0.166],\n",
    "            'dlinear': [0.15, 0.167],\n",
    "            'Autoformer': [0.165, 0.205],\n",
    "            'tcn': [0.12, 0.16],\n",
    "            'nbeats': [0.18, 0.26]\n",
    "        }\n",
    "    }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_data_sort = {\n",
    "'Traffic sort': {\n",
    "    'horizons': [1, 16],\n",
    "    'models': {\n",
    "        'nft': [0.09, 0.18],\n",
    "        'timesnet': [0.53, 0.58],\n",
    "        'patchtst': [0.26, 0.58],\n",
    "        'dlinear': [0.4, 0.63],\n",
    "        'Autoformer': [0.5, 0.56],\n",
    "        'tcn': [0.1, 0.46]\n",
    "    }\n",
    "},\n",
    "'Weather sort': {\n",
    "        'horizons': [1, 7, 15, 30],\n",
    "        'models': {\n",
    "            'nft': [0.19, 0.24, 0.24, 0.25],\n",
    "            'timesnet': [0.33, 0.35, 0.44, 0.36],\n",
    "            'patchtst': [0.34, 0.36, 0.37, 0.38],\n",
    "            'dlinear': [0.29, 0.32, 0.33, 0.35],\n",
    "            'Autoformer': [0.37, 0.37, 0.37, 0.38],\n",
    "            'tcn': [0.25, 0.29, 0.30, 0.32]\n",
    "        }\n",
    "    },\n",
    "\n",
    "'Electricity sort': {\n",
    "        'horizons': [1, 16],\n",
    "        'models': {\n",
    "            'nft': [0.04, 0.08],\n",
    "            'timesnet': [0.14, 0.16],\n",
    "            'patchtst': [0.09, 0.20],\n",
    "            'dlinear': [0.07, 0.16],\n",
    "            'Autoformer': [0.20, 0.40],\n",
    "            'tcn': [0.05, 0.32]\n",
    "        }\n",
    "    },\n",
    "'Exchange Rate sort': {\n",
    "        'horizons': [1, 16, 32, 48],\n",
    "        'models': {\n",
    "            'nft': [0.0001, 0.01, 0.02, 0.02],\n",
    "            'timesnet': [0.05, 0.18, 0.15, 0.11],\n",
    "            'patchtst': [0.01, 0.02, 0.03, 0.04],\n",
    "            'dlinear': [0.01, 0.02, 0.03, 0.05],\n",
    "            'Autoformer': [0.04, 0.04, 0.06, 0.09],\n",
    "            'tcn': [0.2, 0.11, 0.04, 0.06]\n",
    "        }\n",
    "    },\n",
    "'ILI sort': {\n",
    "        'horizons': [1, 12, 24],\n",
    "        'models': {\n",
    "            'nft': [0.05, 0.18, 0.25],\n",
    "            'timesnet': [0.64, 1.01, 2.317],\n",
    "            'patchtst': [0.24, 1.25, 1.319],\n",
    "            'dlinear': [2.32, 2.83, 2.215],\n",
    "            'Autoformer': [0.84, 2.47, 3.483],\n",
    "            'tcn': [0.86, 0.98, 0.95]\n",
    "        }\n",
    "    },\n",
    "'Air Quality sort': {\n",
    "        'horizons': [1, 5, 10, 15],\n",
    "        'models': {\n",
    "            'nft': [0.13, 0.21, 0.28, 0.31],\n",
    "            'timesnet': [0.74, 0.81, 0.85, 0.87],\n",
    "            'patchtst': [0.22, 0.58, 0.68, 0.72],\n",
    "            'dlinear': [0.39, 0.69, 0.78, 0.83],\n",
    "            'Autoformer': [0.72, 0.86, 0.97, 1.02],\n",
    "            'tcn': [0.21, 0.42, 0.53, 0.64]\n",
    "        }\n",
    "    },\n",
    "'ECG (600) sort': {\n",
    "        'horizons': [1, 5, 10, 15],\n",
    "        'models': {\n",
    "            'nft': [0.02, 0.08, 0.07, 0.1],\n",
    "            'timesnet': [0.02,0.08,0.07,0.1],\n",
    "            'patchtst': [0.02 ,0.05 ,0.08 ,0.1],\n",
    "            'dlinear': [0.02 ,0.06 ,0.08 ,0.11],\n",
    "            'Autoformer': [0.8,0.49,0.38, 0.41],\n",
    "            'tcn': [ 0.03 ,0.07 ,0.1 ,0.12]\n",
    "        }\n",
    "    },\n",
    "'ECG sort': {\n",
    "        'horizons': [1, 10, 25],\n",
    "        'models': {\n",
    "            'nft': [0.04, 0.08, 0.12],\n",
    "            'timesnet': [0.31 ,0.49 ,0.625],\n",
    "            'patchtst': [0.34 ,0.57 ,0.77],\n",
    "            'dlinear': [0.35 ,0.46 ,0.58],\n",
    "            'Autoformer': [1.62 ,1.13 ,1.5],\n",
    "            'tcn': [0.16 ,0.42 ,0.56]\n",
    "        }\n",
    "    },\n",
    "'EEG sort': {\n",
    "        'horizons': [1, 10],\n",
    "        'models': {\n",
    "            'nft': [0.03, 0.1],\n",
    "            'timesnet': [0.16, 0.35],\n",
    "            'patchtst': [0.15, 0.345],\n",
    "            'dlinear': [0.135, 0.365],\n",
    "            'Autoformer': [0.63, 0.545],\n",
    "            'tcn': [0.09, 0.32],\n",
    "            'tcn': [0.09, 0.32],\n",
    "        }\n",
    "    },\n",
    "'Chorales sort': {\n",
    "        'horizons': [1, 2, 3],\n",
    "        'models': {\n",
    "            'nft': [0.15, 0.18, 0.20],\n",
    "            'timesnet': [0.29, 0.3, 0.31],\n",
    "            'patchtst': [0.34, 0.27, 0.29],\n",
    "            'dlinear': [0.29, 0.29, 0.40],\n",
    "            'Autoformer': [0.30, 0.25, 0.25],\n",
    "            'tcn': [0.27, 0.63, 0.62]\n",
    "        }\n",
    "    },\n",
    " 'ETTh1 sort': {\n",
    "        'horizons': [1, 10, 24],\n",
    "        'models': {\n",
    "            'nft': [0.07, 0.14, 0.18],\n",
    "            'timesnet': [0.15, 0.29, 0.33],\n",
    "            'patchtst': [0.1, 0.25, 0.29],\n",
    "            'dlinear': [0.12, 0.28, 0.31],\n",
    "            'Autoformer': [0.17, 0.4, 0.4],\n",
    "            'tcn': [0.14, 0.29, 0.43],\n",
    "            'nbeats': [0.11, 0.29, 0.33]\n",
    "        }\n",
    "    },\n",
    "'ETTh2 short': {\n",
    "        'horizons': [1, 10, 24],\n",
    "        'models': {\n",
    "            'nft': [0.06, 0.1, 0.13],\n",
    "            'timesnet': [0.1, 0.15, 0.2],\n",
    "            'patchtst': [0.06, 0.12, 0.17],\n",
    "            'dlinear': [0.11, 0.13, 0.18],\n",
    "            'Autoformer': [0.12, 0.21, 0.26],\n",
    "            'tcn': [0.06, 0.11, 0.15],\n",
    "            'nbeats': [0.14, 0.18, 0.24]\n",
    "        }\n",
    "    },\n",
    "'ETTm1 short': {\n",
    "        'horizons': [1, 10, 24],\n",
    "        'models': {\n",
    "            'nft': [0.04, 0.07, 0.11],\n",
    "            'timesnet': [0.05, 0.13, 0.17],\n",
    "            'patchtst': [0.05, 0.12, 0.21],\n",
    "            'dlinear': [0.05, 0.14, 0.24],\n",
    "            'Autoformer': [0.09, 0.24, 0.4],\n",
    "            'tcn': [0.04, 0.09, 0.14],\n",
    "            'nbeats': [0.05, 0.13, 0.23]\n",
    "        }\n",
    "    },\n",
    "'ETTm2 short': {\n",
    "        'horizons': [1, 10, 24],\n",
    "        'models': {\n",
    "            'nft': [0.03, 0.05, 0.07],\n",
    "            'timesnet': [0.04, 0.07, 0.11],\n",
    "            'patchtst': [0.03, 0.07, 0.1],\n",
    "            'dlinear': [0.03, 0.07, 0.11],\n",
    "            'Autoformer': [0.07, 0.1, 0.14],\n",
    "            'tcn': [0.03, 0.06, 0.09],\n",
    "            'nbeats': [0.04, 0.07, 0.14]\n",
    "        }\n",
    "    },\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Calculate_percentage_MSE_reduction(ETT_mse_data)\n",
    "Calculate_average_percentage_MSE(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Calculate_percentage_MSE_reduction(mse_data_long)\n",
    "Calculate_average_percentage_MSE(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "t_stat_dict, p_value_dict = Calulate_t_test(mse_data)  # Assume 'data' is your dataset dictionary\n",
    "avg_t_stat, avg_p_value, overall_avg_t_stat, overall_avg_p_value = calculate_average_t_p(t_stat_dict, p_value_dict)\n",
    "print(avg_t_stat)\n",
    "print(avg_p_value)\n",
    "print(overall_avg_t_stat)\n",
    "print(overall_avg_p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_ablation_lstm, results_ablation_fc = Calculate_percentage_MSE_reduction_ablation(data_ablation_mse)\n",
    "average_mse_lstm_results = Calculate_average_percentage_MSE(results_ablation_lstm)\n",
    "average_mse_fc_results = Calculate_average_percentage_MSE(results_ablation_fc)\n",
    "average_dict_values(average_mse_lstm_results)\n",
    "average_dict_values(average_mse_fc_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECL: Average Improvement = 12.90%\n",
      "Exchange: Average Improvement = -0.02%\n",
      "Traffic: Average Improvement = 6.06%\n",
      "ETTm1: Average Improvement = 1.82%\n",
      "Weather: Average Improvement = 4.16%\n",
      "ECG: Average Improvement = 4.24%\n",
      "EEG: Average Improvement = 9.99%\n",
      "Chorales: Average Improvement = 5.76%\n",
      "Air_Quality: Average Improvement = 4.02%\n",
      "\n",
      "T-Test Results:\n",
      "ECL: t-stat = -2.2188, p-val = 1.5673e-01\n",
      "Exchange: t-stat = 0.7049, p-val = 5.3167e-01\n",
      "Traffic: t-stat = -1.7998, p-val = 1.6971e-01\n",
      "ETTm1: t-stat = 0.0128, p-val = 9.9055e-01\n",
      "Weather: t-stat = -2.6433, p-val = 7.7434e-02\n",
      "ECG: t-stat = -3.1539, p-val = 5.1109e-02\n",
      "EEG: t-stat = -2.9441, p-val = 9.8600e-02\n",
      "Chorales: t-stat = -0.9506, p-val = 4.1194e-01\n",
      "Air_Quality: t-stat = -1.2778, p-val = 2.9123e-01\n",
      "\n",
      "Average t-statistic: -1.5856\n",
      "Average p-value: 3.0878e-01\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Input data\n",
    "data = {\n",
    "    \"ECL\": {\n",
    "        1: [0.051, 0.054],\n",
    "        16: [0.080, 0.100],\n",
    "        32: [0.141, 0.148],\n",
    "    },\n",
    "    \"Exchange\": {\n",
    "        96: [0.060, 0.081],\n",
    "        192: [0.164, 0.176],\n",
    "        336: [0.422, 0.313],\n",
    "        720: [0.670, 0.668],\n",
    "    },\n",
    "    \"Traffic\": {\n",
    "        1: [0.200, 0.217],\n",
    "        16: [0.320, 0.381],\n",
    "        32: [0.390, 0.418],\n",
    "        48: [0.400, 0.430],\n",
    "    },\n",
    "    \"ETTm1\": {\n",
    "        48: [0.270, 0.280],\n",
    "        96: [0.241, 0.287],\n",
    "        192: [0.320, 0.327],\n",
    "        336: [0.410, 0.355],\n",
    "    },\n",
    "    \"Weather\": {\n",
    "        15: [0.301, 0.330],\n",
    "        30: [0.313, 0.350],\n",
    "        60: [0.325, 0.370],\n",
    "        90: [0.330, 0.370],\n",
    "    },\n",
    "    \"ECG\": {\n",
    "        1: [0.080, 0.085],\n",
    "        10: [0.172, 0.247],\n",
    "        25: [0.258, 0.380],\n",
    "        100: [0.393, 0.460],\n",
    "    },\n",
    "    \"EEG\": {\n",
    "        1: [0.075, 0.108],\n",
    "        10: [0.159, 0.320],\n",
    "        25: [0.238, 0.406],\n",
    "    },\n",
    "    \"Chorales\": {\n",
    "        1: [0.223, 0.270],\n",
    "        2: [0.242, 0.25], \n",
    "        3: [0.259, 0.250],\n",
    "        4: [0.260, 0.265],\n",
    "    },\n",
    "    \"Air_Quality\": {\n",
    "        5: [0.513, 0.580],\n",
    "        10: [0.641, 0.655],\n",
    "        15: [0.717, 0.72],\n",
    "        25: [0.779, 0.78],\n",
    "        },\n",
    "    }\n",
    "\n",
    "# Conduct t-tests and calculate improvement percentages\n",
    "results = {}\n",
    "\n",
    "for dataset, entries in data.items():\n",
    "    improvements = []\n",
    "    values1 = []\n",
    "    values2 = []\n",
    "\n",
    "    for key, values in entries.items():\n",
    "        val1, val2 = values\n",
    "        improvement = ((val2 - val1) / val1) * 100\n",
    "        improvements.append(improvement)\n",
    "        values1.append(val1)\n",
    "        values2.append(val2)\n",
    "\n",
    "    # Average improvement\n",
    "    average_improvement = sum(improvements) / len(improvements)\n",
    "    results[dataset] = average_improvement\n",
    "\n",
    "\n",
    "# Print the results\n",
    "for dataset, avg_improvement in results.items():\n",
    "    print(f\"{dataset}: Average Improvement = {avg_improvement:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Summary:\n",
      "ECL: Average Improvement = 12.90%\n",
      "Exchange: Average Improvement = -0.02%\n",
      "Traffic: Average Improvement = 6.06%\n",
      "ETTm1: Average Improvement = 1.82%\n",
      "Weather: Average Improvement = 4.16%\n",
      "ECG: Average Improvement = 4.24%\n",
      "EEG: Average Improvement = 9.99%\n",
      "Chorales: Average Improvement = 5.76%\n",
      "Air_Quality: Average Improvement = 4.02%\n",
      "\n",
      "Paired t-Test Results:\n",
      "ECL: t-stat = -2.2188, p-val = 1.5673e-01\n",
      "Exchange: t-stat = 0.7049, p-val = 5.3167e-01\n",
      "Traffic: t-stat = -1.7998, p-val = 1.6971e-01\n",
      "ETTm1: t-stat = 0.0128, p-val = 9.9055e-01\n",
      "Weather: t-stat = -2.6433, p-val = 7.7434e-02\n",
      "ECG: t-stat = -3.1539, p-val = 5.1109e-02\n",
      "EEG: t-stat = -2.9441, p-val = 9.8600e-02\n",
      "Chorales: t-stat = -0.9506, p-val = 4.1194e-01\n",
      "Air_Quality: t-stat = -1.2778, p-val = 2.9123e-01\n",
      "\n",
      "Wilcoxon Test Results:\n",
      "ECL: w-stat = 0.0000, p-val = 2.5000e-01\n",
      "Exchange: w-stat = 5.0000, p-val = 1.0000e+00\n",
      "Traffic: w-stat = 0.0000, p-val = 1.2500e-01\n",
      "ETTm1: w-stat = 4.0000, p-val = 8.7500e-01\n",
      "Weather: w-stat = 0.0000, p-val = 1.2500e-01\n",
      "ECG: w-stat = 0.0000, p-val = 1.2500e-01\n",
      "EEG: w-stat = 0.0000, p-val = 2.5000e-01\n",
      "Chorales: w-stat = 1.5000, p-val = 4.1422e-01\n",
      "Air_Quality: w-stat = 0.0000, p-val = 1.7971e-01\n",
      "\n",
      "Fisher's Exact Test Results:\n",
      "ECL: p-val = 1.0000e+00\n",
      "Exchange: Fisher's test not applicable.\n",
      "Traffic: Fisher's test not applicable.\n",
      "ETTm1: Fisher's test not applicable.\n",
      "Weather: Fisher's test not applicable.\n",
      "ECG: Fisher's test not applicable.\n",
      "EEG: p-val = 1.0000e+00\n",
      "Chorales: p-val = 1.0000e+00\n",
      "Air_Quality: Fisher's test not applicable.\n",
      "\n",
      "Average t-statistic: -1.5856\n",
      "Average t-test p-value: 3.0878e-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noam.koren/miniconda3/envs/new_env/lib/python3.9/site-packages/scipy/stats/_wilcoxon.py:199: UserWarning: Sample size too small for normal approximation.\n",
      "  temp = _wilcoxon_iv(x, y, zero_method, correction, alternative, method, axis)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_rel, wilcoxon, fisher_exact\n",
    "\n",
    "# Function to compute Fisher's exact test with approximated contingency table\n",
    "def fisher_test(values1, values2):\n",
    "    contingency_table = [\n",
    "        [int(sum(values1) * 10), int(sum(values2) * 10)],  # Approximation for counts\n",
    "        [int((1 - sum(values1)) * 10), int((1 - sum(values2)) * 10)]\n",
    "    ]\n",
    "    try:\n",
    "        _, p_val = fisher_exact(contingency_table)\n",
    "    except ValueError:\n",
    "        p_val = None  # Invalid table\n",
    "    return p_val\n",
    "\n",
    "# Conduct statistical tests\n",
    "results = {}\n",
    "t_test_results = {}\n",
    "wilcoxon_results = {}\n",
    "fisher_results = {}\n",
    "\n",
    "for dataset, entries in data.items():\n",
    "    improvements = []\n",
    "    values1 = []\n",
    "    values2 = []\n",
    "\n",
    "    for key, values in entries.items():\n",
    "        val1, val2 = values\n",
    "        improvement = ((val2 - val1) / val1) * 100\n",
    "        improvements.append(improvement)\n",
    "        values1.append(val1)\n",
    "        values2.append(val2)\n",
    "\n",
    "    # Average improvement\n",
    "    average_improvement = sum(improvements) / len(improvements)\n",
    "    results[dataset] = average_improvement\n",
    "\n",
    "    # Perform paired t-test\n",
    "    t_stat, t_p_val = ttest_rel(values1, values2)\n",
    "    t_test_results[dataset] = {\"t_stat\": t_stat, \"p_val\": t_p_val}\n",
    "\n",
    "    # Perform Wilcoxon signed-rank test\n",
    "    try:\n",
    "        w_stat, w_p_val = wilcoxon(values1, values2)\n",
    "        wilcoxon_results[dataset] = {\"w_stat\": w_stat, \"p_val\": w_p_val}\n",
    "    except ValueError:\n",
    "        wilcoxon_results[dataset] = {\"w_stat\": None, \"p_val\": None}\n",
    "\n",
    "    # Perform Fisher's exact test\n",
    "    fisher_p_val = fisher_test(values1, values2)\n",
    "    fisher_results[dataset] = {\"p_val\": fisher_p_val}\n",
    "\n",
    "# Print the results\n",
    "print(\"Results Summary:\")\n",
    "for dataset, avg_improvement in results.items():\n",
    "    print(f\"{dataset}: Average Improvement = {avg_improvement:.2f}%\")\n",
    "\n",
    "print(\"\\nPaired t-Test Results:\")\n",
    "for dataset, stats in t_test_results.items():\n",
    "    print(f\"{dataset}: t-stat = {stats['t_stat']:.4f}, p-val = {stats['p_val']:.4e}\")\n",
    "\n",
    "print(\"\\nWilcoxon Test Results:\")\n",
    "for dataset, stats in wilcoxon_results.items():\n",
    "    if stats[\"w_stat\"] is not None:\n",
    "        print(f\"{dataset}: w-stat = {stats['w_stat']:.4f}, p-val = {stats['p_val']:.4e}\")\n",
    "    else:\n",
    "        print(f\"{dataset}: Wilcoxon test not applicable.\")\n",
    "\n",
    "print(\"\\nFisher's Exact Test Results:\")\n",
    "for dataset, stats in fisher_results.items():\n",
    "    if stats[\"p_val\"] is not None:\n",
    "        print(f\"{dataset}: p-val = {stats['p_val']:.4e}\")\n",
    "    else:\n",
    "        print(f\"{dataset}: Fisher's test not applicable.\")\n",
    "\n",
    "# Calculate averages for t-stat and p-values\n",
    "t_stats = [stats[\"t_stat\"] for stats in t_test_results.values()]\n",
    "t_p_vals = [stats[\"p_val\"] for stats in t_test_results.values()]\n",
    "avg_t_stat = sum(t_stats) / len(t_stats)\n",
    "avg_t_p_val = sum(t_p_vals) / len(t_p_vals)\n",
    "\n",
    "print(f\"\\nAverage t-statistic: {avg_t_stat:.4f}\")\n",
    "print(f\"Average t-test p-value: {avg_t_p_val:.4e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sign Test Results:\n",
      "ECL: statistic = 3, p-val = 2.5000e-01\n",
      "Exchange: statistic = 0, p-val = 1.0000e+00\n",
      "Traffic: statistic = 4, p-val = 1.2500e-01\n",
      "ETTm1: statistic = 2, p-val = 6.2500e-01\n",
      "Weather: statistic = 4, p-val = 1.2500e-01\n",
      "ECG: statistic = 4, p-val = 1.2500e-01\n",
      "EEG: statistic = 3, p-val = 2.5000e-01\n",
      "Chorales: statistic = 1, p-val = 1.0000e+00\n",
      "Air_Quality: statistic = 2, p-val = 5.0000e-01\n",
      "\n",
      "Permutation Test Results:\n",
      "ECL: observed_diff = 0.0107, p-val = 6.7700e-01\n",
      "Exchange: observed_diff = -0.0205, p-val = 9.1100e-01\n",
      "Traffic: observed_diff = 0.0192, p-val = 6.8500e-01\n",
      "ETTm1: observed_diff = -0.0003, p-val = 9.9300e-01\n",
      "Weather: observed_diff = 0.0135, p-val = 4.6400e-01\n",
      "ECG: observed_diff = 0.0105, p-val = 7.9200e-01\n",
      "EEG: observed_diff = 0.0163, p-val = 6.6700e-01\n",
      "Chorales: observed_diff = 0.0125, p-val = 4.3300e-01\n",
      "Air_Quality: observed_diff = 0.0212, p-val = 7.3500e-01\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_rel, wilcoxon, binomtest\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Function for Sign Test\n",
    "def sign_test(values1, values2):\n",
    "    differences = np.array(values2) - np.array(values1)\n",
    "    positives = np.sum(differences > 0)\n",
    "    negatives = np.sum(differences < 0)\n",
    "    total = positives + negatives\n",
    "    if total == 0:\n",
    "        return {\"statistic\": None, \"p_val\": 1.0}  # No variability in signs\n",
    "    p_val = binomtest(positives, total, 0.5).pvalue  # Test median = 0\n",
    "    return {\"statistic\": positives - negatives, \"p_val\": p_val}\n",
    "\n",
    "\n",
    "# # Function for Sign Test\n",
    "# def sign_test(values1, values2):\n",
    "#     differences = np.array(values2) - np.array(values1)\n",
    "#     positives = np.sum(differences > 0)\n",
    "#     negatives = np.sum(differences < 0)\n",
    "#     total = positives + negatives\n",
    "#     if total == 0:\n",
    "#         return {\"statistic\": None, \"p_val\": 1.0}  # No variability in signs\n",
    "#     p_val = binom_test(positives, total, 0.5)  # Test median = 0\n",
    "#     return {\"statistic\": positives - negatives, \"p_val\": p_val}\n",
    "\n",
    "# Function for Permutation Test\n",
    "def permutation_test(values1, values2, n_permutations=1000):\n",
    "    observed_diff = np.mean(values2) - np.mean(values1)\n",
    "    combined = np.array(values1 + values2)\n",
    "    count = 0\n",
    "    for _ in range(n_permutations):\n",
    "        permuted = resample(combined, replace=False)\n",
    "        perm_diff = np.mean(permuted[:len(values1)]) - np.mean(permuted[len(values1):])\n",
    "        if abs(perm_diff) >= abs(observed_diff):\n",
    "            count += 1\n",
    "    p_val = count / n_permutations\n",
    "    return {\"observed_diff\": observed_diff, \"p_val\": p_val}\n",
    "\n",
    "# Conduct alternative tests\n",
    "sign_test_results = {}\n",
    "permutation_test_results = {}\n",
    "\n",
    "for dataset, entries in data.items():\n",
    "    values1 = []\n",
    "    values2 = []\n",
    "\n",
    "    for key, values in entries.items():\n",
    "        val1, val2 = values\n",
    "        values1.append(val1)\n",
    "        values2.append(val2)\n",
    "\n",
    "    # Perform Sign Test\n",
    "    sign_test_res = sign_test(values1, values2)\n",
    "    sign_test_results[dataset] = sign_test_res\n",
    "\n",
    "    # Perform Permutation Test\n",
    "    perm_test_res = permutation_test(values1, values2)\n",
    "    permutation_test_results[dataset] = perm_test_res\n",
    "\n",
    "# Print results\n",
    "print(\"\\nSign Test Results:\")\n",
    "for dataset, stats in sign_test_results.items():\n",
    "    if stats[\"statistic\"] is not None:\n",
    "        print(f\"{dataset}: statistic = {stats['statistic']}, p-val = {stats['p_val']:.4e}\")\n",
    "    else:\n",
    "        print(f\"{dataset}: Sign test not applicable.\")\n",
    "\n",
    "print(\"\\nPermutation Test Results:\")\n",
    "for dataset, stats in permutation_test_results.items():\n",
    "    print(f\"{dataset}: observed_diff = {stats['observed_diff']:.4f}, p-val = {stats['p_val']:.4e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECL Percentage Reduction: 16.92%\n",
      "Exchange Percentage Reduction: 6.78%\n",
      "Traffic Percentage Reduction: 9.41%\n",
      "ETTm1 Percentage Reduction: 1.81%\n",
      "Weather Percentage Reduction: 5.92%\n",
      "ECG Percentage Reduction: 13.08%\n",
      "EEG Percentage Reduction: 14.21%\n",
      "Chorales Percentage Reduction: 10.53%\n",
      "Air_Quality Percentage Reduction: 17.03%\n",
      "\n",
      "ECL Times Reduction: 1.20\n",
      "Exchange Times Reduction: 1.07\n",
      "Traffic Times Reduction: 1.10\n",
      "ETTm1 Times Reduction: 1.02\n",
      "Weather Times Reduction: 1.06\n",
      "ECG Times Reduction: 1.15\n",
      "EEG Times Reduction: 1.17\n",
      "Chorales Times Reduction: 1.12\n",
      "Air_Quality Times Reduction: 1.21\n",
      "\n",
      "ECL: t-statistic = -1.4800, p-value = 2.7700e-01\n",
      "Exchange: t-statistic = -0.4249, p-value = 6.9955e-01\n",
      "Traffic: t-statistic = -3.6006, p-value = 3.6746e-02\n",
      "ETTm1: t-statistic = -0.3359, p-value = 7.5908e-01\n",
      "Weather: t-statistic = -0.6911, p-value = 5.3916e-01\n",
      "ECG: t-statistic = -1.2683, p-value = 2.9418e-01\n",
      "EEG: t-statistic = -3.6056, p-value = 6.9051e-02\n",
      "Chorales: t-statistic = -2161727821137837.0000, p-value = 2.1831e-46\n",
      "Air_Quality: t-statistic = -1.2291, p-value = 3.0666e-01\n",
      "\n",
      "ECL: Average Improvement dft = 15.92%\n",
      "Exchange: Average Improvement dft = -1.39%\n",
      "Traffic: Average Improvement dft = 9.38%\n",
      "ETTm1: Average Improvement dft = 1.85%\n",
      "Weather: Average Improvement dft = 1.70%\n",
      "ECG: Average Improvement dft = 1.71%\n",
      "EEG: Average Improvement dft = 20.23%\n",
      "Chorales: Average Improvement dft = 15.87%\n",
      "Air_Quality: Average Improvement dft = 18.46%\n"
     ]
    }
   ],
   "source": [
    " \n",
    "# ILI = {\n",
    "# # 24: [0.250, 2.262, 2.608, 1.824, 4.758, 1.970, 2.317, 1.319, 2.215, 3.483, 0.950, 4.090], \n",
    "# # 36: [0.310, 2.211, 2.572, 1.872, 3.984, 1.982, 1.972, 1.579, 1.963, 3.103, 0.940, 4.160], \n",
    "# # 48: [0.350, 2.165, 2.440, 2.390, 3.702, 1.868, 2.238, 1.553, 2.130, 2.669, 1.010, 4.330], \n",
    "# # 60: [0.420, 2.406, 2.587, 2.904, 3.971, 2.057, 2.027, 1.470, 2.368, 2.770, 1.030, 4.690]\n",
    "# }\n",
    "ECL = {\n",
    "1: [0.050, 0.065, 0.116, 0.054, 0.070, 0.827, 0.140, 0.090, 0.070, 0.200], \n",
    "16: [0.080, 0.131, 0.128, 0.123, 0.167, 0.181, 0.160, 0.200, 0.160, 0.400],\n",
    "32: [0.140, 0.156, 0.151, 0.148, 0.205, 0.197, 0.170, 0.170, 0.180, 0.600], \n",
    "} \n",
    "Exchange = {\n",
    "96:[0.070, 0.107, 0.081, 0.083, 0.086, 0.110, 0.107, 0.107, 0.088, 0.197], \n",
    "192:[0.160, 0.291, 0.178, 0.177, 0.177, 0.207, 0.226, 0.226, 0.176, 0.300], \n",
    "336:[0.420, 0.393, 0.343, 0.331, 0.331, 0.327, 0.367, 0.367, 0.313, 0.509], \n",
    "720:[0.670, 0.668, 0.893, 0.888, 0.847, 0.811, 0.964, 0.964, 0.839, 1.447], \n",
    "} \n",
    "Traffic = {\n",
    "1: [0.200, 0.217, 0.410, 0.218, 0.289, 1.372, 0.530, 0.260, 0.400, 0.500],\n",
    "16: [0.320, 0.381, 0.470, 0.397, 0.622, 0.612, 0.580, 0.580, 0.630, 0.560], \n",
    "32: [0.390, 0.418, 0.521, 0.431, 0.726, 0.640, 0.620, 0.630, 0.670, 0.670], \n",
    "48: [0.400, 0.430, 0.570, 0.450, 0.785, 0.721, 0.620, 0.660, 0.720, 0.650], \n",
    "}\n",
    "# ETTh1 = {\n",
    "# 96: [0.390, 0.405, 0.360, 0.370, 0.386, 0.371, 0.380, 0.370, 0.370, 0.430, 0.460, 0.710],\n",
    "# 192: [0.430, 0.467, 0.395, 0.412, 0.441, 0.414, 0.440, 0.410, 0.410, 0.460, 0.540, 0.780],\n",
    "# 336: [0.440, 0.514, 0.409, 0.399, 0.487, 0.442, 0.490, 0.420, 0.440, 0.490, 0.530, 0.790],\n",
    "# 720: [0.460, 0.614, 0.440, 0.472, 0.503, 0.465, 0.520, 0.450, 0.470, 0.520, 0.580, 0.840],\n",
    "# }\n",
    "# ETTh2 = {\n",
    "# 96: [0.190, 0.171, 0.263, 0.280, 0.297, 0.284, 0.340, 0.274, 0.289, 0.332, 0.240, 1.650],\n",
    "# 192: [0.230, 0.211, 0.325, 0.330, 0.380, 0.357, 0.400, 0.340, 0.380, 0.430, 0.480, 1.950],\n",
    "# 336: [.260, 0.236, 0.320, 0.317, 0.428, 0.377, 0.450, 0.330, 0.450, 0.480, 0.540, 1.750],\n",
    "# 720: [0.340, 0.297, 0.377, 0.404, 0.427, 0.439, 0.460, 0.380, 0.610, 0.450, 0.550, 1.730],\n",
    "# }\n",
    "ETTm1 = {\n",
    "48: [0.270, 0.350, 0.345, 0.330, 0.290, 0.330, 0.293, 0.280, 0.310, 0.560],\n",
    "96: [0.250, 0.327, 0.287, 0.289, 0.334, 0.302, 0.340, 0.293, 0.299, 0.510],\n",
    "192: [0.320, 0.370, 0.327, 0.328, 0.377, 0.338, 0.370, 0.330, 0.340, 0.510],\n",
    "336: [0.410, 0.402, 0.362, 0.355, 0.426, 0.373, 0.410, 0.370, 0.370, 0.510],\n",
    "# 720: [0.430, 0.453, 0.408, 0.421, 0.491, 0.420, 0.480, 0.420, 0.430, 0.530],\n",
    "}\n",
    "# ETTm2 = {\n",
    "# 96: [0.130, 0.114, 0.161, 0.169, 0.180, 0.165, 0.190, 0.166, 0.167, 0.205, 0.160, 0.260],\n",
    "# 192: [0.170, 0.141, 0.219, 0.224, 0.250, 0.222, 0.250, 0.220, 0.220, 0.280, 0.300, 0.590],\n",
    "# 336: [0.220, 0.172, 0.267, 0.275, 0.311, 0.277, 0.320, 0.270, 0.280, 0.340, 0.340, 0.810],\n",
    "# 720: [0.260, 0.219, 0.347, 0.354, 0.412, 0.371, 0.410, 0.360, 0.400, 0.410, 0.420, 1.000],\n",
    "# }\n",
    "Weather = {\n",
    "60: [0.302, 0.400, 0.390, 0.383, 0.426, 0.403, 0.370, 0.400, 0.370, 0.420],\n",
    "90: [0.342, 0.413, 0.403, 0.390, 0.436, 0.404, 0.370, 0.420, 0.400, 0.440],\n",
    "120: [0.357, 0.413, 0.323, 0.393, 0.457, 0.456, 0.380, 0.420, 0.410, 0.460],\n",
    "180: [0.365, 0.436, 0.336, 0.397, 0.466, 0.480, 0.370, 0.430, 0.400, 0.470],\n",
    "}\n",
    "# ECG_600 = {\n",
    "# 10: [0.070, 100, 0.063, 0.070, 0.072, 100, 0.080, 0.080, 0.080, 0.380, 0.100, 0.120],\n",
    "# 20: [0.150, 100, 0.108, 0.120, 0.117, 100, 0.150, 0.150, 0.140, 0.460, 0.160, 0.200],\n",
    "# 30: [0.100, 100, 0.140, 0.140, 0.140, 100, 0.170, 0.180, 0.190, 0.450, 0.210, 0.250],\n",
    "# } \n",
    "ECG = {\n",
    "1: [0.080, 0.085, 0.200, 0.210, 0.419, 1.217, 0.310, 0.340,  0.350, 1.620],\n",
    "10: [0.240, 0.247, 0.277, 0.290, 0.512, 0.842, 0.490, 0.570, 0.460, 1.130],\n",
    "25: [0.360,  0.395, 0.547, 0.38, 0.742, 1.057, 0.625, 0.770, 0.580, 1.500],\n",
    "# 50: [0.140, 0.507, 0.755, 0.410, 0.987, 1.272, 1.160, 0.940, 0.660, 1.150, 0.700, 0.490],\n",
    "100: [0.450, 0.597, 0.835, 0.420, 1.000, 1.100, 1.090, 0.980, 0.690, 1.260],\n",
    "} \n",
    "EEG = {\n",
    "1: [0.090, 100, 100, 0.180, 0.130, 100, 0.160, 0.150, 0.135, 0.630],\n",
    "10: [0.295, 100, 100, 0.345, 0.365, 100, 0.350, 0.345, 0.365, 0.545],\n",
    "25: [0.400, 100, 100, 0.410, 0.435, 100, 0.440, 0.420, 0.435, 0.620],\n",
    "} \n",
    "Chorales = {\n",
    "1: [0.240, 0.310, 0.322, 0.301, 0.322, 0.294, 0.300, 0.270, 0.290],\n",
    "2: [0.260, 100,100,100, 0.348, 0.306, 0.310, 0.290, 0.400, 0.250],\n",
    "3: [0.260, 100,100,100, 0.348, 0.306, 0.310, 0.290, 0.400, 0.250],\n",
    "4: [0.260, 100,100,100, 0.348, 0.306, 0.310, 0.290, 0.400, 0.250],\n",
    "# 5: [0.220, 100,100,100, 0.342, 0.309, 0.320, 0.310, 0.360, 0.240, 0.560, 0.260],\n",
    "}\n",
    "Air_Quality = {\n",
    "5: [0.510, 0.594, 0.586, 0.628, 0.658, 0.766,  0.810, 0.580, 0.690, 0.860],\n",
    "10: [0.280, 100, 0.655, 0.690, 0.739, 0.711, 0.850, 0.680, 0.780, 0.970],\n",
    "15: [0.720, 0.743, 0.736, 0.731, 0.774, 0.766, 0.870, 0.720,  0.830, 1.020],\n",
    "25: [0.780, 0.799, 0.797, 0.814,  0.844, 0.830, 0.920, 0.780, 0.930, 1.030],\n",
    "# 20: [0.350, 100, 0.734, 0.750, 0.805, 0.862, 0.850, 0.750, 0.870, 0.980],\n",
    "# 30: [0.400, 100, 0.827, 0.853, 0.890, 0.904, 0.960, 0.830, 1.010, 0.990],\n",
    "}\n",
    "\n",
    "\n",
    "def calculate_averages(data_dict):\n",
    "    result = {}\n",
    "    for key, lists in data_dict.items():\n",
    "        # Transpose the list of lists to get columns\n",
    "        transposed = zip(*lists)\n",
    "        # Calculate the average for each column\n",
    "        averages = [sum(col) / len(col) for col in transposed]\n",
    "        result[key] = averages\n",
    "    return result\n",
    "\n",
    "\n",
    "# def filter_data(data_dict):\n",
    "#     filtered_result = {}\n",
    "#     for key, values in data_dict.items():\n",
    "#         # Keep the first element and the minimum of the rest\n",
    "#         filtered_result[key] = [values[0]] + [min(values[1:])]\n",
    "#     return filtered_result\n",
    "\n",
    "def filter_data(data_dict):\n",
    "    filtered_result = {}\n",
    "    min_indices = {} \n",
    "    for key, values in data_dict.items():\n",
    "        min_value = min(values[1:])\n",
    "        min_index = values[1:].index(min_value) + 1 \n",
    "        filtered_result[key] = [values[0], min_value]\n",
    "        \n",
    "        min_indices[key] = min_index\n",
    "\n",
    "    return filtered_result, min_indices\n",
    "\n",
    "\n",
    "def calculate_percentage_reduction(filtered_data):\n",
    "    percentage_reduction = {}\n",
    "    for key, values in filtered_data.items():\n",
    "        if len(values) > 1:  # Ensure there are at least two elements\n",
    "            reduction = (1 - (values[0]) / values[1]) * 100\n",
    "            percentage_reduction[key] = reduction\n",
    "    return percentage_reduction\n",
    "\n",
    "def calculate_times_reduction(filtered_data):\n",
    "    times_reduction = {}\n",
    "    for key, values in filtered_data.items():\n",
    "        if len(values) > 1:  # Ensure there are at least two elements\n",
    "            reduction = values[1]/values[0]\n",
    "            times_reduction[key] = reduction\n",
    "    return times_reduction\n",
    "\n",
    "def process_data_dft(data):\n",
    "    processed_data = {}\n",
    "    for key, values in data.items():\n",
    "        # Keep only elements at positions 0, 1, 2, 3, 5, 6, 9, 11\n",
    "        selected_values = [values[i] for i in [0, 1, 2, 3, 5, 6]]\n",
    "        # Keep element 0 and the minimum of the remaining selected values\n",
    "        processed_data[key] = [selected_values[0], min(selected_values[1:])]\n",
    "    return processed_data\n",
    "\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "\n",
    "\n",
    "def perform_t_tests(data_sets, min_col_indices):\n",
    "    \"\"\"Perform t-tests between column 0 and the unfiltered minimum average column.\"\"\"\n",
    "    t_test_results = {}\n",
    "    for data_name, data_dict in data_sets.items():\n",
    "        column_0 = []\n",
    "        min_column = []\n",
    "        for key, list_ in data_dict.items():\n",
    "            column_0.append(list_[0])\n",
    "            min_column.append(list_[min_col_indices[data_name]])\n",
    "        stat, p_value = ttest_rel(column_0, min_column)  # Paired t-test\n",
    "        t_test_results[data_name] = {\"t_stat\": stat, \"p_value\": p_value}\n",
    "    return t_test_results\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "data_sets = {\n",
    "    # \"ILI\": ILI,\n",
    "    \"ECL\": ECL,\n",
    "    \"Exchange\": Exchange,\n",
    "    \"Traffic\": Traffic,\n",
    "    # \"ETTh1\": ETTh1,\n",
    "    # \"ETTh2\": ETTh2,\n",
    "    \"ETTm1\": ETTm1,\n",
    "    # \"ETTm2\": ETTm2,\n",
    "    \"Weather\": Weather,\n",
    "    # \"ECG_600\": ECG_600,\n",
    "    \"ECG\": ECG,\n",
    "    \"EEG\": EEG,\n",
    "    \"Chorales\": Chorales,\n",
    "    \"Air_Quality\": Air_Quality,\n",
    "}\n",
    "\n",
    "averages = {}\n",
    "for name, dataset in data_sets.items():\n",
    "    averages[name] = calculate_averages({name: list(dataset.values())})[name]\n",
    "\n",
    "filtered_results = {}\n",
    "min_indices = {}\n",
    "\n",
    "for name in averages:\n",
    "    result, indices = filter_data({name: averages[name]})\n",
    "    filtered_results[name] = result[name]\n",
    "    min_indices[name] = indices[name]\n",
    "\n",
    "\n",
    "percentage_reductions = calculate_percentage_reduction(filtered_averages)\n",
    "for name, reduction in percentage_reductions.items():\n",
    "    print(f\"{name} Percentage Reduction: {reduction:.2f}%\")\n",
    "print()\n",
    "times_reduction = calculate_times_reduction(filtered_averages)\n",
    "for name, reduction in times_reduction.items():\n",
    "    print(f\"{name} Times Reduction: {reduction:.2f}\")\n",
    "print()\n",
    "\n",
    "\n",
    "t_test_results = perform_t_tests(data_sets, min_indices)\n",
    "\n",
    "# Print t-test results\n",
    "for dataset_name, result in t_test_results.items():\n",
    "    # print(f\"T-Test Results for {dataset_name}:\")\n",
    "    print(f\"{dataset_name}: t-statistic = {result['t_stat']:.4f}, p-value = {result['p_value']:.4e}\")\n",
    "print()\n",
    "\n",
    "data = {}\n",
    "for name, dataset in data_sets.items():\n",
    "    data[name] = process_data_dft(dataset)\n",
    "\n",
    "dft_results = {}\n",
    "\n",
    "for dataset, entries in data.items():\n",
    "    improvements = []\n",
    "    values1 = []\n",
    "    values2 = []\n",
    "\n",
    "    for key, values in entries.items():\n",
    "        val1, val2 = values\n",
    "        improvement = (1 - val1/ val2) * 100\n",
    "        improvements.append(improvement)\n",
    "\n",
    "    # Average improvement\n",
    "    average_improvement = sum(improvements) / len(improvements)\n",
    "    dft_results[dataset] = average_improvement\n",
    "\n",
    "\n",
    "# Print the results\n",
    "for dataset, avg_improvement in dft_results.items():\n",
    "    print(f\"{dataset}: Average Improvement dft = {avg_improvement:.2f}%\")\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the data for each dataset\n",
    "data = {\n",
    "    \"ETTh1\": [\n",
    "        [0.390, 0.405, 0.360, 0.370, 0.386, 0.371, 0.380, 0.370, 0.370, 0.430, 0.460, 0.710],\n",
    "        [0.430, 0.467, 0.395, 0.412, 0.441, 0.414, 0.440, 0.410, 0.410, 0.460, 0.540, 0.780],\n",
    "        [0.440, 0.514, 0.409, 0.399, 0.487, 0.442, 0.490, 0.420, 0.440, 0.490, 0.530, 0.790],\n",
    "        [0.460, 0.614, 0.440, 0.472, 0.503, 0.465, 0.520, 0.450, 0.470, 0.520, 0.580, 0.840],\n",
    "    ],\n",
    "    \"ETTh2\": [\n",
    "        [0.190, 0.171, 0.263, 0.280, 0.297, 0.284, 0.340, 0.274, 0.289, 0.332, 0.240, 1.650],\n",
    "        [0.230, 0.211, 0.325, 0.330, 0.380, 0.357, 0.400, 0.340, 0.380, 0.430, 0.480, 1.950],\n",
    "        [0.260, 0.236, 0.320, 0.317, 0.428, 0.377, 0.450, 0.330, 0.450, 0.480, 0.540, 1.750],\n",
    "        [0.340, 0.297, 0.377, 0.404, 0.427, 0.439, 0.460, 0.380, 0.610, 0.450, 0.550, 1.730],\n",
    "    ],\n",
    "    \"ETTm1\": [\n",
    "        [0.160, 0.327, 0.287, 0.289, 0.334, 0.302, 0.340, 0.293, 0.299, 0.510, 0.230, 0.350],\n",
    "        [0.240, 0.370, 0.327, 0.328, 0.377, 0.338, 0.370, 0.330, 0.340, 0.510, 0.470, 0.500],\n",
    "        [0.370, 0.402, 0.362, 0.355, 0.426, 0.373, 0.410, 0.370, 0.370, 0.510, 0.450, 0.580],\n",
    "        [0.430, 0.453, 0.408, 0.421, 0.491, 0.420, 0.480, 0.420, 0.430, 0.530, 0.540, 0.790],\n",
    "    ],\n",
    "    \"ETTm2\": [\n",
    "        [0.130, 0.114, 0.161, 0.169, 0.180, 0.165, 0.190, 0.166, 0.167, 0.205, 0.160, 0.260],\n",
    "        [0.170, 0.141, 0.219, 0.224, 0.250, 0.222, 0.250, 0.220, 0.220, 0.280, 0.300, 0.590],\n",
    "        [0.220, 0.172, 0.267, 0.275, 0.311, 0.277, 0.320, 0.270, 0.280, 0.340, 0.340, 0.810],\n",
    "        [0.260, 0.219, 0.347, 0.354, 0.412, 0.371, 0.410, 0.360, 0.400, 0.410, 0.420, 1.000],\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Compute averages for each horizon across all datasets\n",
    "horizons = [96, 192, 336, 720]\n",
    "num_metrics = len(data[\"ETTh1\"][0])\n",
    "averages = []\n",
    "\n",
    "for i in range(len(horizons)):\n",
    "    horizon_values = [data[dataset][i] for dataset in data]\n",
    "    horizon_avg = np.mean(horizon_values, axis=0)\n",
    "    averages.append(horizon_avg)\n",
    "\n",
    "# Generate LaTeX table\n",
    "# latex_table = \"\\\\begin{table}[htbp]\\n\\\\centering\\n\\\\begin{tabular}{|c|\" + \"c|\" * num_metrics + \"}\\\\hline\\n\"\n",
    "# latex_table += \"Horizon & \" + \" & \".join([f\"Metric {j+1}\" for j in range(num_metrics)]) + \" \\\\\\\\ \\\\hline\\n\"\n",
    "\n",
    "# for i, horizon in enumerate(horizons):\n",
    "#     latex_table += f\"{horizon} & \" + \" & \".join(f\"{val:.3f}\" for val in averages[i]) + \" \\\\\\\\ \\\\hline\\n\"\n",
    "\n",
    "# latex_table += \"\\\\end{tabular}\\n\\\\caption{Averaged metrics across all datasets for each horizon.}\\n\\\\label{tab:horizon_averages}\\n\\\\end{table}\"\n",
    "\n",
    "# # Print the LaTeX table\n",
    "# print(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "l196 = [0.390, 0.405, 0.360 , 0.370, 0.386, 0.371, 0.380, 0.370, 0.370, 0.430, 0.460, 0.710]\n",
    "l296 = [0.190, 0.171, 0.263, 0.280, 0.297, 0.284, 0.340, 0.274, 0.289, 0.332, 0.240, 1.650]\n",
    "l396 = [0.160, 0.327, 0.287, 0.289 , 0.334, 0.302, 0.340, 0.293, 0.299, 0.510, 0.230, 0.350]\n",
    "l496 = [0.130, 0.114, 0.161, 0.169, 0.180, 0.165, 0.190, 0.166, 0.167, 0.205, 0.160, 0.260]\n",
    "\n",
    "\n",
    "l1192 = [0.430, 0.467, 0.395 , 0.412, 0.441, 0.414, 0.440, 0.410, 0.410, 0.460, 0.540, 0.780]\n",
    "l2192 = [0.230, 0.211, 0.325, 0.330, 0.380, 0.357, 0.400, 0.340, 0.380, 0.430, 0.480, 1.950] \n",
    "l3192 = [0.240, 0.370, 0.327, 0.328 , 0.377, 0.338, 0.370, 0.330, 0.340, 0.510, 0.470, 0.500] \n",
    "l4192 = [0.170, 0.141, 0.219, 0.224, 0.250, 0.222, 0.250, 0.220, 0.220, 0.280, 0.300, 0.590]\n",
    "\n",
    " \n",
    "l1336 = [0.440, 0.514, 0.409, 0.399, 0.487, 0.442, 0.490, 0.420, 0.440, 0.490, 0.530, 0.790] \n",
    "l2336 = [0.260, 0.236, 0.320, 0.317, 0.428, 0.377, 0.450, 0.330, 0.450, 0.480, 0.540, 1.750] \n",
    "l3336 = [0.370 , 0.402, 0.362, 0.355 , 0.426, 0.373, 0.410, 0.370, 0.370, 0.510, 0.450, 0.580] \n",
    "l4336 = [0.220, 0.172, 0.267, 0.275, 0.311, 0.277, 0.320, 0.270, 0.280, 0.340, 0.340, 0.810] \n",
    " \n",
    "l1720 = [0.460, 0.614, 0.440 , 0.472, 0.503, 0.465, 0.520, 0.450, 0.470, 0.520, 0.580, 0.840]\n",
    "l2720 = [0.340, 0.297, 0.377, 0.404, 0.427, 0.439, 0.460, 0.380, 0.610, 0.450, 0.550, 1.730] \n",
    "l3720 = [0.430 , 0.453, 0.408 , 0.421, 0.491, 0.420, 0.480, 0.420, 0.430, 0.530, 0.540, 0.790] \n",
    "l4720 = [0.260, 0.219, 0.347, 0.354, 0.412, 0.371, 0.410, 0.360, 0.400, 0.410, 0.420, 1.000] \n",
    "\n",
    "# Calculate the average for each corresponding index\n",
    "averaged_list96 = [(l196[i] + l296[i] + l396[i] + l496[i]) / 4 for i in range(len(list1))]\n",
    "averaged_list192 = [(l1192[i] + l2192[i] + l3192[i] + l4192[i]) / 4 for i in range(len(list1))]\n",
    "averaged_list336 = [(l1336[i] + l2336[i] + l3336[i] + l4336[i]) / 4 for i in range(len(list1))]\n",
    "averaged_list720 = [(l1720[i] + l2720[i] + l3720[i] + l4720[i]) / 4 for i in range(len(list1))]\n",
    "\n",
    "# Print the resulting averaged list\n",
    "print(\"Averaged list:\", averaged_list96)\n",
    "print(\"Averaged list:\", averaged_list192)\n",
    "print(\"Averaged list:\", averaged_list336)\n",
    "print(\"Averaged list:\", averaged_list720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SD_air_quality = {1: 0.006981970903698131, 5: 0.011255134863186711, 10: 0.03615413284942134, 15: 0.023951309938737442, 20: 0.025947521548064647, 25: 0.01763553306411561, 30: 0.06373393879422216}\n",
      "SD_chorales = {1: 0.012366765428624485, 2: 0.008889436721716869, 3: 0.005597313577427192, 4: 0.0028135897394941764, 5: 0.007265847563100219}\n",
      "SD_ettm1 = {1: 0.03240922577944263, 10: 0.04392838251250575, 24: 0.06716980868043991, 48: 0.014983397254528128, 96: 0.019517797390532287, 192: 0.013107555141997294, 336: 0.021360335990135287, 720: 0.169901806442043}\n",
      "SD_traffic = {1: 0.013165323933911184, 16: 0.03523304337619862, 32: 0.02056600418367745, 48: 0.051405950003577935, 96: nan}\n",
      "SD_electricity = {1: 0.0022567926884794628, 16: 0.011685245528471298, 32: 0.003716364359207388, 96: 0.0551405526958188, 192: 0.003726792932727294}\n",
      "SD_exchange = {1: 0.33476242870041295, 16: 0.5342297414751066, 32: 0.39895559217977267, 48: 0.5532453653362579, 96: 0.17555395876026272, 192: 0.15757448775208754, 336: 0.2801981857710085, 720: 0.984028975017089}\n",
      "SD_noaa = {1: 0.09322379064253916, 7: 0.0974560936879086, 15: 0.010329947740215664, 30: 0.007825756025392469, 60: 0.011882194930327992, 90: 0.012898987563160847, 120: 0.08257425203307868, 180: 0.0798864539646849}\n",
      "SD_ecg_single = {1: 0.008727321392763561, 10: 0.011586556679350577, 25: 0.014293556781441244, 50: 0.11046075995187402, 100: 0.014241447728353788}\n",
      "SD_eeg_single = {1: 0.013644537244074682, 10: 0.025887007712756913, 25: 0.0013551860099907465}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "sys.path.append('/home/noam.koren/multiTS/NFT/')\n",
    "from dicts import single_data_to_series_list\n",
    "\n",
    "def calculate_std_per_horizon(data):\n",
    "    grouped = data.groupby('Horizon')\n",
    "    horizon_std_values = {}\n",
    "    for horizon, group in grouped:\n",
    "        last_5_values = group['test_mse'].tail(5).tolist()\n",
    "        horizon_std_values[horizon] = pd.Series(last_5_values).std()\n",
    "    return horizon_std_values\n",
    "\n",
    "def calculate_avg_std_across_series(dataset, series_list):\n",
    "    horizon_std_aggregated = defaultdict(list)\n",
    "    for series in series_list:\n",
    "        file_path = f\"/home/noam.koren/multiTS/NFT/results/{dataset}/nft/nft_{dataset}_{series}_None_results.xlsx\"\n",
    "        data = pd.read_excel(file_path)\n",
    "        series_horizon_std = calculate_std_per_horizon(data)\n",
    "        for horizon, std in series_horizon_std.items():\n",
    "            horizon_std_aggregated[horizon].append(std)\n",
    "\n",
    "    avg_std_per_horizon = {\n",
    "        horizon: sum(std_list) / len(std_list) if std_list else 0\n",
    "        for horizon, std_list in horizon_std_aggregated.items()\n",
    "    }\n",
    "    return avg_std_per_horizon\n",
    "\n",
    "for dataset in ['air_quality', 'chorales', 'ettm1', 'traffic', 'electricity', 'exchange']:\n",
    "    file_path = f\"/home/noam.koren/multiTS/NFT/results/{dataset}/nft/nft_{dataset}_results.xlsx\"\n",
    "    data = pd.read_excel(file_path)\n",
    "    std_per_horizon = calculate_std_per_horizon(data)\n",
    "    print(f\"SD_{dataset} = {std_per_horizon}\")\n",
    "\n",
    "for dataset in ['noaa', 'ecg_single', 'eeg_single']:\n",
    "    series_list = single_data_to_series_list[dataset]\n",
    "    avg_std_per_horizon = calculate_avg_std_across_series(dataset, series_list)\n",
    "    print(f\"SD_{dataset} = {avg_std_per_horizon}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SD_electricity = {\n",
    "    1: 0.001486866274707531, *\n",
    "    16: 0.011685245528471298, *\n",
    "    32: 0.0.003716364359207388, \n",
    "    }\n",
    "SD_exchange = {\n",
    "    96: 0.07732609947762704, \n",
    "    192: 0.11697775392057777, \n",
    "    336: 0.2784904763044087, \n",
    "    720: 0.9431714171117671\n",
    "    }\n",
    "SD_traffic = {\n",
    "    1: 0.013165323933911184, *\n",
    "    16: 0.03523304337619862,*\n",
    "    32: 0.02056600418367745, *\n",
    "    48: 0.04656076439675355\n",
    "    }\n",
    "SD_ettm1 = {\n",
    "    48: 0.014983397254528128, \n",
    "    96: 0.019517797390532287, *\n",
    "    192: 0.010890653878891316, \n",
    "    336: 0.019506618643152146, \n",
    "    }\n",
    "SD_noaa = {\n",
    "    15: 0.010329947740215664, *\n",
    "    30: 0.007825756025392469, *\n",
    "    60: 0.011882194930327992, *\n",
    "    90: 0.011805440763793746\n",
    "    }\n",
    "SD_ecg_single = {\n",
    "    1: 0.008727321392763561, \n",
    "    10: 0.011586556679350577, \n",
    "    25: 0.014293556781441244, *\n",
    "    100: 0.014241447728353788\n",
    "    }\n",
    "SD_eeg_single = {\n",
    "    1: 0.013644537244074682,  *\n",
    "    10: 0.025887007712756913, *\n",
    "    25: 0.0013551860099907465\n",
    "    }\n",
    "SD_chorales = {\n",
    "    1: 0.010219212863249293,  *\n",
    "    2: 0.008889436721716869, *\n",
    "    3: 0.005597313577427192, \n",
    "    4: 0.002, \n",
    "    }\n",
    "SD_air_quality = { \n",
    "    5: 0.006903352807488888, *\n",
    "    10: 0.013316870709515491, *\n",
    "    15: 0.023951309938737442,  \n",
    "    25: 0.017, \n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "sys.path.append('/home/noam.koren/multiTS/NFT/')\n",
    "from dicts import single_data_to_series_list\n",
    "\n",
    "# Function to calculate min values for each horizon\n",
    "def calculate_min_per_horizon(data):\n",
    "    grouped = data.groupby('Horizon')\n",
    "    horizon_min_values = {}\n",
    "    for horizon, group in grouped:\n",
    "        min_test_mse_row = group.loc[group['test_mse'].idxmin()]  # Row with minimum test_mse\n",
    "        min_test_mae_row = group.loc[group['test_mae'].idxmin()]  # Row with minimum test_mae\n",
    "\n",
    "        horizon_min_values[horizon] = {\n",
    "            \"min_test_mse\": {\n",
    "                \"value\": min_test_mse_row['test_mse'],\n",
    "                \"corresponding_test_mae\": min_test_mse_row['test_mae']\n",
    "            },\n",
    "            \"min_test_mae\": {\n",
    "                \"value\": min_test_mae_row['test_mae'],\n",
    "                \"corresponding_test_mse\": min_test_mae_row['test_mse']\n",
    "            }\n",
    "        }\n",
    "    return horizon_min_values\n",
    "\n",
    "# Function to calculate min values across series for each horizon\n",
    "def calculate_min_per_horizon_across_series(dataset, series_list):\n",
    "    horizon_min_aggregated = defaultdict(list)\n",
    "    for series in series_list:\n",
    "        file_path = f\"/home/noam.koren/multiTS/NFT/results/{dataset}/nft/nft_{dataset}_{series}_None_results.xlsx\"\n",
    "        data = pd.read_excel(file_path)\n",
    "        series_horizon_min = calculate_min_per_horizon(data)\n",
    "        for horizon, min_values in series_horizon_min.items():\n",
    "            horizon_min_aggregated[horizon].append(min_values)\n",
    "\n",
    "    # Find global min per horizon\n",
    "    global_min_per_horizon = {}\n",
    "    for horizon, min_values_list in horizon_min_aggregated.items():\n",
    "        min_test_mse = min(min_values_list, key=lambda x: x[\"min_test_mse\"][\"value\"])\n",
    "        min_test_mae = min(min_values_list, key=lambda x: x[\"min_test_mae\"][\"value\"])\n",
    "        global_min_per_horizon[horizon] = {\n",
    "            \"min_test_mse\": min_test_mse[\"min_test_mse\"],\n",
    "            \"min_test_mae\": min_test_mae[\"min_test_mae\"]\n",
    "        }\n",
    "\n",
    "    return global_min_per_horizon\n",
    "\n",
    "# For datasets without series splitting\n",
    "for dataset in ['air_quality', 'chorales', 'ettm1', 'traffic', 'electricity', 'exchange']:\n",
    "    file_path = f\"/home/noam.koren/multiTS/NFT/results/{dataset}/nft/nft_{dataset}_results.xlsx\"\n",
    "    data = pd.read_excel(file_path)\n",
    "    min_values_per_horizon = calculate_min_per_horizon(data)\n",
    "    print(f\"Min values per horizon for {dataset}:\")\n",
    "    for horizon, values in min_values_per_horizon.items():\n",
    "        print(f\"  Horizon {horizon}:\")\n",
    "        print(f\"    Minimum test_mse: {values['min_test_mse']['value']} (Corresponding test_mae: {values['min_test_mse']['corresponding_test_mae']})\")\n",
    "        print(f\"    Minimum test_mae: {values['min_test_mae']['value']} (Corresponding test_mse: {values['min_test_mae']['corresponding_test_mse']})\")\n",
    "\n",
    "# For datasets with multiple series\n",
    "for dataset in ['noaa', 'ecg_single', 'eeg_single']:\n",
    "    series_list = single_data_to_series_list[dataset]\n",
    "    min_values_per_horizon = calculate_min_per_horizon_across_series(dataset, series_list)\n",
    "    print(f\"Aggregated Min values per horizon for {dataset}:\")\n",
    "    for horizon, values in min_values_per_horizon.items():\n",
    "        print(f\"  Horizon {horizon}:\")\n",
    "        print(f\"    Minimum test_mse: {values['min_test_mse']['value']} (Corresponding test_mae: {values['min_test_mse']['corresponding_test_mae']})\")\n",
    "        print(f\"    Minimum test_mae: {values['min_test_mae']['value']} (Corresponding test_mse: {values['min_test_mae']['corresponding_test_mse']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
