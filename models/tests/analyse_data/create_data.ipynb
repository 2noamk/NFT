{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_to_excel(df, file_path):\n",
    "    # Ensure the file path ends with .xlsx\n",
    "    if not file_path.endswith('.xlsx'):\n",
    "        file_path += '.xlsx'\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            existing_df = pd.read_excel(file_path)\n",
    "            combined_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading existing file: {e}\")\n",
    "            print(\"Creating a new file instead.\")\n",
    "            combined_df = df\n",
    "    else:\n",
    "        combined_df = df\n",
    "    with pd.ExcelWriter(file_path, engine='openpyxl', mode='w') as writer:\n",
    "        combined_df.to_excel(writer, index=False)\n",
    "\n",
    "    print(f\"Data successfully saved to {file_path}\")\n",
    "\n",
    "def create_seasonality_series(n_variables=n_variables, n_timesteps=n_timesteps, seasonality_period=50, noise_level=0.1, save_data=False):\n",
    "        \n",
    "    # Create time vector\n",
    "    time = np.arange(n_timesteps)\n",
    "\n",
    "    # Generate synthetic multivariate time series data\n",
    "    data = np.zeros((n_timesteps, n_variables))\n",
    "    for i in range(n_variables):\n",
    "        # Add high seasonality with different phases and amplitudes\n",
    "        amplitude = np.random.uniform(0.5, 1.5)\n",
    "        phase = np.random.uniform(0, 2 * np.pi)\n",
    "        data[:, i] = amplitude * np.sin(2 * np.pi * time / seasonality_period + phase)\n",
    "\n",
    "        # Add some noise\n",
    "        data[:, i] += np.random.normal(0, noise_level, n_timesteps)\n",
    "\n",
    "    # Convert to a pandas DataFrame for visualization and manipulation\n",
    "    columns = [f\"Variable_{i+1}\" for i in range(n_variables)]\n",
    "    synthetic_data = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    # Plot the generated time series data\n",
    "    synthetic_data.plot(figsize=(12, 6))\n",
    "    plt.title(\"Synthetic Multivariate Time Series with High Seasonality\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Values\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    if save_data:\n",
    "        directory = '/home/noam.koren/multiTS/NFT/data/seasonality/'\n",
    "        file_path = os.path.join(directory, 'seasonality.pkl')\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        synthetic_data.to_pickle(file_path)\n",
    "\n",
    "def process_component_info(seasonality_trend_info, series_name):\n",
    "    component_info = pd.DataFrame(seasonality_trend_info)\n",
    "    component_info['Series_Name'] = series_name\n",
    "\n",
    "    averages = component_info.select_dtypes(include='number').mean()\n",
    "    averages_row = {col: averages[col] if col in averages.index else None for col in component_info.columns}\n",
    "    averages_row['Series_Name'] = series_name\n",
    "    averages_row['Series'] = 'Average'\n",
    "    averages_row['Seasonality_Precetage'] = averages_row['Seasonality_Dominance'] / (averages_row['Seasonality_Dominance']  + averages_row['Trend_Dominance'])\n",
    "    averages_df = pd.DataFrame([averages_row])\n",
    "\n",
    "    component_info = pd.concat([component_info, averages_df], ignore_index=True)\n",
    "\n",
    "    cols = ['Series_Name'] + [col for col in component_info.columns if col != 'Series_Name']\n",
    "    component_info = component_info[cols]\n",
    "\n",
    "    print(\"Averaged Component Info:\")\n",
    "    print(averages_df[cols].head(6))\n",
    "\n",
    "    return component_info, averages_df[cols]\n",
    "\n",
    "def create_time_series(\n",
    "    num_series, \n",
    "    num_points,\n",
    "    trend_amplitude=0.5, \n",
    "    noise_level=1, \n",
    "    seasonal_amplitude=1, # (0.5, 1.5)\n",
    "    save_data=False,\n",
    "    series_name=None,\n",
    "    compute_components=True  # parameter to compute seasonality and trend proportions\n",
    "    ):\n",
    "    time = np.arange(num_points)\n",
    "    data = {}\n",
    "    seasonality_trend_info = []\n",
    "\n",
    "    for i in range(num_series):\n",
    "        trend = trend_amplitude * (time / num_points)\n",
    "        \n",
    "        phase = np.random.uniform(0, 2 * np.pi)\n",
    "        seasonal = (seasonal_amplitude * (i+1)) * np.sin(2 * np.pi * time / (50 + i * 5) + i * phase)\n",
    "    \n",
    "        noise = np.random.normal(0, noise_level, num_points)\n",
    "        \n",
    "        series = trend + seasonal + noise\n",
    "        data[f\"Series_{i+1}\"] = series\n",
    "        \n",
    "        if compute_components:\n",
    "            total_variance = np.var(series)\n",
    "            seasonal_variance = np.var(seasonal)\n",
    "            trend_variance = np.var(trend)\n",
    "            noise_variance = np.var(noise)\n",
    "        \n",
    "\n",
    "            seasonality_ratio = seasonal_variance / total_variance\n",
    "            trend_ratio = trend_variance / total_variance\n",
    "            trend_to_seasonality_ratio = trend_variance / seasonal_variance if seasonal_variance > 0 else np.inf\n",
    "            seasonality_to_trend_ratio = seasonal_variance / trend_variance if trend_variance > 0 else np.inf\n",
    "            noise_ratio = noise_variance / total_variance\n",
    "            \n",
    "            trend_dominance = max(0, 1 - (noise_variance / (noise_variance + trend_variance)))\n",
    "            seasonality_dominance = max(0, 1 - (noise_variance / (noise_variance + seasonal_variance)))\n",
    "\n",
    "            seasonality_trend_info.append({\n",
    "                'Series': f\"Series_{i+1}\",\n",
    "                'Seasonality_Dominance': seasonality_dominance,\n",
    "                'Trend_Dominance': trend_dominance,\n",
    "                'Seasonality_Ratio': seasonality_ratio,\n",
    "                'Trend_Ratio': trend_ratio,\n",
    "                'Trend_to_Seasonality_Ratio': trend_to_seasonality_ratio,\n",
    "                'Seasonality_to_Trend_Ratio': seasonality_to_trend_ratio,\n",
    "                'Noise_Ratio': noise_ratio\n",
    "            })\n",
    "\n",
    "    synthetic_data = pd.DataFrame(data, index=time)\n",
    "    # synthetic_data.plot(figsize=(10, 6), title=\"Synthetic Multivariate Time Series\")\n",
    "    # plt.xlabel(\"Time\")\n",
    "    # plt.ylabel(\"Value\")\n",
    "    # plt.show()\n",
    "    \n",
    "    component_info = process_component_info(seasonality_trend_info, series_name=series_name)\n",
    "\n",
    "    \n",
    "    if save_data:\n",
    "        directory = f'/home/noam.koren/multiTS/NFT/data/{series_name}/'\n",
    "        file_path = os.path.join(directory, f'{series_name}.pkl')\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        synthetic_data.to_pickle(file_path)\n",
    "        \n",
    "        save_to_excel(component_info, '/home/noam.koren/multiTS/NFT/models/tests/analyse_data/data_components.xlsx')\n",
    "\n",
    "n_variables=5\n",
    "n_timesteps=1000\n",
    "  \n",
    "# for seasonal_amplitude in [4, 5, 6, 7, 8, 9, 10]:\n",
    "#     for trend_amplitude in [0.50]:\n",
    "#         # create_time_series(\n",
    "#         #     num_series=n_variables, \n",
    "#         #     num_points=n_timesteps,\n",
    "#         #     trend_amplitude=trend_amplitude, \n",
    "#         #     noise_level=0.1, \n",
    "#         #     seasonal_amplitude=seasonal_amplitude,\n",
    "#         #     save_data=True,\n",
    "#         #     series_name=f'seasonal_{seasonal_amplitude}_trend_{trend_amplitude}'\n",
    "#         #     )\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6700328749294397 0.48294455439953865 0.5811326899255888\n",
      "0.6502333031670686 0.4724909227162899 0.5791567405214452\n",
      "0.5231599939667442 0.38765877440337126 0.5743842926106192\n",
      "0.7342898078158848 0.5798748395248386 0.5587502367391758\n",
      "0.6000845445516669 0.5141361008621911 0.5385688615819678\n",
      "0.6151335306721881 0.5136674407678737 0.5449441896629792\n",
      "0.5450501129949544 0.48267332843129074 0.5303470671434227\n",
      "0.5508879870299195 0.5175580792723631 0.515597374920807\n",
      "0.5566806945241101 0.5102677561264948 0.5217503190380536\n",
      "0.5089783068591609 0.4905006394291781 0.509243650153213\n",
      "0.5162333656083318 0.4972897729160535 0.5093454169777805\n",
      "0.4929739447565967 0.500620748749596 0.4961519500642585\n",
      "0.4847735186754468 0.4886229056813002 0.4980227033356953\n",
      "0.4846313975190054 0.5014236933636711 0.49148511274880496\n",
      "0.5308772509608652 0.5543197593064286 0.4891989619747527\n",
      "0.4297391717667568 0.468692271242498 0.4783216071861479\n",
      "0.5026610586240976 0.5433308498612615 0.48055922282608504\n",
      "0.4777681166355363 0.5349849734740078 0.4717518231258704\n",
      "0.49432642701037854 0.5529308841238934 0.47202002960951356\n",
      "0.49072940106918683 0.5693884088310082 0.4629008177075968\n",
      "0.4848809634538759 0.5654240926290593 0.46165726866270385\n",
      "0.4374585462409085 0.5249917468672741 0.4545258590219336\n",
      "0.4277584091173945 0.5307463479875583 0.4462767721773096\n",
      "0.49108859314695674 0.6162147545020976 0.4434996012516355\n",
      "0.46710709128714656 0.5912822567836911 0.4413376723212102\n",
      "0.47484173986563905 0.6366615718595527 0.4272067701972254\n",
      "0.4505686043837648 0.6009656205642302 0.42848686585170176\n",
      "0.4303432794079748 0.6064479878708355 0.41507224548434435\n",
      "0.42245467086136745 0.5983621630087628 0.4138398357516827\n",
      "0.32581760446970964 0.5239023552694687 0.38344115697802295\n",
      "0.3256097472883511 0.5881357815997919 0.3563462003305856\n",
      "0.302462008588292 0.5364848088357554 0.36052584300514956\n",
      "0 0.9215886314276412 0.0\n",
      "0 0.9338864205240992 0.0\n",
      "Averaged Component Info:\n",
      "   Series_Name   Series  Seasonality_Dominance  Trend_Dominance  \\\n",
      "0  electricity  Average               0.467636         0.557014   \n",
      "\n",
      "   Seasonality_Precetage  total_variance  seasonal_variance  trend_variance  \\\n",
      "0               0.456386   238618.560872      117540.701924   148346.636905   \n",
      "\n",
      "   noise_variance  \n",
      "0   125792.054998  \n",
      "Data successfully saved to /home/noam.koren/multiTS/NFT/models/tests/analyse_data/mini_elect_components.xlsx\n"
     ]
    }
   ],
   "source": [
    "def get_components(y):\n",
    "\n",
    "    # Step 2: Apply Fast Fourier Transform (FFT)\n",
    "    n = len(y)  # Length of the signal\n",
    "    fft_vals = np.fft.fft(y)  # FFT\n",
    "    frequencies = np.fft.fftfreq(n)  # Frequency bins\n",
    "\n",
    "    # Step 3: Identify dominant frequencies (filtering)\n",
    "    # Sort by the magnitude of the FFT values and retain top frequencies\n",
    "    magnitude = np.abs(fft_vals)\n",
    "    threshold = 0.05 * max(magnitude)  # Keep frequencies with > 5% of max amplitude\n",
    "    filtered_fft_vals = fft_vals.copy()\n",
    "    filtered_fft_vals[magnitude < threshold] = 0  # Zero out low-amplitude components\n",
    "\n",
    "    # Step 4: Reconstruct the seasonality using Inverse FFT (IFFT)\n",
    "    seasonality = np.fft.ifft(filtered_fft_vals).real\n",
    "    \n",
    "    # Step 5: Filter out high frequencies (low-pass filter)\n",
    "    # Define a cutoff frequency: retain only the low frequencies\n",
    "    cutoff = 0.05  # Adjust this value based on your data (0 < cutoff < 1)\n",
    "    filtered_fft_vals = fft_vals.copy()\n",
    "    filtered_fft_vals[np.abs(frequencies) > cutoff] = 0  # Zero out high-frequency components\n",
    "\n",
    "    # Step 4: Reconstruct the trend using Inverse FFT (IFFT)\n",
    "    trend = np.fft.ifft(filtered_fft_vals).real\n",
    "    \n",
    "    noise = y - seasonality - trend\n",
    "    \n",
    "    return seasonality, trend, noise\n",
    " \n",
    "def plot_componentes(y, seasonality, trend, noise):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y, label='Original Series', color='blue')\n",
    "    plt.plot(seasonality, label='Extracted Seasonality', color='red')\n",
    "    plt.plot(trend, label='Extracted Trend', color='orange')\n",
    "    plt.plot(noise, label='Extracted Noise', color='green')\n",
    "    plt.title('Components Extraction using Fourier Transform')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    " \n",
    "def calculate_components_dominance(data, data_name, save_Excel=False):\n",
    "    seasonality_trend_info = []\n",
    "    for column in data.columns:\n",
    "        y = data[column]\n",
    "            \n",
    "        seasonal, trend, noise = get_components(y)\n",
    "\n",
    "        total_variance = y.var()\n",
    "        seasonal_variance = seasonal.var()\n",
    "        trend_variance = trend.var()\n",
    "        noise_variance = noise.var()\n",
    "\n",
    "        seasonality_dominance = max(0, 1 - (noise_variance / (noise_variance + seasonal_variance)))\n",
    "        trend_dominance = max(0, 1 - (noise_variance / (noise_variance + trend_variance)))\n",
    "        seasonality_precetage = seasonality_dominance / (seasonality_dominance + trend_dominance) if (seasonality_dominance + trend_dominance) != 0 else 0\n",
    "        \n",
    "        print(seasonality_dominance, trend_dominance, seasonality_precetage)\n",
    "        \n",
    "        seasonality_trend_info.append({\n",
    "            'Series': column,\n",
    "            'Seasonality_Dominance': seasonality_dominance,\n",
    "            'Trend_Dominance': trend_dominance,\n",
    "            'Seasonality_Precetage': seasonality_precetage,\n",
    "            'total_variance': total_variance,\n",
    "            'seasonal_variance': seasonal_variance,\n",
    "            'trend_variance': trend_variance,\n",
    "            'noise_variance': noise_variance,\n",
    "            })\n",
    "        \n",
    "    component_info, averages_df = process_component_info(seasonality_trend_info, series_name=data_name)\n",
    "    if save_Excel:\n",
    "        save_to_excel(averages_df, f'/home/noam.koren/multiTS/NFT/models/tests/analyse_data/real_data_components.xlsx')\n",
    "\n",
    "    return component_info, averages_df\n",
    "\n",
    "# pkl_dir = \"/home/noam.koren/multiTS/NFT/data/ecg/pkl_files/\"\n",
    "# for pkl_file in os.listdir(pkl_dir):\n",
    "#     if pkl_file.endswith(\".pkl\"):  # Check if the file is a .pkl file\n",
    "#         file_path = os.path.join(pkl_dir, pkl_file)  # Full path to the file\n",
    "#         data = pd.read_pickle(file_path)\n",
    "#         component_info, averages_df = calculate_components_dominance(data, pkl_file[:-4], save_Excel=True)\n",
    "\n",
    "# for dataset in ['electricity', 'exchange', 'illness', 'traffic']:\n",
    "#     data = pd.read_pickle(f'/home/noam.koren/multiTS/NFT/data/{dataset}/{dataset}_no_date.pkl')\n",
    "#     component_info, averages_df = calculate_components_dominance(data, dataset, save_Excel=True)\n",
    "\n",
    "\n",
    "data = pd.read_pickle('/home/noam.koren/multiTS/NFT/data/electricity/mini_elctricity.pkl')\n",
    "component_info, averages_df = calculate_components_dominance(data, dataset, save_Excel=True)\n",
    "\n",
    "# for n in ['AE000041196', 'AEM00041194', 'AEM00041217']:\n",
    "#     data = pd.read_pickle(f'/home/noam.koren/multiTS/NFT/data/noaa/noaa_ghcn/noaa_pkl/{n}.pkl')\n",
    "#     component_info, averages_df = calculate_components_dominance(data, n, save_Excel=True)\n",
    "\n",
    "# pkl_dir = \"/home/noam.koren/multiTS/NFT/data/noaa/noaa_ghcn/years/embedded/AEM00041217/\"\n",
    "# for pkl_file in os.listdir(pkl_dir):\n",
    "#     if pkl_file.endswith(\".pkl\"):  # Check if the file is a .pkl file\n",
    "#         file_path = os.path.join(pkl_dir, pkl_file)  # Full path to the file\n",
    "#         data = pd.read_pickle(file_path)\n",
    "#         component_info, averages_df = calculate_components_dominance(data, pkl_file[:-4], save_Excel=True)\n",
    "        \n",
    "# csv_dir = '/home/noam.koren/multiTS/NFT/data/chorales/chorales_csvs'\n",
    "# for csv_file in os.listdir(csv_dir):\n",
    "#     if csv_file.endswith(\".csv\"):  # Check if the file is a .pkl file\n",
    "#         file_path = os.path.join(csv_dir, csv_file)  # Full path to the file\n",
    "#         data = pd.read_csv(file_path)\n",
    "#         component_info, averages_df = calculate_components_dominance(data, csv_file[:-4], save_Excel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_path = '/home/noam.koren/multiTS/NFT/data/electricity/electricity_no_date.pkl'\n",
    "d = pd.read_pickle(pkl_path)\n",
    "column_indices_to_extract = [20, 44, 92, 64, 58, 71, 47, 13, 21, 22, 3, 5, 10, 16, 7, 15, 2, 69, 4, 6, 1, 29, 0, 8, 25, 55, 122, 125, 131, 99, 84, 9, 50, 113, 117, 118]\n",
    "mini_elctricity = d.iloc[:, column_indices_to_extract]\n",
    "mini_elctricity.to_pickle('/home/noam.koren/multiTS/NFT/data/electricity/mini_elctricity.pkl')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
      "       ...\n",
      "       '311', '312', '313', '314', '315', '316', '317', '318', '319', 'OT'],\n",
      "      dtype='object', length=321)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "('0', '1')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/nft/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ('0', '1')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(d\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m----> 2\u001b[0m \u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nft/lib/python3.9/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/nft/lib/python3.9/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: ('0', '1')"
     ]
    }
   ],
   "source": [
    "print(d.columns)\n",
    "d['0', '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/noam.koren/multiTS/NFT/data/chorales/chorales_data_summary.xlsx'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# csv_dir = '/home/noam.koren/multiTS/NFT/data/chorales/chorales_csvs'\n",
    "\n",
    "# data_names = []\n",
    "# data_lengths = []\n",
    "\n",
    "# for csv_file in os.listdir(csv_dir):\n",
    "#     if csv_file.endswith(\".csv\"):  # Check if the file is a .csv file\n",
    "#         file_path = os.path.join(csv_dir, csv_file)  # Full path to the file\n",
    "#         data = pd.read_csv(file_path)\n",
    "#         data_names.append(csv_file[:-4])  # Remove .csv extension\n",
    "#         data_lengths.append(len(data))  # Get length of the data\n",
    "\n",
    "# output_df = pd.DataFrame({\n",
    "#     'Data Name': data_names,\n",
    "#     'Data Length': data_lengths\n",
    "# })\n",
    "\n",
    "# output_excel_path = '/home/noam.koren/multiTS/NFT/data/chorales/chorales_data_summary.xlsx'\n",
    "# output_df.to_excel(output_excel_path, index=False)\n",
    "\n",
    "# output_excel_path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
