{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_variables=5\n",
    "n_timesteps=1000\n",
    "\n",
    "def save_to_excel(df, file_path):\n",
    "    # Ensure the file path ends with .xlsx\n",
    "    if not file_path.endswith('.xlsx'):\n",
    "        file_path += '.xlsx'\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            existing_df = pd.read_excel(file_path)\n",
    "            combined_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading existing file: {e}\")\n",
    "            print(\"Creating a new file instead.\")\n",
    "            combined_df = df\n",
    "    else:\n",
    "        combined_df = df\n",
    "    with pd.ExcelWriter(file_path, engine='openpyxl', mode='w') as writer:\n",
    "        combined_df.to_excel(writer, index=False)\n",
    "\n",
    "    print(f\"Data successfully saved to {file_path}\")\n",
    "\n",
    "def create_seasonality_series(n_variables=n_variables, n_timesteps=n_timesteps, seasonality_period=50, noise_level=0.1, save_data=False):\n",
    "        \n",
    "    # Create time vector\n",
    "    time = np.arange(n_timesteps)\n",
    "\n",
    "    # Generate synthetic multivariate time series data\n",
    "    data = np.zeros((n_timesteps, n_variables))\n",
    "    for i in range(n_variables):\n",
    "        # Add high seasonality with different phases and amplitudes\n",
    "        amplitude = np.random.uniform(0.5, 1.5)\n",
    "        phase = np.random.uniform(0, 2 * np.pi)\n",
    "        data[:, i] = amplitude * np.sin(2 * np.pi * time / seasonality_period + phase)\n",
    "\n",
    "        # Add some noise\n",
    "        data[:, i] += np.random.normal(0, noise_level, n_timesteps)\n",
    "\n",
    "    # Convert to a pandas DataFrame for visualization and manipulation\n",
    "    columns = [f\"Variable_{i+1}\" for i in range(n_variables)]\n",
    "    synthetic_data = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    # Plot the generated time series data\n",
    "    synthetic_data.plot(figsize=(12, 6))\n",
    "    plt.title(\"Synthetic Multivariate Time Series with High Seasonality\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Values\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    if save_data:\n",
    "        directory = '/home/noam.koren/multiTS/NFT/data/seasonality/'\n",
    "        file_path = os.path.join(directory, 'seasonality.pkl')\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        synthetic_data.to_pickle(file_path)\n",
    "\n",
    "def process_component_info(seasonality_trend_info, series_name):\n",
    "    component_info = pd.DataFrame(seasonality_trend_info)\n",
    "    component_info['Series_Name'] = series_name\n",
    "\n",
    "    averages = component_info.select_dtypes(include='number').mean()\n",
    "    averages_row = {col: averages[col] if col in averages.index else None for col in component_info.columns}\n",
    "    averages_row['Series_Name'] = series_name\n",
    "    averages_row['Series'] = 'Average'\n",
    "    averages_row['Seasonality_Precetage'] = averages_row['Seasonality_Dominance'] / (averages_row['Seasonality_Dominance']  + averages_row['Trend_Dominance'])\n",
    "    averages_df = pd.DataFrame([averages_row])\n",
    "\n",
    "    component_info = pd.concat([component_info, averages_df], ignore_index=True)\n",
    "\n",
    "    cols = ['Series_Name'] + [col for col in component_info.columns if col != 'Series_Name']\n",
    "    component_info = component_info[cols]\n",
    "\n",
    "    print(\"Averaged Component Info:\")\n",
    "    print(averages_df[cols].head(6))\n",
    "\n",
    "    return component_info, averages_df[cols]\n",
    "\n",
    "def create_time_series(\n",
    "    num_series, \n",
    "    num_points,\n",
    "    trend_amplitude=0.5, \n",
    "    noise_level=1, \n",
    "    seasonal_amplitude=1, # (0.5, 1.5)\n",
    "    save_data=False,\n",
    "    series_name=None,\n",
    "    compute_components=True  # parameter to compute seasonality and trend proportions\n",
    "    ):\n",
    "    time = np.arange(num_points)\n",
    "    data = {}\n",
    "    seasonality_trend_info = []\n",
    "\n",
    "    for i in range(num_series):\n",
    "        trend = trend_amplitude * (time / num_points)\n",
    "        \n",
    "        phase = np.random.uniform(0, 2 * np.pi)\n",
    "        seasonal = (seasonal_amplitude * (i+1)) * np.sin(2 * np.pi * time / (50 + i * 5) + i * phase)\n",
    "    \n",
    "        noise = np.random.normal(0, noise_level, num_points)\n",
    "        \n",
    "        series = trend + seasonal + noise\n",
    "        data[f\"Series_{i+1}\"] = series\n",
    "        \n",
    "        if compute_components:\n",
    "            total_variance = np.var(series)\n",
    "            seasonal_variance = np.var(seasonal)\n",
    "            trend_variance = np.var(trend)\n",
    "            noise_variance = np.var(noise)\n",
    "        \n",
    "\n",
    "            seasonality_ratio = seasonal_variance / total_variance\n",
    "            trend_ratio = trend_variance / total_variance\n",
    "            trend_to_seasonality_ratio = trend_variance / seasonal_variance if seasonal_variance > 0 else np.inf\n",
    "            seasonality_to_trend_ratio = seasonal_variance / trend_variance if trend_variance > 0 else np.inf\n",
    "            noise_ratio = noise_variance / total_variance\n",
    "            \n",
    "            trend_dominance = max(0, 1 - (noise_variance / (noise_variance + trend_variance)))\n",
    "            seasonality_dominance = max(0, 1 - (noise_variance / (noise_variance + seasonal_variance)))\n",
    "\n",
    "            seasonality_trend_info.append({\n",
    "                'Series': f\"Series_{i+1}\",\n",
    "                'Seasonality_Dominance': seasonality_dominance,\n",
    "                'Trend_Dominance': trend_dominance,\n",
    "                'Seasonality_Ratio': seasonality_ratio,\n",
    "                'Trend_Ratio': trend_ratio,\n",
    "                'Trend_to_Seasonality_Ratio': trend_to_seasonality_ratio,\n",
    "                'Seasonality_to_Trend_Ratio': seasonality_to_trend_ratio,\n",
    "                'Noise_Ratio': noise_ratio\n",
    "            })\n",
    "\n",
    "    synthetic_data = pd.DataFrame(data, index=time)\n",
    "    # synthetic_data.plot(figsize=(10, 6), title=\"Synthetic Multivariate Time Series\")\n",
    "    # plt.xlabel(\"Time\")\n",
    "    # plt.ylabel(\"Value\")\n",
    "    # plt.show()\n",
    "    \n",
    "    component_info = process_component_info(seasonality_trend_info, series_name=series_name)\n",
    "\n",
    "    \n",
    "    if save_data:\n",
    "        directory = f'/home/noam.koren/multiTS/NFT/data/{series_name}/'\n",
    "        file_path = os.path.join(directory, f'{series_name}.pkl')\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        synthetic_data.to_pickle(file_path)\n",
    "        \n",
    "        save_to_excel(component_info, '/home/noam.koren/multiTS/NFT/models/tests/analyse_data/data_components.xlsx')\n",
    "\n",
    "  \n",
    "# for seasonal_amplitude in [4, 5, 6, 7, 8, 9, 10]:\n",
    "#     for trend_amplitude in [0.50]:\n",
    "#         # create_time_series(\n",
    "#         #     num_series=n_variables, \n",
    "#         #     num_points=n_timesteps,\n",
    "#         #     trend_amplitude=trend_amplitude, \n",
    "#         #     noise_level=0.1, \n",
    "#         #     seasonal_amplitude=seasonal_amplitude,\n",
    "#         #     save_data=True,\n",
    "#         #     series_name=f'seasonal_{seasonal_amplitude}_trend_{trend_amplitude}'\n",
    "#         #     )\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_components(y):\n",
    "\n",
    "    # Step 2: Apply Fast Fourier Transform (FFT)\n",
    "    n = len(y)  # Length of the signal\n",
    "    fft_vals = np.fft.fft(y)  # FFT\n",
    "    frequencies = np.fft.fftfreq(n)  # Frequency bins\n",
    "\n",
    "    # Step 3: Identify dominant frequencies (filtering)\n",
    "    # Sort by the magnitude of the FFT values and retain top frequencies\n",
    "    magnitude = np.abs(fft_vals)\n",
    "    threshold = 0.05 * max(magnitude)  # Keep frequencies with > 5% of max amplitude\n",
    "    filtered_fft_vals = fft_vals.copy()\n",
    "    filtered_fft_vals[magnitude < threshold] = 0  # Zero out low-amplitude components\n",
    "\n",
    "    # Step 4: Reconstruct the seasonality using Inverse FFT (IFFT)\n",
    "    seasonality = np.fft.ifft(filtered_fft_vals).real\n",
    "    \n",
    "    # Step 5: Filter out high frequencies (low-pass filter)\n",
    "    # Define a cutoff frequency: retain only the low frequencies\n",
    "    cutoff = 0.05  # Adjust this value based on your data (0 < cutoff < 1)\n",
    "    filtered_fft_vals = fft_vals.copy()\n",
    "    filtered_fft_vals[np.abs(frequencies) > cutoff] = 0  # Zero out high-frequency components\n",
    "\n",
    "    # Step 4: Reconstruct the trend using Inverse FFT (IFFT)\n",
    "    trend = np.fft.ifft(filtered_fft_vals).real\n",
    "    \n",
    "    noise = y - seasonality - trend\n",
    "    \n",
    "    return seasonality, trend, noise\n",
    " \n",
    "def plot_componentes(y, seasonality, trend, noise):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y, label='Original Series', color='blue')\n",
    "    plt.plot(seasonality, label='Extracted Seasonality', color='red')\n",
    "    plt.plot(trend, label='Extracted Trend', color='orange')\n",
    "    plt.plot(noise, label='Extracted Noise', color='green')\n",
    "    plt.title('Components Extraction using Fourier Transform')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    " \n",
    "def calculate_components_dominance(data, data_name, save_Excel=False):\n",
    "    seasonality_trend_info = []\n",
    "    for column in data.columns:\n",
    "        y = data[column]\n",
    "            \n",
    "        seasonal, trend, noise = get_components(y)\n",
    "\n",
    "        total_variance = y.var()\n",
    "        seasonal_variance = seasonal.var()\n",
    "        trend_variance = trend.var()\n",
    "        noise_variance = noise.var()\n",
    "\n",
    "        seasonality_dominance = max(0, 1 - (noise_variance / (noise_variance + seasonal_variance)))\n",
    "        trend_dominance = max(0, 1 - (noise_variance / (noise_variance + trend_variance)))\n",
    "        seasonality_precetage = seasonality_dominance / (seasonality_dominance + trend_dominance) if (seasonality_dominance + trend_dominance) != 0 else 0\n",
    "        \n",
    "        print(seasonality_dominance, trend_dominance, seasonality_precetage)\n",
    "        \n",
    "        seasonality_trend_info.append({\n",
    "            'Series': column,\n",
    "            'Seasonality_Dominance': seasonality_dominance,\n",
    "            'Trend_Dominance': trend_dominance,\n",
    "            'Seasonality_Precetage': seasonality_precetage,\n",
    "            'total_variance': total_variance,\n",
    "            'seasonal_variance': seasonal_variance,\n",
    "            'trend_variance': trend_variance,\n",
    "            'noise_variance': noise_variance,\n",
    "            })\n",
    "        \n",
    "    component_info, averages_df = process_component_info(seasonality_trend_info, series_name=data_name)\n",
    "    if save_Excel:\n",
    "        save_to_excel(averages_df, f'/home/noam.koren/multiTS/NFT/models/tests/analyse_data/real_data_components.xlsx')\n",
    "\n",
    "    return component_info, averages_df\n",
    "\n",
    "# pkl_dir = \"/home/noam.koren/multiTS/NFT/data/ecg/pkl_files/\"\n",
    "# for pkl_file in os.listdir(pkl_dir):\n",
    "#     if pkl_file.endswith(\".pkl\"):  # Check if the file is a .pkl file\n",
    "#         file_path = os.path.join(pkl_dir, pkl_file)  # Full path to the file\n",
    "#         data = pd.read_pickle(file_path)\n",
    "#         component_info, averages_df = calculate_components_dominance(data, pkl_file[:-4], save_Excel=True)\n",
    "\n",
    "\n",
    "# pkl_dir = \"/home/noam.koren/multiTS/NFT/data/eeg_single/\"\n",
    "# for pkl_file in os.listdir(pkl_dir):\n",
    "#     if pkl_file.endswith(\".pkl\"):  # Check if the file is a .pkl file\n",
    "#         file_path = os.path.join(pkl_dir, pkl_file)  # Full path to the file\n",
    "#         data = pd.read_pickle(file_path)\n",
    "#         component_info, averages_df = calculate_components_dominance(data, pkl_file[:-4], save_Excel=True)\n",
    "        \n",
    "# for dataset in ['electricity', 'exchange', 'illness', 'traffic']:\n",
    "#     data = pd.read_pickle(f'/home/noam.koren/multiTS/NFT/data/{dataset}/{dataset}_no_date.pkl')\n",
    "#     component_info, averages_df = calculate_components_dominance(data, dataset, save_Excel=True)\n",
    "\n",
    "\n",
    "# data = pd.read_pickle('/home/noam.koren/multiTS/NFT/data/electricity/mini_elctricity.pkl')\n",
    "# component_info, averages_df = calculate_components_dominance(data, dataset, save_Excel=True)\n",
    "\n",
    "\n",
    "data = pd.read_pickle('/home/noam.koren/multiTS/NFT/data/air_quality/air_quality.pkl')\n",
    "component_info, averages_df = calculate_components_dominance(data, 'air_quality', save_Excel=True)\n",
    "\n",
    "# for n in ['AE000041196', 'AEM00041194', 'AEM00041217']:\n",
    "#     data = pd.read_pickle(f'/home/noam.koren/multiTS/NFT/data/noaa/noaa_ghcn/noaa_pkl/{n}.pkl')\n",
    "#     component_info, averages_df = calculate_components_dominance(data, n, save_Excel=True)\n",
    "\n",
    "# pkl_dir = \"/home/noam.koren/multiTS/NFT/data/noaa/noaa_ghcn/years/embedded/AEM00041217/\"\n",
    "# for pkl_file in os.listdir(pkl_dir):\n",
    "#     if pkl_file.endswith(\".pkl\"):  # Check if the file is a .pkl file\n",
    "#         file_path = os.path.join(pkl_dir, pkl_file)  # Full path to the file\n",
    "#         data = pd.read_pickle(file_path)\n",
    "#         component_info, averages_df = calculate_components_dominance(data, pkl_file[:-4], save_Excel=True)\n",
    "        \n",
    "# csv_dir = '/home/noam.koren/multiTS/NFT/data/chorales/chorales_csvs'\n",
    "# for csv_file in os.listdir(csv_dir):\n",
    "#     if csv_file.endswith(\".csv\"):  # Check if the file is a .pkl file\n",
    "#         file_path = os.path.join(csv_dir, csv_file)  # Full path to the file\n",
    "#         data = pd.read_csv(file_path)\n",
    "#         component_info, averages_df = calculate_components_dominance(data, csv_file[:-4], save_Excel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_path = '/home/noam.koren/multiTS/NFT/data/electricity/electricity_no_date.pkl'\n",
    "d = pd.read_pickle(pkl_path)\n",
    "column_indices_to_extract = [20, 44, 92, 64, 58, 71, 47, 13, 21, 22, 3, 5, 10, 16, 7, 15, 2, 69, 4, 6, 1, 29, 0, 8, 25, 55, 122, 125, 131, 99, 84, 9, 50, 113, 117, 118]\n",
    "mini_elctricity = d.iloc[:, column_indices_to_extract]\n",
    "mini_elctricity.to_pickle('/home/noam.koren/multiTS/NFT/data/electricity/mini_elctricity.pkl')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d.columns)\n",
    "d['0', '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# csv_dir = '/home/noam.koren/multiTS/NFT/data/chorales/chorales_csvs'\n",
    "\n",
    "# data_names = []\n",
    "# data_lengths = []\n",
    "\n",
    "# for csv_file in os.listdir(csv_dir):\n",
    "#     if csv_file.endswith(\".csv\"):  # Check if the file is a .csv file\n",
    "#         file_path = os.path.join(csv_dir, csv_file)  # Full path to the file\n",
    "#         data = pd.read_csv(file_path)\n",
    "#         data_names.append(csv_file[:-4])  # Remove .csv extension\n",
    "#         data_lengths.append(len(data))  # Get length of the data\n",
    "\n",
    "# output_df = pd.DataFrame({\n",
    "#     'Data Name': data_names,\n",
    "#     'Data Length': data_lengths\n",
    "# })\n",
    "\n",
    "# output_excel_path = '/home/noam.koren/multiTS/NFT/data/chorales/chorales_data_summary.xlsx'\n",
    "# output_df.to_excel(output_excel_path, index=False)\n",
    "\n",
    "# output_excel_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slope (m): 13.397449521785292, Intercept (b): 0.10527098831033062\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data points\n",
    "x = np.array([0.44, 0.46, 0.48, 0.50, 0.51, 0.60])\n",
    "y = np.array([1.82, 4.16, 12.9, 6.06, 9.99, 5.76])\n",
    "\n",
    "# Calculate linear regression\n",
    "m, b = np.polyfit(x, y, 1)  # 1 indicates linear fit\n",
    "print(f\"Slope (m): {m}, Intercept (b): {b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
