{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from nbeats_pytorch.model import NBeatsNet\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch.optim as optim\n",
    "\n",
    "sys.path.append('/home/../multiTS/NFT/')\n",
    "from dicts import data_to_num_vars_dict\n",
    "from models.training_functions import read_all_data_and_print_stats, train_model, evaluate_model, save_model\n",
    "from models.baseline_models.base_models import FC, LSTM, CNN, TCN, TimeSeriesTransformer\n",
    "\n",
    "data = 'seasonality'\n",
    "\n",
    "NUM_OF_VARS = data_to_num_vars_dict[data]\n",
    "LOOKBACK = 5\n",
    "HORIZON = 1\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 1\n",
    "PLOT_EPOCH = NUM_EPOCHS\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# save_model_path = f\"NFT/models/baseline_models/{data}/\"\n",
    "\n",
    "n_people = 600\n",
    "n_rows = 700\n",
    "    \n",
    "class dataset(Dataset):\n",
    "    def __init__(self, data, target):\n",
    "        self.input = data\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        return self.input[idx], self.target[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data == 'ecg':\n",
    "    data_path = f\"NFT/data/{data}/{data}_{LOOKBACK}l_{HORIZON}h_{n_people}people_{n_rows}rows/\"\n",
    "else:\n",
    "    data_path = f\"NFT/data/{data}/{data}_{LOOKBACK}l_{HORIZON}h/\"\n",
    "\n",
    "train_X, train_y, val_X, val_y, test_X, test_y = read_all_data_and_print_stats(data_path=data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_X, train_y, val_X, val_y, test_X, test_y):\n",
    "    def print_evaluation(train_mse, val_mse, test_mse,\n",
    "                        train_smape, val_smape, test_smape, \n",
    "                        train_mape, val_mape, test_mape,\n",
    "                        train_mase, val_mase, test_mase):\n",
    "                                    \n",
    "        print(f\"MSE:\\n\"\n",
    "        f\"train MSE = {train_mse}\\n\"\n",
    "        f\"val MSE = {val_mse}\\n\"\n",
    "        f\"test MSE = {test_mse}\\n\")\n",
    "        \n",
    "        print(f\"sMAPE:\\n\"\n",
    "        f\"train sMAPE = {train_smape}\\n\"\n",
    "        f\"val sMAPE = {val_smape}\\n\"\n",
    "        f\"test sMAPE = {test_smape}\\n\")\n",
    "        \n",
    "        print(f\"MAPE:\\n\"\n",
    "        f\"train MAPE = {train_mape}\\n\"\n",
    "        f\"val MAPE = {val_mape}\\n\"\n",
    "        f\"test MAPE = {test_mape}\\n\")\n",
    "        \n",
    "        print(f\"MASE:\\n\"\n",
    "        f\"train MAPE = {train_mase}\\n\"\n",
    "        f\"val MAPE = {val_mase}\\n\"\n",
    "        f\"test MAPE = {test_mase}\\n\")\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # if isinstance(train_X, np.ndarray): train_X = torch.from_numpy(train_X)\n",
    "    # if isinstance(val_X, np.ndarray): train_X = torch.from_numpy(val_X)\n",
    "    # if isinstance(test_X, np.ndarray): train_X = torch.from_numpy(test_X)\n",
    "\n",
    "    train_pred = model.predict(train_X)\n",
    "    val_pred = model.predict(val_X)\n",
    "    test_pred = model.predict(test_X)\n",
    "    \n",
    "    if isinstance(train_pred, np.ndarray): train_pred = torch.from_numpy(train_pred)\n",
    "    if isinstance(val_pred, np.ndarray): val_pred = torch.from_numpy(val_pred)\n",
    "    if isinstance(test_pred, np.ndarray): test_pred = torch.from_numpy(test_pred)\n",
    "\n",
    "    train_mse = F.mse_loss(train_pred.to(device), train_y.to(device))\n",
    "    val_mse = F.mse_loss(val_pred.to(device), val_y.to(device))\n",
    "    test_mse = F.mse_loss(test_pred.to(device), test_y.to(device))\n",
    "\n",
    "    train_smape = calculate_smape(train_pred, train_y)\n",
    "    val_smape = calculate_smape(val_pred, val_y)\n",
    "    test_smape = calculate_smape(test_pred, test_y)\n",
    "    \n",
    "    train_mape = calculate_mape(train_pred, train_y)\n",
    "    val_mape = calculate_mape(val_pred, val_y)\n",
    "    test_mape = calculate_mape(test_pred, test_y)\n",
    "    \n",
    "    train_mase = calculate_mase(train_pred, train_y)\n",
    "    val_mase = calculate_mase(val_pred, val_y)\n",
    "    test_mase = calculate_mase(test_pred, test_y)\n",
    "    \n",
    "    print_evaluation(train_mse, val_mse, test_mse,\n",
    "                     train_smape, val_smape, test_smape, \n",
    "                     train_mape, val_mape, test_mape,\n",
    "                     train_mase, val_mase, test_mase)\n",
    "    \n",
    "    return train_pred, val_pred, test_pred, train_mse, test_mse, train_smape, test_smape, train_mape, test_mape, train_mase, test_mase\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcn_model = TCN(lookback=LOOKBACK, \n",
    "                horizon=HORIZON, \n",
    "                num_vars=NUM_OF_VARS, \n",
    "                device=device,\n",
    "                num_channels=[25, 50], \n",
    "                kernel_size=2, \n",
    "                dropout=0.2).to(device)\n",
    "\n",
    "train_model(dataset=dataset,\n",
    "            epochs=NUM_EPOCHS,\n",
    "            model=tcn_model, \n",
    "            train_X=train_X.to(device), train_y=train_y.to(device),\n",
    "            val_X=val_X.to(device), val_y=val_y.to(device), \n",
    "            device=device, \n",
    "            lookback=LOOKBACK,\n",
    "            horizon=HORIZON, \n",
    "            n_vars=NUM_OF_VARS, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            print_epoch=PLOT_EPOCH, \n",
    "            path_to_save_prediction_plots=None\n",
    "            )\n",
    "\n",
    "evaluate_model(tcn_model, train_X, train_y, val_X, val_y, test_X, test_y)\n",
    "\n",
    "# save_model(tcn_model, \"tcn\", data, save_model_path, LOOKBACK, HORIZON, n_people, n_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "from time import time\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.functional import mse_loss, l1_loss, binary_cross_entropy, cross_entropy\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "\n",
    "class NBeatsNet(nn.Module):\n",
    "    SEASONALITY_BLOCK = 'seasonality'\n",
    "    TREND_BLOCK = 'trend'\n",
    "    GENERIC_BLOCK = 'generic'\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            stack_types=(TREND_BLOCK, SEASONALITY_BLOCK),\n",
    "            nb_blocks_per_stack=3,\n",
    "            forecast_length=5,\n",
    "            backcast_length=10,\n",
    "            thetas_dim=(4, 8),\n",
    "            share_weights_in_stack=False,\n",
    "            hidden_layer_units=256,\n",
    "            nb_harmonics=None\n",
    "    ):\n",
    "        super(NBeatsNet, self).__init__()\n",
    "        self.forecast_length = forecast_length\n",
    "        self.backcast_length = backcast_length\n",
    "        self.hidden_layer_units = hidden_layer_units\n",
    "        self.nb_blocks_per_stack = nb_blocks_per_stack\n",
    "        self.share_weights_in_stack = share_weights_in_stack\n",
    "        self.nb_harmonics = nb_harmonics\n",
    "        self.stack_types = stack_types\n",
    "        self.stacks = []\n",
    "        self.thetas_dim = thetas_dim\n",
    "        self.parameters = []\n",
    "        print('| N-Beats')\n",
    "        for stack_id in range(len(self.stack_types)):\n",
    "            self.stacks.append(self.create_stack(stack_id))\n",
    "        self.parameters = nn.ParameterList(self.parameters)\n",
    "        self._loss = None\n",
    "        self._opt = None\n",
    "        self._gen_intermediate_outputs = False\n",
    "        self._intermediary_outputs = []\n",
    "\n",
    "    def create_stack(self, stack_id):\n",
    "        stack_type = self.stack_types[stack_id]\n",
    "        print(f'| --  Stack {stack_type.title()} (#{stack_id}) (share_weights_in_stack={self.share_weights_in_stack})')\n",
    "        blocks = []\n",
    "        for block_id in range(self.nb_blocks_per_stack):\n",
    "            block_init = NBeatsNet.select_block(stack_type)\n",
    "            if self.share_weights_in_stack and block_id != 0:\n",
    "                block = blocks[-1]  # pick up the last one when we share weights.\n",
    "            else:\n",
    "                block = block_init(\n",
    "                    self.hidden_layer_units, self.thetas_dim[stack_id],\n",
    "                    self.device, self.backcast_length, self.forecast_length,\n",
    "                    self.nb_harmonics\n",
    "                )\n",
    "                self.parameters.extend(block.parameters())\n",
    "            print(f'     | -- {block}')\n",
    "            blocks.append(block)\n",
    "        return blocks\n",
    "\n",
    "    def disable_intermediate_outputs(self):\n",
    "        self._gen_intermediate_outputs = False\n",
    "\n",
    "    def enable_intermediate_outputs(self):\n",
    "        self._gen_intermediate_outputs = True\n",
    "\n",
    "    def save(self, filename: str):\n",
    "        torch.save(self, filename)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(f, map_location=None, pickle_module=pickle, **pickle_load_args):\n",
    "        return torch.load(f, map_location, pickle_module, **pickle_load_args)\n",
    "\n",
    "    @staticmethod\n",
    "    def select_block(block_type):\n",
    "        if block_type == NBeatsNet.SEASONALITY_BLOCK:\n",
    "            return SeasonalityBlock\n",
    "        elif block_type == NBeatsNet.TREND_BLOCK:\n",
    "            return TrendBlock\n",
    "        else:\n",
    "            return GenericBlock\n",
    "\n",
    "    def compile(self, loss: str, optimizer: Union[str, Optimizer]):\n",
    "        if loss == 'mae':\n",
    "            loss_ = l1_loss\n",
    "        elif loss == 'mse':\n",
    "            loss_ = mse_loss\n",
    "        elif loss == 'cross_entropy':\n",
    "            loss_ = cross_entropy\n",
    "        elif loss == 'binary_crossentropy':\n",
    "            loss_ = binary_cross_entropy\n",
    "        else:\n",
    "            raise ValueError(f'Unknown loss name: {loss}.')\n",
    "        # noinspection PyArgumentList\n",
    "        if isinstance(optimizer, str):\n",
    "            if optimizer == 'adam':\n",
    "                opt_ = optim.Adam\n",
    "            elif optimizer == 'sgd':\n",
    "                opt_ = optim.SGD\n",
    "            elif optimizer == 'rmsprop':\n",
    "                opt_ = optim.RMSprop\n",
    "            else:\n",
    "                raise ValueError(f'Unknown opt name: {optimizer}.')\n",
    "            opt_ = opt_(lr=1e-4, params=self.parameters())\n",
    "        else:\n",
    "            opt_ = optimizer\n",
    "        self._opt = opt_\n",
    "        self._loss = loss_\n",
    "\n",
    "    def fit(self, x_train, y_train, validation_data=None, epochs=10, batch_size=32):\n",
    "\n",
    "        def split(arr, size):\n",
    "            arrays = []\n",
    "            while len(arr) > size:\n",
    "                slice_ = arr[:size]\n",
    "                arrays.append(slice_)\n",
    "                arr = arr[size:]\n",
    "            arrays.append(arr)\n",
    "            return arrays\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            x_train_list = split(x_train, batch_size)\n",
    "            y_train_list = split(y_train, batch_size)\n",
    "            assert len(x_train_list) == len(y_train_list)\n",
    "            shuffled_indices = list(range(len(x_train_list)))\n",
    "            random.shuffle(shuffled_indices)\n",
    "            self.train()\n",
    "            train_loss = []\n",
    "            timer = time()\n",
    "            for batch_id in shuffled_indices:\n",
    "                batch_x, batch_y = x_train_list[batch_id], y_train_list[batch_id]\n",
    "                self._opt.zero_grad()\n",
    "                _, forecast = self(batch_x.clone().detach().to(self.device))\n",
    "                loss = self._loss(forecast, squeeze_last_dim(batch_y.clone().detach().to(self.device)))\n",
    "                train_loss.append(loss.item())\n",
    "                loss.backward()\n",
    "                self._opt.step()\n",
    "            elapsed_time = time() - timer\n",
    "            train_loss = np.mean(train_loss)\n",
    "\n",
    "            test_loss = '[undefined]'\n",
    "            if validation_data is not None:\n",
    "                x_test, y_test = validation_data\n",
    "                self.eval()\n",
    "                _, forecast = self(x_test.clone().detach().to(self.device))\n",
    "                test_loss = self._loss(forecast, squeeze_last_dim(y_test.clone().detach())).item()\n",
    "\n",
    "            num_samples = len(x_train_list)\n",
    "            time_per_step = int(elapsed_time / num_samples * 1000)\n",
    "            print(f'Epoch {str(epoch + 1).zfill(len(str(epochs)))}/{epochs}')\n",
    "            print(f'{num_samples}/{num_samples} [==============================] - ')\n",
    "            print(f'{int(elapsed_time)}s {time_per_step}ms/step - ')\n",
    "            # print(f'loss: {train_loss:.4f} - val_loss: {test_loss:.4f}')\n",
    "            \n",
    "    def predict(self, x, return_backcast=False):\n",
    "        self.eval()\n",
    "        b, f = self(x.clone().detach().to(self.device))\n",
    "        b, f = b.detach().cpu().numpy(), f.detach().cpu().numpy()\n",
    "        if len(x.shape) == 3:\n",
    "            b = np.expand_dims(b, axis=-1)\n",
    "            f = np.expand_dims(f, axis=-1)\n",
    "        if return_backcast:\n",
    "            return b\n",
    "        return f\n",
    "\n",
    "    @staticmethod\n",
    "    def name():\n",
    "        return 'NBeatsPytorch'\n",
    "\n",
    "    def get_generic_and_interpretable_outputs(self):\n",
    "        g_pred = sum([a['value'][0] for a in self._intermediary_outputs if 'generic' in a['layer'].lower()])\n",
    "        i_pred = sum([a['value'][0] for a in self._intermediary_outputs if 'generic' not in a['layer'].lower()])\n",
    "        outputs = {o['layer']: o['value'][0] for o in self._intermediary_outputs}\n",
    "        return g_pred, i_pred, outputs\n",
    "\n",
    "    def forward(self, backcast):\n",
    "        self._intermediary_outputs = []\n",
    "        backcast = squeeze_last_dim(backcast)\n",
    "        forecast = torch.zeros(size=(backcast.size()[0], self.forecast_length,))  # maybe batch size here.\n",
    "        for stack_id in range(len(self.stacks)):\n",
    "            for block_id in range(len(self.stacks[stack_id])):\n",
    "                b, f = self.stacks[stack_id][block_id](backcast)\n",
    "                backcast = backcast.to(self.device) - b\n",
    "                forecast = forecast.to(self.device) + f\n",
    "                block_type = self.stacks[stack_id][block_id].__class__.__name__\n",
    "                layer_name = f'stack_{stack_id}-{block_type}_{block_id}'\n",
    "                if self._gen_intermediate_outputs:\n",
    "                    self._intermediary_outputs.append({'value': f.detach().numpy(), 'layer': layer_name})\n",
    "        return backcast, forecast\n",
    "\n",
    "\n",
    "def squeeze_last_dim(tensor):\n",
    "    if len(tensor.shape) == 3 and tensor.shape[-1] == 1:  # (128, 10, 1) => (128, 10).\n",
    "        return tensor[..., 0]\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def seasonality_model(thetas, t, device):\n",
    "    p = thetas.size()[-1]\n",
    "    assert p <= thetas.shape[1], 'thetas_dim is too big.'\n",
    "    p1, p2 = (p // 2, p // 2) if p % 2 == 0 else (p // 2, p // 2 + 1)\n",
    "    s1 = torch.tensor(np.array([np.cos(2 * np.pi * i * t) for i in range(p1)])).float()  # H/2-1\n",
    "    s2 = torch.tensor(np.array([np.sin(2 * np.pi * i * t) for i in range(p2)])).float()\n",
    "    S = torch.cat([s1, s2])\n",
    "    return thetas.mm(S.to(device))\n",
    "\n",
    "\n",
    "def trend_model(thetas, t, device):\n",
    "    p = thetas.size()[-1]\n",
    "    assert p <= 4, 'thetas_dim is too big.'\n",
    "    T = torch.tensor(np.array([t ** i for i in range(p)])).float()\n",
    "    return thetas.mm(T.to(device))\n",
    "\n",
    "\n",
    "def linear_space(backcast_length, forecast_length, is_forecast=True):\n",
    "    horizon = forecast_length if is_forecast else backcast_length\n",
    "    return np.arange(0, horizon) / horizon\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, units, thetas_dim, device, backcast_length=10, forecast_length=5, share_thetas=False,\n",
    "                 nb_harmonics=None):\n",
    "        super(Block, self).__init__()\n",
    "        self.units = units\n",
    "        self.thetas_dim = thetas_dim\n",
    "        self.backcast_length = backcast_length\n",
    "        self.forecast_length = forecast_length\n",
    "        self.share_thetas = share_thetas\n",
    "        self.fc1 = nn.Linear(backcast_length, units)\n",
    "        self.fc2 = nn.Linear(units, units)\n",
    "        self.fc3 = nn.Linear(units, units)\n",
    "        self.fc4 = nn.Linear(units, units)\n",
    "        self.device = device\n",
    "        self.backcast_linspace = linear_space(backcast_length, forecast_length, is_forecast=False)\n",
    "        self.forecast_linspace = linear_space(backcast_length, forecast_length, is_forecast=True)\n",
    "        if share_thetas:\n",
    "            self.theta_f_fc = self.theta_b_fc = nn.Linear(units, thetas_dim, bias=False)\n",
    "        else:\n",
    "            self.theta_b_fc = nn.Linear(units, thetas_dim, bias=False)\n",
    "            self.theta_f_fc = nn.Linear(units, thetas_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = squeeze_last_dim(x)\n",
    "        x = F.relu(self.fc1(x.to(self.device)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "    def __str__(self):\n",
    "        block_type = type(self).__name__\n",
    "        return f'{block_type}(units={self.units}, thetas_dim={self.thetas_dim}, ' \\\n",
    "               f'backcast_length={self.backcast_length}, forecast_length={self.forecast_length}, ' \\\n",
    "               f'share_thetas={self.share_thetas}) at @{id(self)}'\n",
    "\n",
    "\n",
    "class SeasonalityBlock(Block):\n",
    "\n",
    "    def __init__(self, units, thetas_dim, device, backcast_length=10, forecast_length=5, nb_harmonics=None):\n",
    "        if nb_harmonics:\n",
    "            super(SeasonalityBlock, self).__init__(units, nb_harmonics, device, backcast_length,\n",
    "                                                   forecast_length, share_thetas=True)\n",
    "        else:\n",
    "            super(SeasonalityBlock, self).__init__(units, forecast_length, device, backcast_length,\n",
    "                                                   forecast_length, share_thetas=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = super(SeasonalityBlock, self).forward(x)\n",
    "        backcast = seasonality_model(self.theta_b_fc(x), self.backcast_linspace, self.device)\n",
    "        forecast = seasonality_model(self.theta_f_fc(x), self.forecast_linspace, self.device)\n",
    "        return backcast, forecast\n",
    "\n",
    "\n",
    "class TrendBlock(Block):\n",
    "\n",
    "    def __init__(self, units, thetas_dim, device, backcast_length=10, forecast_length=5, nb_harmonics=None):\n",
    "        super(TrendBlock, self).__init__(units, thetas_dim, device, backcast_length,\n",
    "                                         forecast_length, share_thetas=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = super(TrendBlock, self).forward(x)\n",
    "        backcast = trend_model(self.theta_b_fc(x), self.backcast_linspace, self.device)\n",
    "        forecast = trend_model(self.theta_f_fc(x), self.forecast_linspace, self.device)\n",
    "        return backcast, forecast\n",
    "\n",
    "\n",
    "class GenericBlock(Block):\n",
    "\n",
    "    def __init__(self, units, thetas_dim, device, backcast_length=10, forecast_length=5, nb_harmonics=None):\n",
    "        super(GenericBlock, self).__init__(units, thetas_dim, device, backcast_length, forecast_length)\n",
    "\n",
    "        self.backcast_fc = nn.Linear(thetas_dim, backcast_length)\n",
    "        self.forecast_fc = nn.Linear(thetas_dim, forecast_length)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # no constraint for generic arch.\n",
    "        x = super(GenericBlock, self).forward(x)\n",
    "\n",
    "        theta_b = self.theta_b_fc(x)\n",
    "        theta_f = self.theta_f_fc(x)\n",
    "\n",
    "        backcast = self.backcast_fc(theta_b)  # generic. 3.3.\n",
    "        forecast = self.forecast_fc(theta_f)  # generic. 3.3.\n",
    "\n",
    "        return backcast, forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbeats_model = NBeatsNet(\n",
    "        stack_types=(NBeatsNet.TREND_BLOCK, NBeatsNet.SEASONALITY_BLOCK),\n",
    "        forecast_length=HORIZON,\n",
    "        backcast_length=LOOKBACK,\n",
    "        nb_blocks_per_stack=2,\n",
    "        thetas_dim=(4,8),\n",
    "        device=device,\n",
    "        ).to(device)\n",
    "\n",
    "for idx in range(NUM_OF_VARS):\n",
    "        print(f\"idx={idx}\")\n",
    "        print(f\"train_X={train_X.shape}\")\n",
    "\n",
    "        train_X_idx, train_y_idx = train_X[:,:,idx], train_y[:,:,idx]\n",
    "        val_X_idx, val_y_idx = val_X[:,:,idx], val_y[:,:,idx]\n",
    "        test_X_idx, test_y_idx = test_X[:,:,idx], test_y[:,:,idx]\n",
    "        print(f\"train_X={train_X_idx.shape}\")\n",
    "        train_model(\n",
    "                dataset=dataset, \n",
    "                epochs=NUM_EPOCHS, \n",
    "                model=nbeats_model, \n",
    "                train_X=train_X_idx.to(device), train_y=train_y_idx.to(device),\n",
    "                val_X=val_X_idx.to(device), val_y=val_y_idx.to(device), \n",
    "                device=device, \n",
    "                lookback=LOOKBACK,\n",
    "                horizon=HORIZON, \n",
    "                n_vars=NUM_OF_VARS, \n",
    "                batch_size=BATCH_SIZE, \n",
    "                print_epoch=PLOT_EPOCH, \n",
    "                path_to_save_prediction_plots=None)\n",
    "        \n",
    "        evaluate_model(nbeats_model, train_X_idx, train_y_idx, val_X_idx, val_y_idx, test_X_idx, test_y_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_model = FC(input_dim=NUM_OF_VARS, \n",
    "              output_dim=NUM_OF_VARS,\n",
    "              horizon=HORIZON, \n",
    "              ).to(device)\n",
    "\n",
    "train_model(dataset=dataset, \n",
    "            epochs=NUM_EPOCHS, \n",
    "            model=fc_model, \n",
    "            train_X=train_X.to(device), train_y=train_y.to(device),\n",
    "            val_X=val_X.to(device), val_y=val_y.to(device), \n",
    "            device=device, \n",
    "            lookback=LOOKBACK,\n",
    "            horizon=HORIZON, \n",
    "            n_vars=NUM_OF_VARS, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            print_epoch=PLOT_EPOCH, \n",
    "            path_to_save_prediction_plots=None)\n",
    "\n",
    "evaluate_model(fc_model, train_X.to(device), train_y.to(device), val_X.to(device), val_y.to(device), test_X, test_y)\n",
    "\n",
    "# save_model(fc_model, \"fc\", data, save_model_path, LOOKBACK, HORIZON, n_people, n_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTM(num_vars=NUM_OF_VARS, \n",
    "                     lookback=LOOKBACK, \n",
    "                     horizon=HORIZON, \n",
    "                     hidden_dim=50, \n",
    "                     num_layers=2, \n",
    "                     device=device).to(device)\n",
    "\n",
    "train_model(dataset=dataset, \n",
    "            epochs=NUM_EPOCHS, \n",
    "            model=lstm_model, \n",
    "            train_X=train_X.to(device), train_y=train_y.to(device),\n",
    "            val_X=val_X.to(device), val_y=val_y.to(device), \n",
    "            device=device, \n",
    "            lookback=LOOKBACK,\n",
    "            horizon=HORIZON, \n",
    "            n_vars=NUM_OF_VARS, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            print_epoch=PLOT_EPOCH, \n",
    "            path_to_save_prediction_plots=None)\n",
    "\n",
    "evaluate_model(lstm_model, train_X, train_y, val_X, val_y, test_X, test_y)\n",
    "\n",
    "save_model(lstm_model, \"lstm\", data, save_model_path, LOOKBACK, HORIZON, n_people, n_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CNN(lookback=LOOKBACK, \n",
    "                horizon=HORIZON, \n",
    "                num_vars=NUM_OF_VARS,\n",
    "                device=device,\n",
    "                num_channels=[16], \n",
    "                kernel_size=2, \n",
    "                dropout=0.2).to(device)\n",
    "\n",
    "train_model(dataset=dataset,\n",
    "            epochs=NUM_EPOCHS,\n",
    "            model=cnn_model, \n",
    "            train_X=train_X.to(device), train_y=train_y.to(device),\n",
    "            val_X=val_X.to(device), val_y=val_y.to(device), \n",
    "            device=device, \n",
    "            lookback=LOOKBACK,\n",
    "            horizon=HORIZON, \n",
    "            n_vars=NUM_OF_VARS, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            print_epoch=PLOT_EPOCH, \n",
    "            path_to_save_prediction_plots=None\n",
    "            )\n",
    "\n",
    "evaluate_model(cnn_model, train_X, train_y, val_X, val_y, test_X, test_y)\n",
    "\n",
    "save_model(cnn_model, \"cnn\", data, save_model_path, LOOKBACK, HORIZON, n_people, n_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = TimeSeriesTransformer(\n",
    "    lookback=LOOKBACK,\n",
    "    horizon=HORIZON,\n",
    "    num_vars=NUM_OF_VARS,\n",
    "    device=device,\n",
    "    num_layers=1,\n",
    "    num_heads=1,\n",
    "    dim_feedforward=16,\n",
    ").to(device)\n",
    "\n",
    "train_model(dataset=dataset, \n",
    "            epochs=NUM_EPOCHS, \n",
    "            model=transformer_model, \n",
    "            train_X=train_X.to(device), train_y=train_y.to(device),\n",
    "            val_X=val_X.to(device), val_y=val_y.to(device), \n",
    "            device=device, \n",
    "            lookback=LOOKBACK,\n",
    "            horizon=HORIZON, \n",
    "            n_vars=NUM_OF_VARS, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            print_epoch=PLOT_EPOCH, \n",
    "            path_to_save_prediction_plots=None)\n",
    "\n",
    "evaluate_model(transformer_model, train_X, train_y, val_X, val_y, test_X, test_y)\n",
    "\n",
    "save_model(transformer_model, \"transformer\", data, save_model_path, LOOKBACK, HORIZON, n_people, n_rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
