{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correlation between MSE reduction percentage and seasonality component is: 0.07915988888273079\n",
      "The correlation between MSE reduction percentage and trend component is: 0.07689174802108674\n",
      "The correlation between MSE reduction percentage and average variance is: 0.20057235782394525\n",
      "The correlation between average variance and seasonality component is: 0.49543682280258766\n",
      "The correlation between average variance and trend component is: 0.838586237003058\n",
      "The correlation between trend component and seasonality componente is: 0.17075620305693662\n",
      "\n",
      "The correlation between MSE reduction percentage and real_std is: 0.09467739473804553\n",
      "The correlation between real_std and trend component is: 0.059217617751036115\n",
      "The correlation between real_std and seasonality componente is: 0.05969716340506315\n",
      "\n",
      "The correlation between MSE reduction percentage and train_var is: 0.45936523970948573\n",
      "The correlation between train_var and trend component is: 0.14965647371303673\n",
      "The correlation between train_var and seasonality componente is: -0.15426355448972637\n",
      "\n",
      "The correlation between MSE reduction percentage and real_var is: 0.17704319563975104\n",
      "The correlation between average variance and real_var is: 0.036433718918945895\n",
      "The correlation between real_var and trend component is: -0.07025051975141107\n",
      "The correlation between real_var and seasonality componente is: -0.04191126733564976\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "variance_data = {\n",
    "    \"dataset\": [\"ETTh1\", \"ETTh2\", \"ETTm1\", \"ETTm2\", \"Chorales\", \"NOAA\", \"Electricity\", \"Traffic\", \"Air Quality\", \"ILI\",\n",
    "                \"ECG Single\", \"Exchange\"],\n",
    "    \"mse_reduction_percentage\": [15.14, 15.39, 15.66, 17.91, 22.46, 20.88, 31.17, 34.84, 46.04, 71.01, 77.44,82.69],\n",
    "    \"train_var\": [1.24,  0.83,  1.26,   0.88,  1.14,  1.04,  0.96,1.05,1.06,1.17,1.05,1.52],\n",
    "    \"average_trend\": [0.01, 0.07, 0.01, 0.15, 0.00, 0.65, 0.26, 0.04, 0.02, 0.00, 0.03, 0.46],\n",
    "    \"average_seasonal\": [0.01, 0.07, 0.01, 0.20, 0.02, 0.01, 0.82, 0.57, 0.11, 0.05, 0.07, 0.30],\n",
    "    \"average_var\": [0.02, 0.10, 0.02, 0.20, 0.00, 0.83, 0.47, 0.66, 0.14, 0.06, 0.09, 0.82],\n",
    "    \"real_var\": [0.68, 0.1, 0.84, 0.1, 0.24, 1.14, 0.55, 1.25, 0.5,2.67, 0.09, 0.16],\n",
    "    \"real_std\" : [0.72,0.27,0.76,0.27,0.29,1.06,0.72,1.01,0.63,1.52,0.30,0.35],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(variance_data)\n",
    "\n",
    "# Calculate the correlation for each component\n",
    "corr_seasonality = df[\"mse_reduction_percentage\"].corr(df[\"average_seasonal\"])\n",
    "corr_trend = df[\"mse_reduction_percentage\"].corr(df[\"average_trend\"])\n",
    "corr_var = df[\"mse_reduction_percentage\"].corr(df[\"average_var\"])\n",
    "corr_var_trend = df[\"average_var\"].corr(df[\"average_trend\"])\n",
    "corr_var_seasonal = df[\"average_var\"].corr(df[\"average_seasonal\"])\n",
    "corr_seasonality_trend = df[\"average_trend\"].corr(df[\"average_seasonal\"])\n",
    "\n",
    "\n",
    "print(f\"The correlation between MSE reduction percentage and seasonality component is: {corr_seasonality}\")\n",
    "print(f\"The correlation between MSE reduction percentage and trend component is: {corr_trend}\")\n",
    "print(f\"The correlation between MSE reduction percentage and average variance is: {corr_var}\")\n",
    "print(f\"The correlation between average variance and seasonality component is: {corr_var_seasonal}\")\n",
    "print(f\"The correlation between average variance and trend component is: {corr_var_trend}\")\n",
    "print(f\"The correlation between trend component and seasonality componente is: {corr_seasonality_trend}\")\n",
    "print()\n",
    "\n",
    "mse_real_std = df[\"mse_reduction_percentage\"].corr(df[\"real_std\"])\n",
    "real_std_trend = df[\"real_std\"].corr(df[\"average_trend\"])\n",
    "real_std_seasonal = df[\"real_std\"].corr(df[\"average_seasonal\"])\n",
    "\n",
    "print(f\"The correlation between MSE reduction percentage and real_std is: {mse_real_std}\")\n",
    "print(f\"The correlation between real_std and trend component is: {real_std_trend}\")\n",
    "print(f\"The correlation between real_std and seasonality componente is: {real_std_seasonal}\")\n",
    "print()\n",
    "\n",
    "mse_train_var = df[\"mse_reduction_percentage\"].corr(df[\"train_var\"])\n",
    "train_var_trend = df[\"train_var\"].corr(df[\"average_trend\"])\n",
    "train_var_seasonal = df[\"train_var\"].corr(df[\"average_seasonal\"])\n",
    "\n",
    "print(f\"The correlation between MSE reduction percentage and train_var is: {mse_train_var}\")\n",
    "print(f\"The correlation between train_var and trend component is: {train_var_trend}\")\n",
    "print(f\"The correlation between train_var and seasonality componente is: {train_var_seasonal}\")\n",
    "print()\n",
    "\n",
    "\n",
    "mse_real_var = df[\"mse_reduction_percentage\"].corr(df[\"real_var\"])\n",
    "real_var_avg = df[\"real_var\"].corr(df[\"average_var\"])\n",
    "real_var_trend = df[\"real_var\"].corr(df[\"average_trend\"])\n",
    "real_var_seasonal = df[\"real_var\"].corr(df[\"average_seasonal\"])\n",
    "\n",
    "print(f\"The correlation between MSE reduction percentage and real_var is: {mse_real_var}\")\n",
    "print(f\"The correlation between average variance and real_var is: {real_var_avg}\")\n",
    "print(f\"The correlation between real_var and trend component is: {real_var_trend}\")\n",
    "print(f\"The correlation between real_var and seasonality componente is: {real_var_seasonal}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7824, 96, 7)\n",
      "(7824, 720, 7)\n",
      "(2160, 96, 7)\n",
      "(2160, 720, 7)\n",
      "(2160, 96, 7)\n",
      "(2160, 720, 7)\n",
      "(12144, 7)\n",
      "(12144,)\n",
      "Data etth1 has been successfully written to the Excel file.\n",
      "(7824, 96, 7)\n",
      "(7824, 720, 7)\n",
      "(2160, 96, 7)\n",
      "(2160, 720, 7)\n",
      "(2160, 96, 7)\n",
      "(2160, 720, 7)\n",
      "(12144, 7)\n",
      "(12144,)\n",
      "Data etth2 has been successfully written to the Excel file.\n",
      "(33744, 96, 7)\n",
      "(33744, 720, 7)\n",
      "(10800, 96, 7)\n",
      "(10800, 720, 7)\n",
      "(10800, 96, 7)\n",
      "(10800, 720, 7)\n",
      "(55344, 7)\n",
      "(55344,)\n",
      "Data ettm1 has been successfully written to the Excel file.\n",
      "(33744, 96, 7)\n",
      "(33744, 720, 7)\n",
      "(10800, 96, 7)\n",
      "(10800, 720, 7)\n",
      "(10800, 96, 7)\n",
      "(10800, 720, 7)\n",
      "(55344, 7)\n",
      "(55344,)\n",
      "Data ettm2 has been successfully written to the Excel file.\n",
      "(963, 10, 6)\n",
      "(963, 5, 6)\n",
      "(448, 10, 6)\n",
      "(448, 5, 6)\n",
      "(515, 10, 6)\n",
      "(515, 5, 6)\n",
      "(1926, 6)\n",
      "(1926,)\n",
      "Data chorales has been successfully written to the Excel file.\n",
      "(9105, 360, 3)\n",
      "(9105, 180, 3)\n",
      "(3318, 360, 3)\n",
      "(3318, 180, 3)\n",
      "(5247, 360, 3)\n",
      "(5247, 180, 3)\n",
      "(17670, 3)\n",
      "(17670,)\n",
      "Data noaa has been successfully written to the Excel file.\n",
      "(18284, 96, 321)\n",
      "(18284, 32, 321)\n",
      "(2600, 96, 321)\n",
      "(2600, 32, 321)\n",
      "(5228, 96, 321)\n",
      "(5228, 32, 321)\n",
      "(26112, 321)\n",
      "(26112,)\n",
      "Data electricity has been successfully written to the Excel file.\n",
      "(12136, 96, 862)\n",
      "(12136, 48, 862)\n",
      "(1708, 96, 862)\n",
      "(1708, 48, 862)\n",
      "(3460, 96, 862)\n",
      "(3460, 48, 862)\n",
      "(17304, 862)\n",
      "(17304,)\n",
      "Data traffic has been successfully written to the Excel file.\n",
      "(3503, 40, 13)\n",
      "(3503, 30, 13)\n",
      "(1359, 40, 13)\n",
      "(1359, 30, 13)\n",
      "(2074, 40, 13)\n",
      "(2074, 30, 13)\n",
      "(6936, 13)\n",
      "(6936,)\n",
      "Data air_quality has been successfully written to the Excel file.\n",
      "(87069, 100, 36)\n",
      "(87069, 25, 36)\n",
      "(34752, 100, 36)\n",
      "(34752, 25, 36)\n",
      "(52191, 100, 36)\n",
      "(52191, 25, 36)\n",
      "(174012, 36)\n",
      "(174012,)\n",
      "Data eeg_single has been successfully written to the Excel file.\n",
      "(580, 36, 7)\n",
      "(580, 60, 7)\n",
      "(37, 36, 7)\n",
      "(37, 60, 7)\n",
      "(133, 36, 7)\n",
      "(133, 60, 7)\n",
      "(750, 7)\n",
      "(750,)\n",
      "Data illness has been successfully written to the Excel file.\n",
      "(1255, 200, 12)\n",
      "(1255, 100, 12)\n",
      "(322, 200, 12)\n",
      "(322, 100, 12)\n",
      "(633, 200, 12)\n",
      "(633, 100, 12)\n",
      "(2210, 12)\n",
      "(2210,)\n",
      "Data ecg_single has been successfully written to the Excel file.\n",
      "(4495, 96, 8)\n",
      "(4495, 720, 8)\n",
      "(40, 96, 8)\n",
      "(40, 720, 8)\n",
      "(797, 96, 8)\n",
      "(797, 720, 8)\n",
      "(5332, 8)\n",
      "(5332,)\n",
      "Data exchange has been successfully written to the Excel file.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('NFT/')\n",
    "from models.training_functions import get_data\n",
    "sys.path.append('NFT/')\n",
    "from dicts import data_to_num_vars_dict\n",
    "\n",
    "def save_results(data, average_var, average_std, file_name):\n",
    "    # Output file path\n",
    "    output_file = f'/home/../multiTS/NFT/models/trained_models/{file_name}.xlsx'\n",
    "\n",
    "    # Data to be written\n",
    "    new_data = {\n",
    "        'data': [data],\n",
    "        'average_std': [average_std],\n",
    "        'average_var': [average_var]\n",
    "    }\n",
    "    new_df = pd.DataFrame(new_data)\n",
    "\n",
    "    # Check if the output file exists\n",
    "    if os.path.exists(output_file):\n",
    "        # If it exists, append the new data\n",
    "        existing_df = pd.read_excel(output_file)\n",
    "        combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "        combined_df.to_excel(output_file, index=False)\n",
    "    else:\n",
    "        # If it doesn't exist, create a new file\n",
    "        new_df.to_excel(output_file, index=False)\n",
    "\n",
    "    print(f\"Data {data} has been successfully written to the Excel file.\")\n",
    "\n",
    "data_to_params = {\n",
    "    'etth1': (96, 720, None, 10, 3), 'etth2': (96, 720, None, 10, 3), 'ettm1': (96, 720, None, 10, 3), \n",
    "    'ettm2': (96, 720, None, 10, 3), 'chorales': (10, 5, None, 10, 3), 'noaa': (360, 180, 'AE000041196', 10, 2),\n",
    "    'electricity': (96, 32, None, 10, 2), 'traffic': (96, 48, None, 10, 2),\n",
    "    'air_quality': (40, 30, None, 10, 5), 'eeg_single': (100, 25, 'test_0', 10, 2), 'illness': (36, 60, None, 10, 2), \n",
    "    'ecg_single': (200, 100, 'E00001', 10, 5), 'exchange': (96, 720, None, 10, 3)\n",
    "}\n",
    "datasets = ['etth1', 'etth2', 'ettm1', 'ettm2', 'chorales', 'noaa', 'electricity', 'traffic',\n",
    "                  'air_quality', 'eeg_single', 'illness', 'ecg_single', 'exchange']\n",
    "\n",
    "\n",
    "for data_name in datasets:\n",
    "    num_of_vars = data_to_num_vars_dict[data_name]   \n",
    "    lookback, horizon, series, epochs, blocks = data_to_params[data_name]\n",
    "    train_X, train_y, val_X, val_y, test_X, test_y = get_data(\n",
    "        data=data_name, \n",
    "        lookback=lookback, \n",
    "        horizon=horizon,\n",
    "        n_series=num_of_vars,\n",
    "        print_stats=False,\n",
    "        series=series\n",
    "    )\n",
    "    # Combine train, validation, and test sets\n",
    "    combined_data = np.concatenate((train_X[:,0,:], val_X[:,0,:], test_X[:,0,:]), axis=0)\n",
    "    print(combined_data.shape)\n",
    "    print(combined_data[:,0].shape)\n",
    "    total_var = []\n",
    "    total_std = []\n",
    "    for i in range(num_of_vars):\n",
    "        original_series = combined_data[:,0]\n",
    "        total_var.append(np.var(original_series))\n",
    "        total_std.append(np.std(original_series))\n",
    "        \n",
    "    average_var = np.mean(total_var)\n",
    "    average_std = np.mean(total_std)\n",
    "\n",
    "    save_results(data_name, average_var, average_std, file_name='x_original_data_var_std_summary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7824, 96, 7)\n",
      "(7824, 720, 7)\n",
      "(2160, 96, 7)\n",
      "(2160, 720, 7)\n",
      "(2160, 96, 7)\n",
      "(2160, 720, 7)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "save_results() got multiple values for argument 'file_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 65\u001b[0m\n\u001b[1;32m     61\u001b[0m     total\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mvar(original_series))\n\u001b[1;32m     63\u001b[0m average_var \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(total)\n\u001b[0;32m---> 65\u001b[0m \u001b[43msave_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moriginal_data_var_std_summary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: save_results() got multiple values for argument 'file_name'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('NFT/')\n",
    "from models.training_functions import get_data\n",
    "sys.path.append('NFT/')\n",
    "from dicts import data_to_num_vars_dict\n",
    "\n",
    "def save_results(data, average_var, average_std, file_name):\n",
    "    # Output file path\n",
    "    output_file = f'/home/../multiTS/NFT/models/trained_models/{file_name}.xlsx'\n",
    "\n",
    "    # Data to be written\n",
    "    new_data = {\n",
    "        'data': [data],\n",
    "        'average_std': [average_std],\n",
    "        'average_var': [average_var]\n",
    "    }\n",
    "    new_df = pd.DataFrame(new_data)\n",
    "\n",
    "    # Check if the output file exists\n",
    "    if os.path.exists(output_file):\n",
    "        # If it exists, append the new data\n",
    "        existing_df = pd.read_excel(output_file)\n",
    "        combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "        combined_df.to_excel(output_file, index=False)\n",
    "    else:\n",
    "        # If it doesn't exist, create a new file\n",
    "        new_df.to_excel(output_file, index=False)\n",
    "\n",
    "    print(f\"Data {data} has been successfully written to the Excel file.\")\n",
    "\n",
    "data_to_params = {\n",
    "    'etth1': (96, 720, None, 10, 3), 'etth2': (96, 720, None, 10, 3), 'ettm1': (96, 720, None, 10, 3), \n",
    "    'ettm2': (96, 720, None, 10, 3), 'chorales': (10, 5, None, 10, 3), 'noaa': (360, 180, 'AE000041196', 10, 2),\n",
    "    'electricity': (96, 32, None, 10, 2), 'traffic': (96, 48, None, 10, 2),\n",
    "    'air_quality': (40, 30, None, 10, 5), 'eeg_single': (100, 25, 'test_0', 10, 2), 'illness': (36, 60, None, 10, 2), \n",
    "    'ecg_single': (200, 100, 'E00001', 10, 5), 'exchange': (96, 720, None, 10, 3)\n",
    "}\n",
    "datasets = ['etth1', 'etth2', 'ettm1', 'ettm2', 'chorales', 'noaa', 'electricity', 'traffic',\n",
    "                  'air_quality', 'eeg_single', 'illness', 'ecg_single', 'exchange']\n",
    "\n",
    "\n",
    "for data_name in ['etth1', 'etth2', 'ettm1', 'ettm2', 'chorales', 'noaa', 'electricity', 'traffic',\n",
    "                  'air_quality', 'illness', 'ecg_single', 'exchange']:\n",
    "    num_of_vars = data_to_num_vars_dict[data_name]   \n",
    "    lookback, horizon, series, epochs, blocks = data_to_params[data_name]\n",
    "    train_X, train_y, val_X, val_y, test_X, test_y = get_data(\n",
    "        data=data_name, \n",
    "        lookback=lookback, \n",
    "        horizon=horizon,\n",
    "        n_series=num_of_vars,\n",
    "        print_stats=False,\n",
    "        series=series\n",
    "    )\n",
    "    test_y = test_y.numpy()\n",
    "    total = []\n",
    "    for i in range(num_of_vars):\n",
    "        original_series = test_y[:, :, i][0]\n",
    "        total.append(np.var(original_series))\n",
    "        \n",
    "    average_var = np.mean(total)\n",
    "\n",
    "    save_results(data_name, average_var, average_var, average_var, file_name='original_data_var_std_summary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correlation between MSE reduction percentage and seasonality component is: 0.1521870071285632\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Provided data\n",
    "data = {\n",
    "    \"dataset\": [\"Exchange Rate\", \"ILI\", \"Air Quality\", \"ETTh1\", \"ETTh2\", \"ETTm1\", \"ETTm2\", \"Traffic\", \"Weather\", \"Electricity\", \"ECG\", \"ECG of 600 patients\", \"EEG\", \"Chorales\"],\n",
    "    \"mse_reduction_percentage\": [82.69, 71.01, 46.04, 15.14, 15.39, 15.66, 17.91, 34.84, 20.88, 31.17, 77.44, 5.53, 64.80, 22.46],\n",
    "    \"seasonality_component\": [0.30, 0.05, 0.11, 0.01, 0.07, 0.01, 0.20, 0.57, 0.01, 0.82, 0.07, 0.07, 0.30, 0.02]\n",
    "}\n",
    "\n",
    "# Create a dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate the correlation\n",
    "correlation = df[\"mse_reduction_percentage\"].corr(df[\"seasonality_component\"])\n",
    "\n",
    "print(f\"The correlation between MSE reduction percentage and seasonality component is: {correlation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('/home/../multiTS/NFT/models/NFT')\n",
    "from NFT import NFT\n",
    "\n",
    "sys.path.append('NFT/')\n",
    "from dicts import data_to_num_vars_dict\n",
    "\n",
    "data = 'ettm2'\n",
    "file_path = f'/home/../multiTS/NFT/models/trained_models/ettm1/nft/nft_96l_720h_10epochs_3blocks_tcn/interpret_Results_ettm1.xlsx'\n",
    "\n",
    "data_to_params = {\n",
    "    'etth1': (96, 720, None, 10, 3), 'etth2': (96, 720, None, 10, 3), 'ettm1': (96, 720, None, 10, 3), \n",
    "    'ettm2': (96, 720, None, 10, 3), 'chorales': (10, 5, None, 10, 3), 'noaa': (360, 180, 'AE000041196', 10, 2),\n",
    "    'electricity': (96, 32, None, 10, 2), 'traffic': (96, 48, None, 10, 2),\n",
    "    'air_quality': (40, 30, None, 10, 5), 'eeg_single': (100, 25, None, 10, 2), 'illness': (36, 60, None, 10, 2), \n",
    "    'ecg_single': (200, 100, 'E00001', 10, 5), 'exchange': (96, 720, None, 15, 3)\n",
    "}\n",
    "\n",
    "for data in ['eeg_single']:\n",
    "    num_of_vars = data_to_num_vars_dict[data]   \n",
    "    lookback, horizon, series, epochs, blocks = data_to_params[data]\n",
    "    file_path = f'/home/../multiTS/NFT/models/trained_models/{data}/nft/nft_{lookback}l_{horizon}h_{epochs}epochs_{blocks}blocks_tcn/interpret_Results_{data}.xlsx'\n",
    "    if data in ['noaa', 'ecg_single']:\n",
    "        file_path = f'/home/../multiTS/NFT/models/trained_models/{data}/nft/nft_{lookback}l_{horizon}h_{epochs}epochs_{blocks}blocks_tcn_{series}/interpret_Results_{data}.xlsx'\n",
    "        \n",
    "    df = pd.read_excel(file_path, sheet_name='Sheet1')\n",
    "    num_of_vars = data_to_num_vars_dict[data]   \n",
    "\n",
    "    #  Function to calculate variance explained\n",
    "    def variance_explained(original_series, trend_component, seasonal_component):\n",
    "        total_variance = np.var(original_series)\n",
    "        trend_variance = np.var(trend_component)\n",
    "        seasonal_variance = np.var(seasonal_component)\n",
    "        \n",
    "        trend_explained = trend_variance #/ total_variance\n",
    "        seasonal_explained = seasonal_variance #/ total_variance\n",
    "        \n",
    "        return trend_explained, seasonal_explained, total_variance\n",
    "\n",
    "    def save_results(data, average_trend, average_seasonal, average_var, file_name):\n",
    "        # Output file path\n",
    "        output_file = f'/home/../multiTS/NFT/models/trained_models/{file_name}.xlsx'\n",
    "\n",
    "        # Data to be written\n",
    "        new_data = {\n",
    "            'data': [data],\n",
    "            'average_trend': [average_trend],\n",
    "            'average_seasonal': [average_seasonal],\n",
    "            'average_var': [average_var]\n",
    "        }\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "\n",
    "        # Check if the output file exists\n",
    "        if os.path.exists(output_file):\n",
    "            # If it exists, append the new data\n",
    "            existing_df = pd.read_excel(output_file)\n",
    "            combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "            combined_df.to_excel(output_file, index=False)\n",
    "        else:\n",
    "            # If it doesn't exist, create a new file\n",
    "            new_df.to_excel(output_file, index=False)\n",
    "\n",
    "        print(f\"Data {data} has been successfully written to the Excel file.\")\n",
    "\n",
    "\n",
    "    d = df.to_numpy().T\n",
    "    trend_list, seasonal_list, total = [], [], []\n",
    "    for s in range(num_of_vars):\n",
    "        original_series = d[s]\n",
    "        print(original_series.shape)\n",
    "        trend_component = d[s + num_of_vars]\n",
    "        seasonal_component = d[s + num_of_vars + num_of_vars]\n",
    "        trend_explained, seasonal_explained, total_variance = variance_explained(original_series, trend_component, seasonal_component)\n",
    "        trend_list.append(trend_explained)\n",
    "        seasonal_list.append(seasonal_explained)\n",
    "        total.append(total_variance)\n",
    "\n",
    "    average_trend = sum(trend_list) / len(trend_list)\n",
    "    average_seasonal = sum(seasonal_list) / len(seasonal_list)\n",
    "    average_var = sum(total) / len(total)\n",
    "\n",
    "    save_results(data, average_trend, average_seasonal, average_var, file_name='trend_seasonal_forecast_summary')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('/home/../multiTS/NFT/models/NFT')\n",
    "from NFT import NFT\n",
    "\n",
    "sys.path.append('NFT/')\n",
    "from dicts import data_to_num_vars_dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import sys\n",
    "import torch\n",
    "sys.path.append('NFT/')\n",
    "from models.training_functions import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import sys\n",
    "import torch\n",
    "sys.path.append('NFT/')\n",
    "from models.training_functions import get_data\n",
    "\n",
    "data_to_params = {\n",
    "    'etth1': (96, 720, None, 10, 3), 'etth2': (96, 720, None, 10, 3), 'ettm1': (96, 720, None, 10, 3), \n",
    "    'ettm2': (96, 720, None, 10, 3), 'chorales': (10, 5, None, 10, 3), 'noaa': (360, 180, 'AE000041196', 10, 2),\n",
    "    'electricity': (96, 32, None, 10, 2), 'traffic': (96, 48, None, 10, 2),\n",
    "    'air_quality': (40, 30, None, 10, 5), 'eeg_single': (100, 25, 'test_0', 10, 2), 'illness': (36, 60, None, 10, 2), \n",
    "    'ecg_single': (200, 100, 'E00001', 10, 5), 'exchange': (96, 720, None, 10, 3)\n",
    "}\n",
    "datasets = ['etth1', 'etth2', 'ettm1', 'ettm2', 'chorales', 'noaa', 'electricity', 'traffic',\n",
    "                  'air_quality', 'eeg_single', 'illness', 'ecg_single', 'exchange']\n",
    "\n",
    "\n",
    "for data_name in ['etth1', 'etth2', 'ettm1', 'ettm2', 'chorales', 'noaa', 'electricity', 'traffic',\n",
    "                  'air_quality', 'eeg_single', 'illness', 'ecg_single', 'exchange']:\n",
    "    num_of_vars = data_to_num_vars_dict[data_name]   \n",
    "    lookback, horizon, series, epochs, blocks = data_to_params[data_name]\n",
    "    train_X, train_y, val_X, val_y, test_X, test_y = get_data(\n",
    "        data=data_name, \n",
    "        lookback=lookback, \n",
    "        horizon=horizon,\n",
    "        n_series=num_of_vars,\n",
    "        print_stats=False,\n",
    "        series=series\n",
    "    )\n",
    "    test_y = test_y.numpy()\n",
    "    print(test_y.shape)\n",
    "    def variance_explained(original_series, trend_component, seasonal_component):\n",
    "        # valid_idx = ~np.isnan(trend_component) & ~np.isnan(seasonal_component)\n",
    "        # valid_trend = trend_component[valid_idx]\n",
    "        # valid_seasonal = seasonal_component[valid_idx]\n",
    "        # valid_original = original_series[valid_idx]\n",
    "\n",
    "        total_variance = np.var(original_series)\n",
    "        # trend_variance = np.var(valid_trend)\n",
    "        # seasonal_variance = np.var(valid_seasonal)\n",
    "        \n",
    "        # trend_explained = trend_variance / total_variance\n",
    "        # seasonal_explained = seasonal_variance / total_variance\n",
    "        \n",
    "        return total_variance\n",
    "\n",
    "    total = []\n",
    "\n",
    "    for i in range(num_of_vars):\n",
    "        original_series = test_y[:, :, i][0]\n",
    "        # periods = original_series.shape[0]\n",
    "\n",
    "        # get_fourier_coeff(original_series)\n",
    "        \n",
    "        # trend_explained, seasonal_explained = variance_explained(original_series, trend_component, seasonal_component)\n",
    "        # trend_list.append(trend_explained)\n",
    "        # seasonal_list.append(seasonal_explained)\n",
    "        \n",
    "        # print(seasonal_component)\n",
    "        # print(seasonal_explained)\n",
    "        total.append(np.var(original_series))\n",
    "        \n",
    "    # average_trend = np.mean(trend_list)\n",
    "    # average_seasonal = np.mean(seasonal_list)\n",
    "    average_var = np.mean(total)\n",
    "\n",
    "    # print(f\"Average trend variance explained: {average_trend}\")\n",
    "    # print(f\"Average seasonal variance explained: {average_seasonal}\")\n",
    "\n",
    "\n",
    "    save_results(data_name, average_var, average_var, average_var, file_name='original_data_var_summary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7824, 96, 7)\n",
      "(7824, 720, 7)\n",
      "(2160, 96, 7)\n",
      "(2160, 720, 7)\n",
      "(2160, 96, 7)\n",
      "(2160, 720, 7)\n",
      "(2160, 720, 7)\n",
      "Data etth1 has been successfully written to the Excel file.\n",
      "(7824, 96, 7)\n",
      "(7824, 720, 7)\n",
      "(2160, 96, 7)\n",
      "(2160, 720, 7)\n",
      "(2160, 96, 7)\n",
      "(2160, 720, 7)\n",
      "(2160, 720, 7)\n",
      "Data etth2 has been successfully written to the Excel file.\n",
      "(33744, 96, 7)\n",
      "(33744, 720, 7)\n",
      "(10800, 96, 7)\n",
      "(10800, 720, 7)\n",
      "(10800, 96, 7)\n",
      "(10800, 720, 7)\n",
      "(10800, 720, 7)\n",
      "Data ettm1 has been successfully written to the Excel file.\n",
      "(33744, 96, 7)\n",
      "(33744, 720, 7)\n",
      "(10800, 96, 7)\n",
      "(10800, 720, 7)\n",
      "(10800, 96, 7)\n",
      "(10800, 720, 7)\n",
      "(10800, 720, 7)\n",
      "Data ettm2 has been successfully written to the Excel file.\n",
      "(963, 10, 6)\n",
      "(963, 5, 6)\n",
      "(448, 10, 6)\n",
      "(448, 5, 6)\n",
      "(515, 10, 6)\n",
      "(515, 5, 6)\n",
      "(515, 5, 6)\n",
      "Data chorales has been successfully written to the Excel file.\n",
      "(9105, 360, 3)\n",
      "(9105, 180, 3)\n",
      "(3318, 360, 3)\n",
      "(3318, 180, 3)\n",
      "(5247, 360, 3)\n",
      "(5247, 180, 3)\n",
      "(5247, 180, 3)\n",
      "Data noaa has been successfully written to the Excel file.\n",
      "(18284, 96, 321)\n",
      "(18284, 32, 321)\n",
      "(2600, 96, 321)\n",
      "(2600, 32, 321)\n",
      "(5228, 96, 321)\n",
      "(5228, 32, 321)\n",
      "(5228, 32, 321)\n",
      "Data electricity has been successfully written to the Excel file.\n",
      "(12136, 96, 862)\n",
      "(12136, 48, 862)\n",
      "(1708, 96, 862)\n",
      "(1708, 48, 862)\n",
      "(3460, 96, 862)\n",
      "(3460, 48, 862)\n",
      "(3460, 48, 862)\n",
      "Data traffic has been successfully written to the Excel file.\n",
      "(3503, 40, 13)\n",
      "(3503, 30, 13)\n",
      "(1359, 40, 13)\n",
      "(1359, 30, 13)\n",
      "(2074, 40, 13)\n",
      "(2074, 30, 13)\n",
      "(2074, 30, 13)\n",
      "Data air_quality has been successfully written to the Excel file.\n",
      "(87069, 100, 36)\n",
      "(87069, 25, 36)\n",
      "(34752, 100, 36)\n",
      "(34752, 25, 36)\n",
      "(52191, 100, 36)\n",
      "(52191, 25, 36)\n",
      "(52191, 25, 36)\n",
      "Data eeg_single has been successfully written to the Excel file.\n",
      "(580, 36, 7)\n",
      "(580, 60, 7)\n",
      "(37, 36, 7)\n",
      "(37, 60, 7)\n",
      "(133, 36, 7)\n",
      "(133, 60, 7)\n",
      "(133, 60, 7)\n",
      "Data illness has been successfully written to the Excel file.\n",
      "(1255, 200, 12)\n",
      "(1255, 100, 12)\n",
      "(322, 200, 12)\n",
      "(322, 100, 12)\n",
      "(633, 200, 12)\n",
      "(633, 100, 12)\n",
      "(633, 100, 12)\n",
      "Data ecg_single has been successfully written to the Excel file.\n",
      "(4495, 96, 8)\n",
      "(4495, 720, 8)\n",
      "(40, 96, 8)\n",
      "(40, 720, 8)\n",
      "(797, 96, 8)\n",
      "(797, 720, 8)\n",
      "(797, 720, 8)\n",
      "Data exchange has been successfully written to the Excel file.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for data_name in ['etth1', 'etth2', 'ettm1', 'ettm2', 'chorales', 'noaa', 'electricity', 'traffic',\n",
    "                  'air_quality', 'eeg_single', 'illness', 'ecg_single', 'exchange']:\n",
    "    num_of_vars = data_to_num_vars_dict[data_name]   \n",
    "    lookback, horizon, series, epochs, blocks = data_to_params[data_name]\n",
    "    train_X, train_y, val_X, val_y, test_X, test_y = get_data(\n",
    "        data=data_name, \n",
    "        lookback=lookback, \n",
    "        horizon=horizon,\n",
    "        n_series=num_of_vars,\n",
    "        print_stats=False,\n",
    "        series=series\n",
    "    )\n",
    "    test_y = test_y.numpy()\n",
    "    print(test_y.shape)\n",
    "    def variance_explained(original_series, trend_component, seasonal_component):\n",
    "        # valid_idx = ~np.isnan(trend_component) & ~np.isnan(seasonal_component)\n",
    "        # valid_trend = trend_component[valid_idx]\n",
    "        # valid_seasonal = seasonal_component[valid_idx]\n",
    "        # valid_original = original_series[valid_idx]\n",
    "\n",
    "        total_variance = np.var(original_series)\n",
    "        # trend_variance = np.var(valid_trend)\n",
    "        # seasonal_variance = np.var(valid_seasonal)\n",
    "        \n",
    "        # trend_explained = trend_variance / total_variance\n",
    "        # seasonal_explained = seasonal_variance / total_variance\n",
    "        \n",
    "        return total_variance\n",
    "\n",
    "    total = []\n",
    "\n",
    "    for i in range(num_of_vars):\n",
    "        original_series = test_y[:, :, i][0]\n",
    "        # periods = original_series.shape[0]\n",
    "\n",
    "        # get_fourier_coeff(original_series)\n",
    "        \n",
    "        # trend_explained, seasonal_explained = variance_explained(original_series, trend_component, seasonal_component)\n",
    "        # trend_list.append(trend_explained)\n",
    "        # seasonal_list.append(seasonal_explained)\n",
    "        \n",
    "        # print(seasonal_component)\n",
    "        # print(seasonal_explained)\n",
    "        total.append(np.var(original_series))\n",
    "        \n",
    "    # average_trend = np.mean(trend_list)\n",
    "    # average_seasonal = np.mean(seasonal_list)\n",
    "    average_var = np.mean(total)\n",
    "\n",
    "    # print(f\"Average trend variance explained: {average_trend}\")\n",
    "    # print(f\"Average seasonal variance explained: {average_seasonal}\")\n",
    "\n",
    "\n",
    "    save_results(data_name, average_var, average_var, average_var, file_name='original_data_var_summary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for data_name in ['etth1','etth2', 'ettm1', 'ettm2', 'noaa', 'electricity', 'traffic',\n",
    "                  'air_quality', 'illness', 'ecg_single', 'exchange']:\n",
    "    num_of_vars = data_to_num_vars_dict[data_name]   \n",
    "    lookback, horizon, series, epochs, blocks = data_to_params[data_name]\n",
    "    file_path = f'/home/../multiTS/NFT/models/trained_models/{data_name}/nft/nft_{lookback}l_{horizon}h_{epochs}epochs_{blocks}blocks_tcn/interpret_Results_{data_name}.xlsx'\n",
    "    if data_name in ['noaa', 'ecg_single']:\n",
    "        file_path = f'/home/../multiTS/NFT/models/trained_models/{data_name}/nft/nft_{lookback}l_{horizon}h_{epochs}epochs_{blocks}blocks_tcn_{series}/interpret_Results_{data_name}.xlsx'\n",
    "    df = pd.read_excel(file_path, sheet_name='Sheet1')\n",
    "    forecasts = df.to_numpy().T\n",
    "    def variance_explained(original_series, trend_component, seasonal_component):\n",
    "        valid_idx = ~np.isnan(trend_component) & ~np.isnan(seasonal_component)\n",
    "        valid_trend = trend_component[valid_idx]\n",
    "        valid_seasonal = seasonal_component[valid_idx]\n",
    "        valid_original = original_series[valid_idx]\n",
    "\n",
    "        total_variance = np.var(valid_original)\n",
    "        trend_variance = np.var(valid_trend)\n",
    "        seasonal_variance = np.var(valid_seasonal)\n",
    "        \n",
    "        trend_explained = trend_variance / total_variance\n",
    "        seasonal_explained = seasonal_variance / total_variance\n",
    "        \n",
    "        return trend_explained, seasonal_explained\n",
    "\n",
    "    trend_list, seasonal_list = [], []\n",
    "    for i in range(num_of_vars):\n",
    "        print(i)\n",
    "        original_series = forecasts[i]\n",
    "        periods = original_series.shape[0]\n",
    "        print(original_series.shape, periods)\n",
    "        d = {\n",
    "            'date': pd.date_range(start='2020-01-01', periods=periods, freq='D'),\n",
    "            'value': original_series\n",
    "        }\n",
    "        df = pd.DataFrame(d)\n",
    "        df.set_index('date', inplace=True)\n",
    "\n",
    "        result = seasonal_decompose(df['value'], model='additive', period=lookback)\n",
    "\n",
    "        trend_component = result.trend.values\n",
    "        seasonal_component = result.seasonal.values\n",
    "     \n",
    "        trend_explained, seasonal_explained = variance_explained(original_series, trend_component, seasonal_component)\n",
    "        trend_list.append(trend_explained)\n",
    "        seasonal_list.append(seasonal_explained)\n",
    "        \n",
    "\n",
    "    average_trend = np.mean(trend_list)\n",
    "    average_seasonal = np.mean(seasonal_list)\n",
    "\n",
    "    print(f\"Average trend variance explained: {average_trend}\")\n",
    "    print(f\"Average seasonal variance explained: {average_seasonal}\")\n",
    "\n",
    "\n",
    "    save_results(data_name, average_trend, average_seasonal, file_name='forcasted_data_trend_seasonal_summary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(580, 36, 7)\n",
      "(580, 60, 7)\n",
      "(37, 36, 7)\n",
      "(37, 60, 7)\n",
      "(133, 36, 7)\n",
      "(133, 60, 7)\n",
      "(133, 60, 7)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 55\u001b[0m\n\u001b[1;32m     50\u001b[0m periods \u001b[38;5;241m=\u001b[39m original_series\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     51\u001b[0m d \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m: pd\u001b[38;5;241m.\u001b[39mdate_range(start\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2020-01-01\u001b[39m\u001b[38;5;124m'\u001b[39m, periods\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m36\u001b[39m, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m: original_series\n\u001b[1;32m     54\u001b[0m }\n\u001b[0;32m---> 55\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m df\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     58\u001b[0m result \u001b[38;5;241m=\u001b[39m seasonal_decompose(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m], model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madditive\u001b[39m\u001b[38;5;124m'\u001b[39m, period\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/multTS/lib/python3.9/site-packages/pandas/core/frame.py:529\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    524\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    525\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    526\u001b[0m     )\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 529\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43minit_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmrecords\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmrecords\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/multTS/lib/python3.9/site-packages/pandas/core/internals/construction.py:287\u001b[0m, in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    281\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    282\u001b[0m         arr \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ABCIndexClass) \u001b[38;5;28;01melse\u001b[39;00m arr\u001b[38;5;241m.\u001b[39m_data \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[1;32m    283\u001b[0m     ]\n\u001b[1;32m    284\u001b[0m     arrays \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    285\u001b[0m         arr \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_datetime64tz_dtype(arr) \u001b[38;5;28;01melse\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[1;32m    286\u001b[0m     ]\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/multTS/lib/python3.9/site-packages/pandas/core/internals/construction.py:80\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43mextract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/miniconda3/envs/multTS/lib/python3.9/site-packages/pandas/core/internals/construction.py:401\u001b[0m, in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    399\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 401\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marrays must all be same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    406\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import sys\n",
    "sys.path.append('NFT/')\n",
    "from models.training_functions import get_data\n",
    "\n",
    "data_to_params = {\n",
    "    'etth1': (96, 720, None), 'etth2': (96, 720, None), 'ettm1': (96, 720, None), 'ettm2': (96, 720, None), \n",
    "    'chorales': (10, 5, None), 'noaa': (360, 180, 'AE000041196'), 'electricity': (96, 32, None), 'traffic': (96, 48, None),\n",
    "    'air_quality': (40, 30, None), 'eeg_single': (100, 25, 'test_0'), 'illness': (36, 60, None), 'ecg_single': (200, 100, 'E00001'), 'exchange': (96, 720, None)\n",
    "}\n",
    "['etth1', 'etth2', 'ettm1', 'ettm2', 'chorales', 'noaa', 'electricity', 'traffic',\n",
    "                  'air_quality', 'eeg_single', 'illness', 'ecg_single', 'exchange']\n",
    "# data_name = 'etth2'\n",
    "for data_name in [\n",
    "                  'illness']:\n",
    "    num_of_vars = data_to_num_vars_dict[data_name]   \n",
    "    lookback, horizon, series = data_to_params[data_name]\n",
    "    train_X, train_y, val_X, val_y, test_X, test_y = get_data(\n",
    "        data=data_name, \n",
    "        lookback=lookback, \n",
    "        horizon=horizon,\n",
    "        n_series=num_of_vars,\n",
    "        print_stats=False,\n",
    "        series=series\n",
    "    )\n",
    "    test_y = test_y.numpy()\n",
    "    print(test_y.shape)\n",
    "    def variance_explained(original_series, trend_component, seasonal_component):\n",
    "        valid_idx = ~np.isnan(trend_component) & ~np.isnan(seasonal_component)\n",
    "        valid_trend = trend_component[valid_idx]\n",
    "        valid_seasonal = seasonal_component[valid_idx]\n",
    "        valid_original = original_series[valid_idx]\n",
    "\n",
    "        total_variance = np.var(valid_original)\n",
    "        trend_variance = np.var(valid_trend)\n",
    "        seasonal_variance = np.var(valid_seasonal)\n",
    "        \n",
    "        trend_explained = trend_variance / total_variance\n",
    "        seasonal_explained = seasonal_variance / total_variance\n",
    "        \n",
    "        return trend_explained, seasonal_explained\n",
    "\n",
    "    trend_list, seasonal_list = [], []\n",
    "\n",
    "    for i in range(num_of_vars):\n",
    "        original_series = test_y[:, :, i][0]\n",
    "        periods = original_series.shape[0]\n",
    "        d = {\n",
    "            'date': pd.date_range(start='2020-01-01', periods=36, freq='D'),\n",
    "            'value': original_series\n",
    "        }\n",
    "        df = pd.DataFrame(d)\n",
    "        df.set_index('date', inplace=True)\n",
    "\n",
    "        result = seasonal_decompose(df['value'], model='additive', period=7)\n",
    "\n",
    "        trend_component = result.trend.values\n",
    "        seasonal_component = result.seasonal.values\n",
    "        # result.plot()\n",
    "        # plt.show()\n",
    "        trend_explained, seasonal_explained = variance_explained(original_series, trend_component, seasonal_component)\n",
    "        trend_list.append(trend_explained)\n",
    "        seasonal_list.append(seasonal_explained)\n",
    "        \n",
    "        # print(seasonal_component)\n",
    "        print(seasonal_explained)\n",
    "\n",
    "    average_trend = np.mean(trend_list)\n",
    "    average_seasonal = np.mean(seasonal_list)\n",
    "\n",
    "    print(f\"Average trend variance explained: {average_trend}\")\n",
    "    print(f\"Average seasonal variance explained: {average_seasonal}\")\n",
    "\n",
    "\n",
    "    save_results(data_name, average_trend, average_seasonal, file_name='original_data_trend_seasonal_summary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etth1: Dominant Seasonality\n",
      "etth2: Dominant Seasonality\n",
      "ettm1: Balanced Components\n",
      "ettm2: Balanced Components\n",
      "chorales: Dominant Seasonality\n",
      "noaa: Dominant Trend\n",
      "electricity: Dominant Seasonality\n",
      "traffic: Dominant Seasonality\n",
      "air_quality: Dominant Seasonality\n",
      "eeg_single: Dominant Seasonality\n",
      "illness: Dominant Seasonality\n",
      "ecg_single: Dominant Seasonality\n",
      "exchange: Balanced Components\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the Excel file\n",
    "file_path = '/home/../multiTS/NFT/models/trained_models/trend_seasonal_summary.xlsx'\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Create the dictionary\n",
    "results = {}\n",
    "for index, row in df.iterrows():\n",
    "    results[row['data']] = [row['average_trend'], row['average_seasonal']]\n",
    "\n",
    "# Function to classify the dataset\n",
    "def classify_dataset(trend_explained, seasonal_explained, threshold=0.1):\n",
    "    if trend_explained < threshold and seasonal_explained < threshold:\n",
    "        return \"No Significant Components\"\n",
    "    elif seasonal_explained > trend_explained + threshold:\n",
    "        return \"Dominant Seasonality\"\n",
    "    elif trend_explained > seasonal_explained + threshold:\n",
    "        return \"Dominant Trend\"\n",
    "    else:\n",
    "        return \"Balanced Components\"\n",
    "\n",
    "# Classify each dataset\n",
    "classification_results = {}\n",
    "for dataset, values in results.items():\n",
    "    trend_explained, seasonal_explained = values\n",
    "    classification = classify_dataset(trend_explained, seasonal_explained)\n",
    "    classification_results[dataset] = classification\n",
    "\n",
    "# Print the classification results\n",
    "for dataset, classification in classification_results.items():\n",
    "    print(f\"{dataset}: {classification}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multTS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
